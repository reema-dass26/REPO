{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b379083d-146e-4d73-845d-2e11287fc469",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Full dynamic mapping file created: mappings/full_mapping.json with 149 fields!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def fetch_all_keys(json_path):\n",
    "    \"\"\"Recursively fetch all keys from a JSON file.\"\"\"\n",
    "    keys = set()\n",
    "\n",
    "    def _recursive_extract(obj, prefix=\"\"):\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in obj.items():\n",
    "                full_key = f\"{prefix}.{k}\" if prefix else k\n",
    "                keys.add(full_key)\n",
    "                _recursive_extract(v, prefix=full_key)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                _recursive_extract(item, prefix=prefix)\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        _recursive_extract(data)\n",
    "\n",
    "    return keys\n",
    "\n",
    "# 🔥 Dynamically fetch keys from all MODEL_PROVENANCE run summaries\n",
    "all_json_files = glob.glob(\"MODEL_PROVENANCE/*/*_run_summary.json\")\n",
    "\n",
    "collected_keys = set()\n",
    "for json_file in all_json_files:\n",
    "    keys = fetch_all_keys(json_file)\n",
    "    collected_keys.update(keys)\n",
    "\n",
    "# ✅ Now build the mapping\n",
    "mapping = {key: {\"@id\": key} for key in collected_keys}\n",
    "\n",
    "# Special case for timestamps\n",
    "if \"start_time\" in mapping:\n",
    "    mapping[\"start_time\"][\"@id\"] = \"prov:startedAtTime\"\n",
    "    mapping[\"start_time\"][\"@type\"] = \"xsd:dateTime\"\n",
    "if \"end_time\" in mapping:\n",
    "    mapping[\"end_time\"][\"@id\"] = \"prov:endedAtTime\"\n",
    "    mapping[\"end_time\"][\"@type\"] = \"xsd:dateTime\"\n",
    "\n",
    "# 🔥 Save mapping dynamically\n",
    "os.makedirs(\"mappings\", exist_ok=True)\n",
    "with open(\"mappings/full_mapping.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mapping, f, indent=2)\n",
    "\n",
    "print(f\"✅ Full dynamic mapping file created: mappings/full_mapping.json with {len(mapping)} fields!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae3bbfbe-cf32-4436-8b68-8c30b5588cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted RandomForest_Iris_v20250425_121328_run_summary.json → RandomForest_Iris_v20250425_121328.jsonld, RandomForest_Iris_v20250425_121328.ttl\n",
      "✅ Converted RandomForest_Iris_v20250425_125653_run_summary.json → RandomForest_Iris_v20250425_125653.jsonld, RandomForest_Iris_v20250425_125653.ttl\n",
      "✅ Converted RandomForest_Iris_v20250425_131407_run_summary.json → RandomForest_Iris_v20250425_131407.jsonld, RandomForest_Iris_v20250425_131407.ttl\n",
      "✅ Converted RandomForest_Iris_v20250425_132526_run_summary.json → RandomForest_Iris_v20250425_132526.jsonld, RandomForest_Iris_v20250425_132526.ttl\n",
      "✅ Converted RandomForest_Iris_v20250425_135553_run_summary.json → RandomForest_Iris_v20250425_135553.jsonld, RandomForest_Iris_v20250425_135553.ttl\n",
      "✅ Converted RandomForest_Iris_v20250425_135900_run_summary.json → RandomForest_Iris_v20250425_135900.jsonld, RandomForest_Iris_v20250425_135900.ttl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from rdflib import Graph\n",
    "\n",
    "import os\n",
    "def iso8601(ms):\n",
    "    \"\"\"Convert milliseconds since epoch to ISO8601 UTC.\"\"\"\n",
    "    return datetime.fromtimestamp(ms / 1000, tz=timezone.utc).isoformat()\n",
    "# Load the context mapping\n",
    "with open(\"mappings/full_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ctx = json.load(f)\n",
    "\n",
    "# Loop through your run_summary files\n",
    "for json_path in glob.glob(\"MODEL_PROVENANCE/*/*_run_summary.json\"):\n",
    "    basename   = os.path.basename(json_path)\n",
    "    model_name = basename.rsplit(\"_run_summary.json\", 1)[0]\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        summary = json.load(f)\n",
    "\n",
    "    doc = {\n",
    "        \"@context\": ctx,\n",
    "        \"run_id\": summary.get(\"run_id\", \"\"),\n",
    "        \"run_name\": summary.get(\"run_name\", \"\"),\n",
    "        \"experiment_id\": summary.get(\"experiment_id\", \"\"),\n",
    "        \"params\": summary.get(\"params\", {}),\n",
    "        \"metrics\": summary.get(\"metrics\", {}),\n",
    "        \"artifacts\": summary.get(\"artifacts\", []),\n",
    "        \"tags\": summary.get(\"tags\", {}),\n",
    "        \"start_time\": iso8601(summary[\"start_time\"])\n",
    "    }\n",
    "\n",
    "    if summary.get(\"end_time\") is not None:\n",
    "        doc[\"end_time\"] = iso8601(summary[\"end_time\"])\n",
    "\n",
    "    doc[\"used\"] = summary.get(\"tags\", {}).get(\"dataset_uri\") or []\n",
    "    doc[\"generated\"] = [\n",
    "        art.get(\"uri\") or art.get(\"path\")\n",
    "        for art in summary.get(\"artifacts\", [])\n",
    "    ]\n",
    "\n",
    "    # Save .jsonld\n",
    "    out_jsonld = os.path.join(\"MODEL_PROVENANCE\", model_name, f\"{model_name}.jsonld\")\n",
    "    with open(out_jsonld, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(doc, f, indent=2)\n",
    "\n",
    "    # Save .ttl\n",
    "    g = Graph().parse(data=json.dumps(doc), format=\"json-ld\")\n",
    "    out_ttl = os.path.join(\"MODEL_PROVENANCE\", model_name, f\"{model_name}.ttl\")\n",
    "    g.serialize(destination=out_ttl, format=\"turtle\")\n",
    "\n",
    "    print(f\"✅ Converted {basename} → {os.path.basename(out_jsonld)}, {os.path.basename(out_ttl)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c3576c1-6df9-469f-8b72-302e9738bbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Differences for RandomForest_Iris_v20250425_121328: 4 differences found\n",
      "\n",
      "🔎 Differences for RandomForest_Iris_v20250425_125653: 5 differences found\n",
      "\n",
      "🔎 Differences for RandomForest_Iris_v20250425_131407: 4 differences found\n",
      "\n",
      "🔎 Differences for RandomForest_Iris_v20250425_132526: 4 differences found\n",
      "\n",
      "🔎 Differences for RandomForest_Iris_v20250425_135553: 5 differences found\n",
      "\n",
      "🔎 Differences for RandomForest_Iris_v20250425_135900: 5 differences found\n",
      "\n",
      "Summary of all differences:\n",
      "type\n",
      "added      12\n",
      "removed     9\n",
      "changed     6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from rdflib import Graph\n",
    "\n",
    "# ---------- Helper functions -------------\n",
    "\n",
    "def load_as_dict(path):\n",
    "    \"\"\"Load a JSON or JSON-LD/Turtle file as dictionary.\"\"\"\n",
    "    if path.endswith((\".ttl\", \".turtle\")):\n",
    "        g = Graph()\n",
    "        g.parse(path, format=\"turtle\")\n",
    "        return json.loads(g.serialize(format=\"json-ld\", indent=2))\n",
    "    else:\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "def compare_json(a, b, path=\"\"):\n",
    "    \"\"\"Recursively compare two JSON structures.\"\"\"\n",
    "    diffs = []\n",
    "    if isinstance(a, dict) and isinstance(b, dict):\n",
    "        a = {k: v for k, v in a.items() if k != \"@context\"}\n",
    "        b = {k: v for k, v in b.items() if k != \"@context\"}\n",
    "        all_keys = set(a) | set(b)\n",
    "        for k in all_keys:\n",
    "            new_path = f\"{path}/{k}\" if path else k\n",
    "            if k not in a:\n",
    "                diffs.append({\"path\": new_path, \"type\": \"added\", \"a\": None, \"b\": b[k]})\n",
    "            elif k not in b:\n",
    "                diffs.append({\"path\": new_path, \"type\": \"removed\", \"a\": a[k], \"b\": None})\n",
    "            else:\n",
    "                diffs.extend(compare_json(a[k], b[k], new_path))\n",
    "    elif isinstance(a, list) and isinstance(b, list):\n",
    "        for i, (ia, ib) in enumerate(zip(a, b)):\n",
    "            diffs.extend(compare_json(ia, ib, f\"{path}[{i}]\"))\n",
    "        if len(a) < len(b):\n",
    "            for i in range(len(a), len(b)):\n",
    "                diffs.append({\"path\": f\"{path}[{i}]\", \"type\": \"added\", \"a\": None, \"b\": b[i]})\n",
    "        elif len(a) > len(b):\n",
    "            for i in range(len(b), len(a)):\n",
    "                diffs.append({\"path\": f\"{path}[{i}]\", \"type\": \"removed\", \"a\": a[i], \"b\": None})\n",
    "    else:\n",
    "        if a != b:\n",
    "            diffs.append({\"path\": path, \"type\": \"changed\", \"a\": a, \"b\": b})\n",
    "    return diffs\n",
    "\n",
    "# ---------- Main comparison -------------\n",
    "\n",
    "# 1. Scan for all run folders\n",
    "base_dir = \"MODEL_PROVENANCE\"\n",
    "runs = [d for d in glob.glob(os.path.join(base_dir, \"*\")) if os.path.isdir(d)]\n",
    "\n",
    "# 2. Compare only JSON vs JSON-LD\n",
    "all_diffs = []\n",
    "\n",
    "for run_dir in runs:\n",
    "    model_name = os.path.basename(run_dir)\n",
    "    \n",
    "    json_path = os.path.join(run_dir, f\"{model_name}_run_summary.json\")\n",
    "    jsonld_path = os.path.join(run_dir, f\"{model_name}.jsonld\")\n",
    "\n",
    "    if os.path.exists(json_path) and os.path.exists(jsonld_path):\n",
    "        try:\n",
    "            json_obj = load_as_dict(json_path)\n",
    "            jsonld_obj = load_as_dict(jsonld_path)\n",
    "\n",
    "            diffs = compare_json(json_obj, jsonld_obj)\n",
    "            if diffs:\n",
    "                print(f\"\\n🔎 Differences for {model_name}: {len(diffs)} differences found\")\n",
    "                all_diffs.extend(diffs)\n",
    "            else:\n",
    "                print(f\"✅ {model_name}: No differences detected\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error comparing {model_name}: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"⚠️ Missing files in {model_name}: Skipping.\")\n",
    "\n",
    "# 3. Summarize if needed\n",
    "if all_diffs:\n",
    "    df_diffs = pd.DataFrame(all_diffs)\n",
    "    print(\"\\nSummary of all differences:\")\n",
    "    print(df_diffs['type'].value_counts())\n",
    "else:\n",
    "    print(\"\\n🎉 All JSON and JSON-LD files match perfectly!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c62d5-79bd-4141-9c68-b6e7361d3a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010368c0-94d4-4f69-a050-13bf8e02fdf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1882902a-7acd-4efe-b53f-4c4463cbb3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mapping file created: mapping_files\\dynamic_mapping.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def fetch_all_keys(json_path):\n",
    "    \"\"\"Recursively fetch all keys from a JSON file.\"\"\"\n",
    "    keys = set()\n",
    "\n",
    "    def _recursive_extract(obj, prefix=\"\"):\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in obj.items():\n",
    "                full_key = f\"{prefix}.{k}\" if prefix else k\n",
    "                keys.add(full_key)\n",
    "                _recursive_extract(v, prefix=full_key)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                _recursive_extract(item, prefix=prefix)\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        _recursive_extract(data)\n",
    "\n",
    "    return keys\n",
    "\n",
    "def create_mapping_from_keys(keys):\n",
    "    \"\"\"Create a simple mapping where each key maps to itself, with special rules for timestamps.\"\"\"\n",
    "    mapping = {}\n",
    "\n",
    "    for key in sorted(keys):\n",
    "        if \".\" not in key:\n",
    "            # Top-level fields\n",
    "            if key in [\"start_time\", \"end_time\"]:\n",
    "                mapping[key] = {\n",
    "                    \"@id\": f\"prov:{'startedAtTime' if key == 'start_time' else 'endedAtTime'}\",\n",
    "                    \"@type\": \"xsd:dateTime\"\n",
    "                }\n",
    "            elif key in [\"run_id\", \"run_name\", \"experiment_id\"]:\n",
    "                mapping[key] = {\"@id\": key}\n",
    "            else:\n",
    "                mapping[key] = {\"@id\": key}\n",
    "        else:\n",
    "            # Nested fields\n",
    "            mapping[key] = {\"@id\": key}\n",
    "\n",
    "    # Attach namespaces\n",
    "    mapping[\"@context\"] = {\n",
    "        \"prov\": \"http://www.w3.org/ns/prov#\",\n",
    "        \"xsd\":  \"http://www.w3.org/2001/XMLSchema#\"\n",
    "    }\n",
    "\n",
    "    return mapping\n",
    "\n",
    "# --- Main execution ---\n",
    "\n",
    "# 1. Fetch keys from all JSONs\n",
    "all_keys = set()\n",
    "for json_path in glob.glob(\"MODEL_PROVENANCE/*/*_run_summary.json\"):\n",
    "    keys = fetch_all_keys(json_path)\n",
    "    all_keys.update(keys)\n",
    "\n",
    "# 2. Create mapping\n",
    "mapping_dict = create_mapping_from_keys(all_keys)\n",
    "\n",
    "# 3. Save mapping\n",
    "output_dir = \"mapping_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"dynamic_mapping.json\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mapping_dict, f, indent=2)\n",
    "\n",
    "print(f\"✅ Mapping file created: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e36288ee-3033-473a-ba9e-4233d10869a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RandomForest_Iris_v20250425_121328: JSON-LD and TTL generated\n",
      "✅ RandomForest_Iris_v20250425_125653: JSON-LD and TTL generated\n",
      "✅ RandomForest_Iris_v20250425_131407: JSON-LD and TTL generated\n",
      "✅ RandomForest_Iris_v20250425_132526: JSON-LD and TTL generated\n",
      "✅ RandomForest_Iris_v20250425_135553: JSON-LD and TTL generated\n",
      "✅ RandomForest_Iris_v20250425_135900: JSON-LD and TTL generated\n",
      "🚀 All model runs converted successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "from rdflib import Graph\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# === Utility functions ===\n",
    "\n",
    "def iso8601(ms):\n",
    "    \"\"\"Convert milliseconds since epoch to ISO8601 UTC.\"\"\"\n",
    "    return datetime.fromtimestamp(ms / 1000, tz=timezone.utc).isoformat()\n",
    "\n",
    "def load_mapping(mapping_path=\"mapping_files/dynamic_mapping.json\"):\n",
    "    \"\"\"Load dynamic mapping file.\"\"\"\n",
    "    with open(mapping_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def map_json_fields(summary, mapping):\n",
    "    \"\"\"Apply dynamic field mapping to summary dict.\"\"\"\n",
    "    doc = {\"@context\": mapping[\"@context\"]}\n",
    "\n",
    "    for key, map_info in mapping.items():\n",
    "        if key == \"@context\":\n",
    "            continue\n",
    "\n",
    "        value = get_nested(summary, key)\n",
    "        if value is not None:\n",
    "            mapped_key = map_info[\"@id\"]\n",
    "            # Apply ISO8601 if type is datetime\n",
    "            if map_info.get(\"@type\") == \"xsd:dateTime\":\n",
    "                value = iso8601(value)\n",
    "            doc[mapped_key] = value\n",
    "\n",
    "    return doc\n",
    "\n",
    "def get_nested(data, dotted_key):\n",
    "    \"\"\"Safely get nested keys like artifacts.uri.\"\"\"\n",
    "    parts = dotted_key.split(\".\")\n",
    "    for part in parts:\n",
    "        if isinstance(data, dict):\n",
    "            data = data.get(part)\n",
    "        else:\n",
    "            return None\n",
    "    return data\n",
    "\n",
    "# === Main execution ===\n",
    "\n",
    "# Load the dynamic mapping\n",
    "mapping = load_mapping()\n",
    "\n",
    "# Process all summaries\n",
    "all_json_files = glob.glob(\"MODEL_PROVENANCE/*/*_run_summary.json\")\n",
    "\n",
    "for json_path in all_json_files:\n",
    "    base_dir = os.path.dirname(json_path)\n",
    "    basename = os.path.basename(json_path)\n",
    "    model_name = basename.replace(\"_run_summary.json\", \"\")\n",
    "\n",
    "    # Load JSON\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        summary = json.load(f)\n",
    "\n",
    "    # Map using dynamic mapping\n",
    "    jsonld_doc = map_json_fields(summary, mapping)\n",
    "\n",
    "    # Save as .jsonld\n",
    "    jsonld_path = os.path.join(base_dir, f\"{model_name}.jsonld\")\n",
    "    with open(jsonld_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(jsonld_doc, f, indent=2)\n",
    "\n",
    "    # Convert to .ttl\n",
    "    g = Graph()\n",
    "    g.parse(data=json.dumps(jsonld_doc), format=\"json-ld\")\n",
    "    ttl_path = os.path.join(base_dir, f\"{model_name}.ttl\")\n",
    "    g.serialize(destination=ttl_path, format=\"turtle\")\n",
    "\n",
    "    print(f\"✅ {model_name}: JSON-LD and TTL generated\")\n",
    "\n",
    "print(\"🚀 All model runs converted successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb0175dd-db95-444e-b762-323162bf5c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RDF/XML written to MODEL_PROVENANCE/RandomForest_Iris_v20250425_135900/RandomForest_Iris_v20250425_135900.rdf\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from rdflib import Graph\n",
    "\n",
    "def convert_jsonld_to_rdfxml(jsonld_path, rdfxml_out_path):\n",
    "    # 1. Load your JSON-LD\n",
    "    with open(jsonld_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        jsonld_data = json.load(f)\n",
    "\n",
    "    # 2. Parse it into an RDF Graph\n",
    "    g = Graph()\n",
    "    g.parse(data=json.dumps(jsonld_data), format=\"json-ld\")\n",
    "\n",
    "    # 3. Serialize it into RDF/XML\n",
    "    g.serialize(destination=rdfxml_out_path, format=\"xml\")\n",
    "\n",
    "    print(f\"✅ RDF/XML written to {rdfxml_out_path}\")\n",
    "\n",
    "# Example usage\n",
    "convert_jsonld_to_rdfxml(\n",
    "    \"MODEL_PROVENANCE/RandomForest_Iris_v20250425_135900/RandomForest_Iris_v20250425_135900.jsonld\",\n",
    "    \"MODEL_PROVENANCE/RandomForest_Iris_v20250425_135900/RandomForest_Iris_v20250425_135900.rdf\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c3430a-a621-4445-b4dc-87587ec769bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41d1a0-e99e-40ae-830a-a64c57676b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15882d8-a2ff-43e0-966c-24e3555edf25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4902d153-42ad-4558-966d-08f0fe19ced2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213544a2-c989-4905-a21a-872c9cf756b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3410e-9e9d-4e35-afb9-62844c4df3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9773f38-8440-46ed-a330-8f00405d2368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Found 149 unique fields across 6 run summaries.\n",
      "✅ Mapping created and saved at: mappings\\json_to_rdf_mapping.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "def fetch_all_keys(json_path):\n",
    "    \"\"\"Recursively fetch all JSON keys in dot notation.\"\"\"\n",
    "    keys = set()\n",
    "\n",
    "    def _recursive_extract(obj, prefix=\"\"):\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in obj.items():\n",
    "                full_key = f\"{prefix}.{k}\" if prefix else k\n",
    "                keys.add(full_key)\n",
    "                _recursive_extract(v, prefix=full_key)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                _recursive_extract(item, prefix=prefix)\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        _recursive_extract(data)\n",
    "\n",
    "    return keys\n",
    "\n",
    "# 🔥 Fetch all keys from MODEL_PROVENANCE\n",
    "all_json_files = glob.glob(\"MODEL_PROVENANCE/*/*_run_summary.json\")\n",
    "collected_keys = set()\n",
    "\n",
    "for json_file in all_json_files:\n",
    "    keys = fetch_all_keys(json_file)\n",
    "    collected_keys.update(keys)\n",
    "\n",
    "print(f\"🔎 Found {len(collected_keys)} unique fields across {len(all_json_files)} run summaries.\")\n",
    "\n",
    "# ✨ Auto-build the mapping: JSON key ➔ RDF property\n",
    "mapping = {}\n",
    "\n",
    "for key in sorted(collected_keys):\n",
    "    rdf_key = key.replace(\".\", \"_\")  # replace dot with underscore\n",
    "    mapping[key] = f\"prov:{rdf_key}\"\n",
    "\n",
    "# 📂 Save to file\n",
    "os.makedirs(\"mappings\", exist_ok=True)\n",
    "mapping_file = os.path.join(\"mappings\", \"json_to_rdf_mapping.json\")\n",
    "\n",
    "with open(mapping_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mapping, f, indent=2)\n",
    "\n",
    "print(f\"✅ Mapping created and saved at: {mapping_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dd52c15-e6a2-4b89-b26f-03bb64936beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RDF/XML created: rdf_exports\\RandomForest_Iris_v20250425_121328.rdf\n",
      "✅ RDF/XML created: rdf_exports\\RandomForest_Iris_v20250425_125653.rdf\n",
      "✅ RDF/XML created: rdf_exports\\RandomForest_Iris_v20250425_131407.rdf\n",
      "✅ RDF/XML created: rdf_exports\\RandomForest_Iris_v20250425_132526.rdf\n",
      "✅ RDF/XML created: rdf_exports\\RandomForest_Iris_v20250425_135553.rdf\n",
      "✅ RDF/XML created: rdf_exports\\RandomForest_Iris_v20250425_135900.rdf\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "from rdflib import Graph, URIRef, Literal, Namespace, RDF\n",
    "from rdflib.namespace import XSD\n",
    "\n",
    "# Load your dynamic JSON ➔ RDF mapping\n",
    "with open(\"mappings/json_to_rdf_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    field_mapping = json.load(f)\n",
    "\n",
    "prov = Namespace(\"http://www.w3.org/ns/prov#\")\n",
    "\n",
    "def flatten_json(obj, parent_key=''):\n",
    "    \"\"\"Flatten nested JSON with dot notation.\"\"\"\n",
    "    items = []\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            new_key = f\"{parent_key}.{k}\" if parent_key else k\n",
    "            items.extend(flatten_json(v, new_key))\n",
    "    elif isinstance(obj, list):\n",
    "        for i, v in enumerate(obj):\n",
    "            new_key = f\"{parent_key}[{i}]\"\n",
    "            items.extend(flatten_json(v, new_key))\n",
    "    else:\n",
    "        items.append((parent_key, obj))\n",
    "    return items\n",
    "\n",
    "def create_rdf_from_json(json_path):\n",
    "    \"\"\"Given a run_summary JSON file, create RDF/XML.\"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Flatten the JSON\n",
    "    flat_data = dict(flatten_json(data))\n",
    "\n",
    "    # Build RDF graph\n",
    "    g = Graph()\n",
    "    g.bind(\"prov\", prov)\n",
    "\n",
    "    # Create a blank subject (could also use the run_id if you want)\n",
    "    subj = URIRef(f\"urn:uuid:{data.get('run_id', 'unknown-run')}\")\n",
    "\n",
    "    for key, value in flat_data.items():\n",
    "        if key in field_mapping:\n",
    "            pred = URIRef(field_mapping[key].replace(\"prov:\", str(prov)))\n",
    "            if isinstance(value, (int, float)):\n",
    "                obj = Literal(value)\n",
    "            else:\n",
    "                obj = Literal(str(value))\n",
    "            g.add((subj, pred, obj))\n",
    "        else:\n",
    "            # Keys that don't have mapping: (skip or warn)\n",
    "            pass\n",
    "\n",
    "    return g\n",
    "\n",
    "# 🔥 Process all run summaries\n",
    "os.makedirs(\"rdf_exports\", exist_ok=True)\n",
    "\n",
    "for json_file in glob.glob(\"MODEL_PROVENANCE/*/*_run_summary.json\"):\n",
    "    model_name = os.path.basename(json_file).replace(\"_run_summary.json\", \"\")\n",
    "\n",
    "    rdf_graph = create_rdf_from_json(json_file)\n",
    "\n",
    "    # Save as RDF/XML\n",
    "    out_path = os.path.join(\"rdf_exports\", f\"{model_name}.rdf\")\n",
    "    rdf_graph.serialize(destination=out_path, format=\"xml\")\n",
    "\n",
    "    print(f\"✅ RDF/XML created: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab0122-20c3-4f9d-8f8c-476b6bef5529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "580a6eff-2a2d-4bb7-9e6e-7804e8c1c30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fpdf\n",
      "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [1 lines of output]\n",
      "  ERROR: Can not execute `setup.py` since setuptools is not available in the build environment.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install fpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "220210bd-dbc2-4b37-a6de-5a3684640d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\reema\\anaconda3\\lib\\site-packages (23.2.1)\n",
      "Collecting pip\n",
      "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/c9/bc/b7db44f5f39f9d0494071bddae6880eb645970366d0a200022a1a93d57f5/pip-25.0.1-py3-none-any.whl.metadata\n",
      "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\reema\\anaconda3\\lib\\site-packages (78.1.0)\n",
      "Collecting setuptools\n",
      "  Obtaining dependency information for setuptools from https://files.pythonhosted.org/packages/0d/6d/b4752b044bf94cb802d88a888dc7d288baaf77d7910b7dedda74b5ceea0c/setuptools-79.0.1-py3-none-any.whl.metadata\n",
      "  Downloading setuptools-79.0.1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.8 MB 2.0 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.5/1.8 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.1/1.8 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 10.6 MB/s eta 0:00:00\n",
      "Downloading setuptools-79.0.1-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 0.8/1.3 MB 17.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 16.0 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\reema\\anaconda3\\python.exe -m pip install --upgrade pip setuptools\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Obtaining dependency information for python-docx from https://files.pythonhosted.org/packages/3e/3d/330d9efbdb816d3f60bf2ad92f05e1708e4a1b9abe80461ac3444c83f749/python_docx-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "   ---------------------------------------- 0.0/244.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/244.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/244.3 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 30.7/244.3 kB ? eta -:--:--\n",
      "   -------------------------------------- - 235.5/244.3 kB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 244.3/244.3 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-1.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools\n",
    "!pip install python-docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afb5f5c1-acf9-4e6d-b497-2358e866d5cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fpdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfpdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FPDF\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create Word Document\u001b[39;00m\n\u001b[0;32m      5\u001b[0m doc \u001b[38;5;241m=\u001b[39m Document()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fpdf'"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "\n",
    "# Create Word Document\n",
    "doc = Document()\n",
    "doc.add_heading('Reema Dass', 0)\n",
    "doc.add_paragraph('Data Scientist | Data Engineer | Data Analyst\\n')\n",
    "doc.add_paragraph('Email: reema.g.dass@gmail.com | Phone: +43 676 7317181 | Location: Vienna, Austria')\n",
    "doc.add_paragraph('LinkedIn | GitHub | Website\\n')\n",
    "\n",
    "doc.add_heading('Profile Summary:', level=1)\n",
    "doc.add_paragraph(\n",
    "    \"Master's student in Data Science with four years of professional experience in data engineering, \"\n",
    "    \"cloud computing, and machine learning applications. Skilled at transforming complex data into actionable \"\n",
    "    \"insights through innovative, scalable solutions. Passionate about solving real-world problems using sophisticated \"\n",
    "    \"analytics and driving meaningful impact across organizations.\"\n",
    ")\n",
    "\n",
    "doc.add_heading('Education:', level=1)\n",
    "doc.add_paragraph('Master\\'s in Data Science\\nTechnical University of Vienna, Austria | Mar 2022 – Present\\n- Majoring in Machine Learning and Visual Analytics')\n",
    "doc.add_paragraph('Bachelor\\'s in Computer Science\\nVisvesvaraya Technological University (VTU), India | Sep 2013 – Dec 2017\\n- Thesis: Image Processing for Medical Disease Diagnosis')\n",
    "\n",
    "doc.add_heading('Professional Experience:', level=1)\n",
    "experiences = [\n",
    "    (\"Data Science Research Intern (FFG-Femtech)\", \"SBA Research, Vienna | Sep 2023 – Feb 2024\",\n",
    "     \"- Investigated security improvements for edge-deployed Deep Neural Networks (DNNs), proposing alternative obfuscation techniques that reduced vulnerability risk by 10%.\\n\"\n",
    "     \"- Evaluated Trusted Execution Environments (TEEs) and encryption methods to validate and enhance data security.\\n\"\n",
    "     \"- Developed obfuscation methods that decreased reverse engineering risk by 25%.\"),\n",
    "    (\"Software Developer\", \"Nextpart, Linz | Nov 2022 – Jul 2023\",\n",
    "     \"- Enhanced a malicious analysis tool, improving threat detection accuracy by 20%.\\n\"\n",
    "     \"- Integrated external software connectors, boosting issue identification and evaluation by 30%.\\n\"\n",
    "     \"- Conducted vulnerability testing, increasing overall system resilience.\"),\n",
    "    (\"Cloud Consultant\", \"Deloitte, India | Aug 2021 – Feb 2022\",\n",
    "     \"- Designed and implemented PII redaction and anonymization solutions, reducing exposure risk by 85%.\\n\"\n",
    "     \"- Engineered optimized AWS Fargate data pipelines, achieving a 30% processing speed increase and 20% cost reduction.\\n\"\n",
    "     \"- Collaborated with cross-functional teams to attain a 95% data privacy compliance rate.\"),\n",
    "    (\"Data Engineer\", \"Stats Perform, India | Mar 2020 – Jul 2021\",\n",
    "     \"- Processed and analyzed over 10 TB of sports analytics data using AWS Glue, Spark, Redshift, and DynamoDB.\\n\"\n",
    "     \"- Integrated AWS services with external APIs, enhancing pipeline scalability by 60% and cutting data retrieval time by 35%.\\n\"\n",
    "     \"- Built resilient, dynamic-load handling ETL pipelines, improving system uptime by 25%.\"),\n",
    "    (\"Full-Stack Developer\", \"Infosys Pvt Ltd, India | Dec 2018 – Feb 2020\",\n",
    "     \"- Led backend development for Belgium Post, reducing query response time by 50% and improving storage efficiency by 40% using Azure Blob Storage, SQL, and C#.\\n\"\n",
    "     \"- Developed dynamic, multilingual Angular UI to enhance user experience and accessibility.\")\n",
    "]\n",
    "for role, company, details in experiences:\n",
    "    doc.add_paragraph(f'{role}\\n{company}\\n{details}')\n",
    "\n",
    "doc.add_heading('Skills:', level=1)\n",
    "doc.add_paragraph('- Programming: Python (Pandas, NumPy), R, SQL, Scala\\n'\n",
    "                  '- Machine Learning: TensorFlow, scikit-learn, PyTorch, Keras, Random Forests, SVM, Gradient Boosting, Time Series Analysis, Hypothesis Testing\\n'\n",
    "                  '- Data Engineering: Hadoop, Spark, Kafka, MongoDB, Redshift, AWS Glue, S3\\n'\n",
    "                  '- Visualization: Tableau, Power BI, Excel, ggplot2, Matplotlib, Plotly\\n'\n",
    "                  '- Cloud Platforms: AWS (S3, Glue, EMR, SageMaker), Azure (Data Factory, Machine Learning)\\n'\n",
    "                  '- DevOps: Docker, Kubernetes')\n",
    "\n",
    "doc.add_heading('Certifications:', level=1)\n",
    "doc.add_paragraph('- AWS Certified Cloud Practitioner\\n'\n",
    "                  '- Microsoft Certified: Azure Developer Associate\\n'\n",
    "                  '- English (C1 Level)\\n'\n",
    "                  '- German (B1 Level)')\n",
    "\n",
    "doc.add_heading('Projects:', level=1)\n",
    "projects = [\n",
    "    (\"Testing Identity Theft\",\n",
    "     \"Tools: Transfer Learning, Record Linkage, Differential Privacy, ARX Data Anonymization\\n- Conducted ethical Record Linkage attacks to evaluate data vulnerability.\"),\n",
    "    (\"Reinforcement Learning-based Breakout Game\",\n",
    "     \"Tools: Pygame, reward/penalty tuning, evaluated across thousands of iterations.\"),\n",
    "    (\"Sentiment Analysis on Twitter Feeds\",\n",
    "     \"Tools: NLP (BERT, GPT), Tableau, NLTK, Python.\"),\n",
    "    (\"Influenza Prediction\",\n",
    "     \"Tools: Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn.\")\n",
    "]\n",
    "for project, desc in projects:\n",
    "    doc.add_paragraph(f'{project}\\n{desc}')\n",
    "\n",
    "doc.add_heading('Languages:', level=1)\n",
    "doc.add_paragraph('- English (Fluent - C1)\\n- German (Intermediate - B1)')\n",
    "\n",
    "# Save Word Document\n",
    "word_path = '/mnt/data/Reema_Dass_CV_Enhanced.docx'\n",
    "doc.save(word_path)\n",
    "\n",
    "# Create PDF\n",
    "class PDF(FPDF):\n",
    "    def header(self):\n",
    "        self.set_font('Arial', 'B', 16)\n",
    "        self.cell(0, 10, 'Reema Dass', ln=True, align='C')\n",
    "        self.set_font('Arial', '', 12)\n",
    "        self.cell(0, 10, 'Data Scientist | Data Engineer | Data Analyst', ln=True, align='C')\n",
    "        self.ln(10)\n",
    "\n",
    "pdf = PDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "sections = [\n",
    "    (\"Profile Summary\", \"Master's student in Data Science with four years of professional experience in data engineering, cloud computing, and machine learning applications. Skilled at transforming complex data into actionable insights through innovative, scalable solutions. Passionate about solving real-world problems using sophisticated analytics and driving meaningful impact across organizations.\"),\n",
    "    (\"Education\", \"Master's in Data Science\\nTechnical University of Vienna, Austria | Mar 2022 – Present\\n- Majoring in Machine Learning and Visual Analytics\\n\\nBachelor's in Computer Science\\nVisvesvaraya Technological University (VTU), India | Sep 2013 – Dec 2017\\n- Thesis: Image Processing for Medical Disease Diagnosis\"),\n",
    "    (\"Professional Experience\", \"\\n\\n\".join([f\"{role}\\n{company}\\n{details}\" for role, company, details in experiences])),\n",
    "    (\"Skills\", \"Programming: Python (Pandas, NumPy), R, SQL, Scala\\nMachine Learning: TensorFlow, scikit-learn, PyTorch, Keras, Random Forests, SVM, Gradient Boosting, Time Series Analysis, Hypothesis Testing\\nData Engineering: Hadoop, Spark, Kafka, MongoDB, Redshift, AWS Glue, S3\\nVisualization: Tableau, Power BI, Excel, ggplot2, Matplotlib, Plotly\\nCloud Platforms: AWS (S3, Glue, EMR, SageMaker), Azure (Data Factory, Machine Learning)\\nDevOps: Docker, Kubernetes\"),\n",
    "    (\"Certifications\", \"AWS Certified Cloud Practitioner\\nMicrosoft Certified: Azure Developer Associate\\nEnglish (C1 Level)\\nGerman (B1 Level)\"),\n",
    "    (\"Projects\", \"\\n\\n\".join([f\"{project}\\n{desc}\" for project, desc in projects])),\n",
    "    (\"Languages\", \"English (Fluent - C1)\\nGerman (Intermediate - B1)\")\n",
    "]\n",
    "\n",
    "for title, content in sections:\n",
    "    pdf.set_font('Arial', 'B', 14)\n",
    "    pdf.cell(0, 10, title, ln=True)\n",
    "    pdf.set_font('Arial', '', 12)\n",
    "    for line in content.split('\\n'):\n",
    "        pdf.multi_cell(0, 10, line)\n",
    "    pdf.ln(5)\n",
    "\n",
    "pdf_path = '/mnt/data/Reema_Dass_CV_Enhanced.pdf'\n",
    "pdf.output(pdf_path)\n",
    "\n",
    "word_path, pdf_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4d10a-b080-4c7c-a85a-1be63c09fda9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
