{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ff24f2a-931c-4543-b179-dc8bf1bfd74d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "########################################################\n",
    "# EXPERIMENT CODE\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f97a375-387a-4368-a629-556f56f51dcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import platform\n",
    "import sys\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "def prompt_if_none(env_key, prompt_text, default_value=\"unknown\"):\n",
    "    val = os.getenv(env_key)\n",
    "    if not val:\n",
    "        try:\n",
    "            val = input(f\"{prompt_text} (default: {default_value}): \").strip() or default_value\n",
    "        except Exception:\n",
    "            val = default_value\n",
    "    return val\n",
    "\n",
    "def collect_session_metadata(\n",
    "    prompt_fields=True,\n",
    "    fixed_role=None,\n",
    "    fixed_project_id=None\n",
    "):\n",
    "    session_id = str(uuid.uuid4())\n",
    "    \n",
    "    session_metadata = {\n",
    "        \"session_id\": session_id,\n",
    "        \"username\": os.getenv(\"JUPYTERHUB_USER\", getpass.getuser()),\n",
    "        \"timestamp_utc\": datetime.utcnow().isoformat(),\n",
    "        \"hostname\": platform.node(),\n",
    "        \"platform\": platform.system(),\n",
    "        \"os_version\": platform.version(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "    }\n",
    "\n",
    "    # Prompt or use defaults\n",
    "    session_metadata[\"role\"] = fixed_role or (\n",
    "        prompt_if_none(\"RESEARCHER_ROLE\", \"Enter your role\", \"collaborator\") if prompt_fields \n",
    "        else os.getenv(\"RESEARCHER_ROLE\", \"researcher\")\n",
    "    )\n",
    "    session_metadata[\"project_id\"] = fixed_project_id or (\n",
    "        prompt_if_none(\"PROJECT_ID\", \"Enter project ID\", \"default_project\") if prompt_fields \n",
    "        else os.getenv(\"PROJECT_ID\", \"default_project\")\n",
    "    )\n",
    "\n",
    "    print(\"\\nüìå Session Metadata:\")\n",
    "    for k, v in session_metadata.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    return session_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca4f0ae-39ee-4b22-b013-dfb1fa1b5694",
   "metadata": {},
   "source": [
    "LIBRARY IMPORTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca332e5-6501-4310-920b-2b769477b46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# üì¶ Standard Library Imports\n",
    "# ============================\n",
    "import os\n",
    "import glob\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import ast\n",
    "import pickle\n",
    "import platform\n",
    "import subprocess\n",
    "from datetime import datetime, timezone\n",
    "from pprint import pprint\n",
    "from typing import List, Dict, Any\n",
    "import xml.etree.ElementTree as ET\n",
    "import urllib.parse\n",
    "import yaml\n",
    "\n",
    "# ============================\n",
    "# üìä Data and Visualization\n",
    "# ============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================\n",
    "# ü§ñ Machine Learning\n",
    "# ============================\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    RocCurveDisplay,\n",
    "    PrecisionRecallDisplay\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "# ============================\n",
    "# üî¨ Experiment Tracking\n",
    "# ============================\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# ============================\n",
    "# üåê Web / API / Networking\n",
    "# ============================\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ============================\n",
    "# üß™ Git & Version Control\n",
    "# ============================\n",
    "import git\n",
    "from git import Repo, GitCommandError\n",
    "import hashlib\n",
    "\n",
    "\n",
    "# ============================\n",
    "# üß† SHAP for Explainability\n",
    "# ============================\n",
    "import shap\n",
    "\n",
    "# ============================\n",
    "# üß¨ RDF & Provenance (rdflib)\n",
    "# ============================\n",
    "from rdflib import Graph, URIRef, Literal\n",
    "from rdflib.namespace import PROV, XSD\n",
    "\n",
    "# ============================\n",
    "# ‚öôÔ∏è System Monitoring\n",
    "# ============================\n",
    "import psutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfde18b5-ae9c-442c-a5b3-7dfb06957646",
   "metadata": {},
   "source": [
    "#Dataset metadata!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a394398-cd25-45b5-89ac-6d909b65d417",
   "metadata": {},
   "source": [
    "#Metadata from ZONEDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba00ff-139b-4433-ab7d-2170cd137012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c3e945-508a-4c1c-8bb1-c1b5d9121615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def extract_dataset_metadata_from_doi(doi: str) -> dict:\n",
    "    base_url = f\"https://api.datacite.org/dois/{doi.lower()}\"\n",
    "    r = requests.get(base_url)\n",
    "    r.raise_for_status()\n",
    "    meta = r.json().get(\"data\", {}).get(\"attributes\", {})\n",
    "\n",
    "    # Extract fields\n",
    "    title = meta.get(\"titles\", [{}])[0].get(\"title\", \"info not available\")\n",
    "    creators = [c.get(\"name\", \"\") for c in meta.get(\"creators\", [])]\n",
    "    publisher = meta.get(\"publisher\", \"info not available\")\n",
    "    pub_year = meta.get(\"publicationYear\", \"info not available\")\n",
    "    url = meta.get(\"url\", f\"https://doi.org/{doi}\")\n",
    "\n",
    "    dataset_metadata = {\n",
    "        \"dataset_id\": doi,\n",
    "        \"dataset_title\": title,\n",
    "        \"dataset_description\": meta.get(\"descriptions\", \"info not available\"),\n",
    "        \"dataset_creator\": \", \".join(creators) if creators else \"info not available\",\n",
    "        \"dataset_publisher\": publisher,\n",
    "        \"dataset_publication_date\": pub_year,\n",
    "        \"dataset_version\": meta.get(\"version\", \"info not available\"),\n",
    "        \"dataset_license\": meta.get(\"rightsList\", \"info not available\"),\n",
    "        \"dataset_keywords\": \"info not available\",  # not always exposed\n",
    "        \"dataset_access_url\": url,\n",
    "        \"dataset_documentation\": url,\n",
    "        \"metadata_standard\": meta.get(\"types\", {}).get(\"resourceTypeGeneral\", \"info not available\"),\n",
    "        \"related_resources\": url,\n",
    "\n",
    "        # PROV-O traceability fields\n",
    "        \"prov_entity\": title,\n",
    "        \"prov_activity\": \"Ingestion and Publication\",\n",
    "        \"prov_agent_dataset_creator\": \", \".join(creators) if creators else \"info not available\",\n",
    "        \"prov_used\": url,\n",
    "        \"prov_wasDerivedFrom\": doi,\n",
    "        \"prov_wasAttributedTo\": \", \".join(creators) if creators else \"info not available\",\n",
    "        \"prov_startedAtTime\": pub_year,\n",
    "        \"prov_role_dataset_creator\": \"Original Data Author\",\n",
    "        \"prov_role_database_creator\": \"Database Ingestor and Maintainer\"\n",
    "    }\n",
    "\n",
    "    return dataset_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "529b2898-a4e7-4f5b-a1eb-c44f78620414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_dataset_metadata_from_doi(\"10.24432/C56C76\") #dataset related metadata logging \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ee5cf89-7c47-4601-b57c-04415d5966c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "DB_API = \"http://localhost/api/database/{db_id}\"\n",
    "HISTORY_API = \"http://localhost/api/database/{db_id}/table/{table_id}/history\"\n",
    "\n",
    "def fetch_db_dataset_metadata(\n",
    "    db_id: str,\n",
    "    table_id: str,\n",
    "    selected_version: str,\n",
    "    target_variable: str,\n",
    "    num_samples: int\n",
    ") -> dict:\n",
    "    try:\n",
    "        # Fetch main DB metadata\n",
    "        db_url = DB_API.format(db_id=db_id)\n",
    "        db_response = requests.get(db_url)\n",
    "        db_response.raise_for_status()\n",
    "        db_data = db_response.json()\n",
    "        print(db_data)\n",
    "\n",
    "        # Fetch table history metadata\n",
    "        history_url = HISTORY_API.format(db_id=db_id, table_id=table_id)\n",
    "        history_response = requests.get(history_url)\n",
    "        timestamp = \"info not available\"\n",
    "        if history_response.status_code == 200:\n",
    "            history_data = history_response.json()\n",
    "            print(history_data)\n",
    "            if isinstance(history_data, list) and len(history_data) > 0:\n",
    "                timestamp = history_data[0].get(\"timestamp\", timestamp)\n",
    "\n",
    "        # Build flat metadata structure for DB storage\n",
    "        dataset_metadata = {\n",
    "            # Basic identity\n",
    "            \"dataset_id\": table_id,\n",
    "            \"dataset_name\": next(\n",
    "                (t.get(\"name\") for t in db_data.get(\"tables\", []) if t.get(\"id\") == table_id),\n",
    "                \"table name not available\"\n",
    "            ),\n",
    "            \"dataset_version\": selected_version,\n",
    "            \"dataset_title\": db_data.get(\"name\", \"info not available\"),\n",
    "            \"dataset_description\": db_data.get(\"description\", \"info not available\"),\n",
    "\n",
    "            # Ownership and access\n",
    "            \"dataset_creator\": \"info not available\",\n",
    "            \"dataset_publisher\": db_data.get(\"owner\", {}).get(\"name\", \"info not available\"),\n",
    "            \"dataset_access_url\": db_url,\n",
    "            \"dataset_publication_date\": timestamp,\n",
    "            \"dataset_license\": \"info not available\",\n",
    "\n",
    "            # Structure\n",
    "            \"columns\": db_data.get(\"columns\", \"info not available\"),\n",
    "            \"dataset_dataset_type\": \"tabular\",\n",
    "            \"target_variable\": target_variable,\n",
    "            \"ml_task\": \"classification\",\n",
    "            \"num_samples\": num_samples,\n",
    "\n",
    "            # FAIR4ML placeholders\n",
    "            \"data_distribution\": \"info not available\",\n",
    "            \"known_issues\": \"info not available\",\n",
    "            \"trainedOn\": \"info not available\",\n",
    "            \"testedOn\": \"info not available\",\n",
    "            \"validatedOn\": \"info not available\",\n",
    "            \"modelRisks\": \"info not available\",\n",
    "            \"usageInstructions\": \"info not available\",\n",
    "            \"ethicalLegalSocial\": \"info not available\",\n",
    "\n",
    "            # PROV-style fields\n",
    "            \"prov_entity\": db_data.get(\"name\", \"info not available\"),\n",
    "            \"prov_activity\": \"Ingestion and Publication\",\n",
    "            \"prov_agent_dataset_creator\": \"info not available\",\n",
    "            \"prov_agent_database_creator\": db_data.get('owner', {}).get('name', 'info not available'),\n",
    "            \"prov_wasGeneratedBy\": db_data.get('owner', {}).get('name', 'info not available'),\n",
    "            \"prov_used\": db_url,\n",
    "            \"prov_wasDerivedFrom\": \"info not available\",\n",
    "            \"prov_wasAttributedTo\": \"info not available\",\n",
    "            \"prov_wasAssociatedWith\": db_data.get('owner', {}).get('name', 'info not available'),\n",
    "            \"prov_startedAtTime\": \"info not available\",\n",
    "            \"prov_endedAtTime\": timestamp,\n",
    "            \"prov_location\": db_url,\n",
    "            \"prov_role_dataset_creator\": \"\",\n",
    "            \"prov_role_database_creator\": \"Database Ingestor and Maintainer\"\n",
    "        }\n",
    "\n",
    "        return dataset_metadata\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[‚ö†Ô∏è Error] Failed to fetch DB metadata for {db_id}: {e}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b61178-0a4a-48d5-8b6f-737104605005",
   "metadata": {},
   "source": [
    "Fetch info needed to fetch metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91e0611d-d579-485a-ac24-094c1890bc2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select dataset version:\n",
      "  v0 - Original\n",
      "  v1 - Duplicated\n",
      "  v2 - First 100\n",
      "  v3 - Shuffled\n",
      "  v4 - Normalized\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter version (v0‚Äìv4):  v4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ You selected version 'v4' ‚Üí Table ID: 3cb219b2-8cc6-4698-b69f-213deacc763c\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mapping of version tags to table UUIDs\n",
    "version_to_table_id = {\n",
    "    \"v0\": \"519eb3fc-687c-4791-aa13-96d5bee8cbad\",  # Original\n",
    "    \"v1\": \"3fd0f36e-572e-4f99-841b-a8381a052a97\",  # Duplicated\n",
    "    \"v2\": \"2a8083fa-8270-49c1-80ea-86ce6bf39977\",  # First 100\n",
    "    \"v3\": \"14cc6f38-b5c6-4225-83ce-3dc92b7c045a\",  # Shuffled\n",
    "    \"v4\": \"3cb219b2-8cc6-4698-b69f-213deacc763c\"   # Normalized\n",
    "}\n",
    "\n",
    "db_id = \"4bd4ddc7-378c-4ffa-8bdb-0bf8969c80a1\"  # Static DB ID\n",
    "\n",
    "def select_dataset_version():\n",
    "    print(\"Select dataset version:\")\n",
    "    print(\"  v0 - Original\")\n",
    "    print(\"  v1 - Duplicated\")\n",
    "    print(\"  v2 - First 100\")\n",
    "    print(\"  v3 - Shuffled\")\n",
    "    print(\"  v4 - Normalized\")\n",
    "    \n",
    "    selected_version = input(\"Enter version (v0‚Äìv4): \").strip().lower()\n",
    "    \n",
    "    if selected_version not in version_to_table_id:\n",
    "        raise ValueError(f\"‚ùå Invalid version selected: {selected_version}\")\n",
    "    \n",
    "    selected_table_id = version_to_table_id[selected_version]\n",
    "    \n",
    "    print(f\"\\n‚úÖ You selected version '{selected_version}' ‚Üí Table ID: {selected_table_id}\\n\")\n",
    "    \n",
    "    return selected_version, selected_table_id\n",
    "\n",
    "# Usage: #TODO CALL\n",
    "selected_version, selected_table_id = select_dataset_version()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aab28b04-4db7-43bf-8320-f6382120984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import mlflow\n",
    "# \n",
    "def log_metadata_dict_to_mlflow(metadata: dict, prefix: str = \"\", snapshot_name: str = \"metadata_snapshot.json\"):\n",
    "    \"\"\"\n",
    "    Logs a flat metadata dictionary to MLflow:\n",
    "    - Adds prefix to each key if provided (e.g., \"session_\")\n",
    "    - Skips empty values\n",
    "    - Logs a full JSON artifact for traceability\n",
    "    \"\"\"\n",
    "    \n",
    "    def safe_tag(key, value):\n",
    "        if not mlflow.active_run():\n",
    "            raise RuntimeError(\"‚ùå No active MLflow run.\")\n",
    "        \n",
    "        key_clean = key.replace(\":\", \"_\").replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "        try:\n",
    "            val_str = json.dumps(value) if isinstance(value, (dict, list)) else str(value)\n",
    "            if len(val_str) > 5000:\n",
    "                val_str = val_str[:5000] + \"...[TRUNCATED]\"\n",
    "            if len(key_clean) > 255:\n",
    "                print(f\"‚ö†Ô∏è Skipped tag (key too long): {key_clean}\")\n",
    "                return\n",
    "            mlflow.set_tag(key_clean, val_str)\n",
    "            print(f\"‚úÖ Logged tag: {key_clean}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[‚ö†Ô∏è Error logging tag] {key_clean}: {e}\")\n",
    "\n",
    "    for key, value in metadata.items():\n",
    "        if value not in [None, \"\"]:\n",
    "            full_key = f\"{prefix}{key}\" if prefix else key\n",
    "            safe_tag(full_key, value)\n",
    "\n",
    "    # Save full metadata snapshot as JSON artifact\n",
    "    os.makedirs(\"metadata\", exist_ok=True)\n",
    "    full_path = os.path.join(\"metadata\", snapshot_name)\n",
    "    with open(full_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    mlflow.log_artifact(full_path, artifact_path=\"metadata\")\n",
    "    print(f\"üìÅ Full metadata snapshot logged as: {snapshot_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063803f5-8a93-420a-8a98-d2b5371c9574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61d4d6b8-34a9-47b5-974d-5927c0ee2256",
   "metadata": {},
   "source": [
    "DBREPO INTEGRETION: API call to fetch the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e3570e2-9a60-45b4-8653-28060071e728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '1', 'sepallengthcm': '0.222222222222222100', 'sepalwidthcm': '0.625000000000000000', 'petallengthcm': '0.067796610169491510', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '2', 'sepallengthcm': '0.166666666666666740', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.067796610169491510', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '3', 'sepallengthcm': '0.111111111111111160', 'sepalwidthcm': '0.500000000000000000', 'petallengthcm': '0.050847457627118650', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '4', 'sepallengthcm': '0.083333333333333260', 'sepalwidthcm': '0.458333333333333260', 'petallengthcm': '0.084745762711864400', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '5', 'sepallengthcm': '0.194444444444444420', 'sepalwidthcm': '0.666666666666666700', 'petallengthcm': '0.067796610169491510', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '6', 'sepallengthcm': '0.305555555555555600', 'sepalwidthcm': '0.791666666666666500', 'petallengthcm': '0.118644067796610130', 'petalwidthcm': '0.125000000000000000', 'species': 'Iris-setosa'}, {'id': '7', 'sepallengthcm': '0.083333333333333260', 'sepalwidthcm': '0.583333333333333300', 'petallengthcm': '0.067796610169491510', 'petalwidthcm': '0.083333333333333330', 'species': 'Iris-setosa'}, {'id': '8', 'sepallengthcm': '0.194444444444444420', 'sepalwidthcm': '0.583333333333333300', 'petallengthcm': '0.084745762711864400', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '9', 'sepallengthcm': '0.027777777777777900', 'sepalwidthcm': '0.375000000000000000', 'petallengthcm': '0.067796610169491510', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '10', 'sepallengthcm': '0.166666666666666740', 'sepalwidthcm': '0.458333333333333260', 'petallengthcm': '0.084745762711864400', 'petalwidthcm': '0E-18', 'species': 'Iris-setosa'}, {'id': '11', 'sepallengthcm': '0.305555555555555600', 'sepalwidthcm': '0.708333333333333300', 'petallengthcm': '0.084745762711864400', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '12', 'sepallengthcm': '0.138888888888888840', 'sepalwidthcm': '0.583333333333333300', 'petallengthcm': '0.101694915254237300', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '13', 'sepallengthcm': '0.138888888888888840', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.067796610169491510', 'petalwidthcm': '0E-18', 'species': 'Iris-setosa'}, {'id': '14', 'sepallengthcm': '0E-18', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.016949152542372890', 'petalwidthcm': '0E-18', 'species': 'Iris-setosa'}, {'id': '15', 'sepallengthcm': '0.416666666666666500', 'sepalwidthcm': '0.833333333333333300', 'petallengthcm': '0.033898305084745756', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '16', 'sepallengthcm': '0.388888888888888840', 'sepalwidthcm': '1.000000000000000000', 'petallengthcm': '0.084745762711864400', 'petalwidthcm': '0.125000000000000000', 'species': 'Iris-setosa'}, {'id': '17', 'sepallengthcm': '0.305555555555555600', 'sepalwidthcm': '0.791666666666666500', 'petallengthcm': '0.050847457627118650', 'petalwidthcm': '0.125000000000000000', 'species': 'Iris-setosa'}, {'id': '18', 'sepallengthcm': '0.222222222222222100', 'sepalwidthcm': '0.625000000000000000', 'petallengthcm': '0.067796610169491510', 'petalwidthcm': '0.083333333333333330', 'species': 'Iris-setosa'}, {'id': '19', 'sepallengthcm': '0.388888888888888840', 'sepalwidthcm': '0.749999999999999800', 'petallengthcm': '0.118644067796610130', 'petalwidthcm': '0.083333333333333330', 'species': 'Iris-setosa'}, {'id': '20', 'sepallengthcm': '0.222222222222222100', 'sepalwidthcm': '0.749999999999999800', 'petallengthcm': '0.084745762711864400', 'petalwidthcm': '0.083333333333333330', 'species': 'Iris-setosa'}, {'id': '21', 'sepallengthcm': '0.305555555555555600', 'sepalwidthcm': '0.583333333333333300', 'petallengthcm': '0.118644067796610130', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '22', 'sepallengthcm': '0.222222222222222100', 'sepalwidthcm': '0.708333333333333300', 'petallengthcm': '0.084745762711864400', 'petalwidthcm': '0.125000000000000000', 'species': 'Iris-setosa'}, {'id': '23', 'sepallengthcm': '0.083333333333333260', 'sepalwidthcm': '0.666666666666666700', 'petallengthcm': '0E-18', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '24', 'sepallengthcm': '0.222222222222222100', 'sepalwidthcm': '0.541666666666666500', 'petallengthcm': '0.118644067796610130', 'petalwidthcm': '0.166666666666666690', 'species': 'Iris-setosa'}, {'id': '25', 'sepallengthcm': '0.138888888888888840', 'sepalwidthcm': '0.583333333333333300', 'petallengthcm': '0.152542372881355910', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '26', 'sepallengthcm': '0.194444444444444420', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.101694915254237300', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '27', 'sepallengthcm': '0.194444444444444420', 'sepalwidthcm': '0.583333333333333300', 'petallengthcm': '0.101694915254237300', 'petalwidthcm': '0.125000000000000000', 'species': 'Iris-setosa'}, {'id': '28', 'sepallengthcm': '0.250000000000000000', 'sepalwidthcm': '0.625000000000000000', 'petallengthcm': '0.084745762711864400', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '29', 'sepallengthcm': '0.250000000000000000', 'sepalwidthcm': '0.583333333333333300', 'petallengthcm': '0.067796610169491510', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '30', 'sepallengthcm': '0.111111111111111160', 'sepalwidthcm': '0.500000000000000000', 'petallengthcm': '0.101694915254237300', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '31', 'sepallengthcm': '0.138888888888888840', 'sepalwidthcm': '0.458333333333333260', 'petallengthcm': '0.101694915254237300', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '32', 'sepallengthcm': '0.305555555555555600', 'sepalwidthcm': '0.583333333333333300', 'petallengthcm': '0.084745762711864400', 'petalwidthcm': '0.125000000000000000', 'species': 'Iris-setosa'}, {'id': '33', 'sepallengthcm': '0.250000000000000000', 'sepalwidthcm': '0.874999999999999800', 'petallengthcm': '0.084745762711864400', 'petalwidthcm': '0E-18', 'species': 'Iris-setosa'}, {'id': '34', 'sepallengthcm': '0.333333333333333260', 'sepalwidthcm': '0.916666666666666700', 'petallengthcm': '0.067796610169491510', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '35', 'sepallengthcm': '0.166666666666666740', 'sepalwidthcm': '0.458333333333333260', 'petallengthcm': '0.084745762711864400', 'petalwidthcm': '0E-18', 'species': 'Iris-setosa'}, {'id': '36', 'sepallengthcm': '0.194444444444444420', 'sepalwidthcm': '0.500000000000000000', 'petallengthcm': '0.033898305084745756', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '37', 'sepallengthcm': '0.333333333333333260', 'sepalwidthcm': '0.625000000000000000', 'petallengthcm': '0.050847457627118650', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '38', 'sepallengthcm': '0.166666666666666740', 'sepalwidthcm': '0.458333333333333260', 'petallengthcm': '0.084745762711864400', 'petalwidthcm': '0E-18', 'species': 'Iris-setosa'}, {'id': '39', 'sepallengthcm': '0.027777777777777900', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.050847457627118650', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '40', 'sepallengthcm': '0.222222222222222100', 'sepalwidthcm': '0.583333333333333300', 'petallengthcm': '0.084745762711864400', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '41', 'sepallengthcm': '0.194444444444444420', 'sepalwidthcm': '0.625000000000000000', 'petallengthcm': '0.050847457627118650', 'petalwidthcm': '0.083333333333333330', 'species': 'Iris-setosa'}, {'id': '42', 'sepallengthcm': '0.055555555555555580', 'sepalwidthcm': '0.124999999999999890', 'petallengthcm': '0.050847457627118650', 'petalwidthcm': '0.083333333333333330', 'species': 'Iris-setosa'}, {'id': '43', 'sepallengthcm': '0.027777777777777900', 'sepalwidthcm': '0.500000000000000000', 'petallengthcm': '0.050847457627118650', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '44', 'sepallengthcm': '0.194444444444444420', 'sepalwidthcm': '0.625000000000000000', 'petallengthcm': '0.101694915254237300', 'petalwidthcm': '0.208333333333333310', 'species': 'Iris-setosa'}, {'id': '45', 'sepallengthcm': '0.222222222222222100', 'sepalwidthcm': '0.749999999999999800', 'petallengthcm': '0.152542372881355910', 'petalwidthcm': '0.125000000000000000', 'species': 'Iris-setosa'}, {'id': '46', 'sepallengthcm': '0.138888888888888840', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.067796610169491510', 'petalwidthcm': '0.083333333333333330', 'species': 'Iris-setosa'}, {'id': '47', 'sepallengthcm': '0.222222222222222100', 'sepalwidthcm': '0.749999999999999800', 'petallengthcm': '0.101694915254237300', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '48', 'sepallengthcm': '0.083333333333333260', 'sepalwidthcm': '0.500000000000000000', 'petallengthcm': '0.067796610169491510', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '49', 'sepallengthcm': '0.277777777777777700', 'sepalwidthcm': '0.708333333333333300', 'petallengthcm': '0.084745762711864400', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '50', 'sepallengthcm': '0.194444444444444420', 'sepalwidthcm': '0.541666666666666500', 'petallengthcm': '0.067796610169491510', 'petalwidthcm': '0.041666666666666670', 'species': 'Iris-setosa'}, {'id': '51', 'sepallengthcm': '0.750000000000000000', 'sepalwidthcm': '0.500000000000000000', 'petallengthcm': '0.627118644067796600', 'petalwidthcm': '0.541666666666666700', 'species': 'Iris-versicolor'}, {'id': '52', 'sepallengthcm': '0.583333333333333500', 'sepalwidthcm': '0.500000000000000000', 'petallengthcm': '0.593220338983050800', 'petalwidthcm': '0.583333333333333400', 'species': 'Iris-versicolor'}, {'id': '53', 'sepallengthcm': '0.722222222222222300', 'sepalwidthcm': '0.458333333333333260', 'petallengthcm': '0.661016949152542400', 'petalwidthcm': '0.583333333333333400', 'species': 'Iris-versicolor'}, {'id': '54', 'sepallengthcm': '0.333333333333333260', 'sepalwidthcm': '0.124999999999999890', 'petallengthcm': '0.508474576271186400', 'petalwidthcm': '0.500000000000000100', 'species': 'Iris-versicolor'}, {'id': '55', 'sepallengthcm': '0.611111111111111200', 'sepalwidthcm': '0.333333333333333260', 'petallengthcm': '0.610169491525423700', 'petalwidthcm': '0.583333333333333400', 'species': 'Iris-versicolor'}, {'id': '56', 'sepallengthcm': '0.388888888888888840', 'sepalwidthcm': '0.333333333333333260', 'petallengthcm': '0.593220338983050800', 'petalwidthcm': '0.500000000000000100', 'species': 'Iris-versicolor'}, {'id': '57', 'sepallengthcm': '0.555555555555555600', 'sepalwidthcm': '0.541666666666666500', 'petallengthcm': '0.627118644067796600', 'petalwidthcm': '0.625000000000000100', 'species': 'Iris-versicolor'}, {'id': '58', 'sepallengthcm': '0.166666666666666740', 'sepalwidthcm': '0.166666666666666630', 'petallengthcm': '0.389830508474576230', 'petalwidthcm': '0.375000000000000000', 'species': 'Iris-versicolor'}, {'id': '59', 'sepallengthcm': '0.638888888888888800', 'sepalwidthcm': '0.375000000000000000', 'petallengthcm': '0.610169491525423700', 'petalwidthcm': '0.500000000000000100', 'species': 'Iris-versicolor'}, {'id': '60', 'sepallengthcm': '0.250000000000000000', 'sepalwidthcm': '0.291666666666666740', 'petallengthcm': '0.491525423728813470', 'petalwidthcm': '0.541666666666666700', 'species': 'Iris-versicolor'}, {'id': '61', 'sepallengthcm': '0.194444444444444420', 'sepalwidthcm': '0E-18', 'petallengthcm': '0.423728813559322000', 'petalwidthcm': '0.375000000000000000', 'species': 'Iris-versicolor'}, {'id': '62', 'sepallengthcm': '0.444444444444444640', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.542372881355932200', 'petalwidthcm': '0.583333333333333400', 'species': 'Iris-versicolor'}, {'id': '63', 'sepallengthcm': '0.472222222222222300', 'sepalwidthcm': '0.083333333333333370', 'petallengthcm': '0.508474576271186400', 'petalwidthcm': '0.375000000000000000', 'species': 'Iris-versicolor'}, {'id': '64', 'sepallengthcm': '0.499999999999999800', 'sepalwidthcm': '0.375000000000000000', 'petallengthcm': '0.627118644067796600', 'petalwidthcm': '0.541666666666666700', 'species': 'Iris-versicolor'}, {'id': '65', 'sepallengthcm': '0.361111111111110940', 'sepalwidthcm': '0.375000000000000000', 'petallengthcm': '0.440677966101694960', 'petalwidthcm': '0.500000000000000100', 'species': 'Iris-versicolor'}, {'id': '66', 'sepallengthcm': '0.666666666666666700', 'sepalwidthcm': '0.458333333333333260', 'petallengthcm': '0.576271186440678000', 'petalwidthcm': '0.541666666666666700', 'species': 'Iris-versicolor'}, {'id': '67', 'sepallengthcm': '0.361111111111110940', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.593220338983050800', 'petalwidthcm': '0.583333333333333400', 'species': 'Iris-versicolor'}, {'id': '68', 'sepallengthcm': '0.416666666666666500', 'sepalwidthcm': '0.291666666666666740', 'petallengthcm': '0.525423728813559300', 'petalwidthcm': '0.375000000000000000', 'species': 'Iris-versicolor'}, {'id': '69', 'sepallengthcm': '0.527777777777777900', 'sepalwidthcm': '0.083333333333333370', 'petallengthcm': '0.593220338983050800', 'petalwidthcm': '0.583333333333333400', 'species': 'Iris-versicolor'}, {'id': '70', 'sepallengthcm': '0.361111111111110940', 'sepalwidthcm': '0.208333333333333260', 'petallengthcm': '0.491525423728813470', 'petalwidthcm': '0.416666666666666700', 'species': 'Iris-versicolor'}, {'id': '71', 'sepallengthcm': '0.444444444444444640', 'sepalwidthcm': '0.500000000000000000', 'petallengthcm': '0.644067796610169400', 'petalwidthcm': '0.708333333333333400', 'species': 'Iris-versicolor'}, {'id': '72', 'sepallengthcm': '0.499999999999999800', 'sepalwidthcm': '0.333333333333333260', 'petallengthcm': '0.508474576271186400', 'petalwidthcm': '0.500000000000000100', 'species': 'Iris-versicolor'}, {'id': '73', 'sepallengthcm': '0.555555555555555600', 'sepalwidthcm': '0.208333333333333260', 'petallengthcm': '0.661016949152542400', 'petalwidthcm': '0.583333333333333400', 'species': 'Iris-versicolor'}, {'id': '74', 'sepallengthcm': '0.499999999999999800', 'sepalwidthcm': '0.333333333333333260', 'petallengthcm': '0.627118644067796600', 'petalwidthcm': '0.458333333333333300', 'species': 'Iris-versicolor'}, {'id': '75', 'sepallengthcm': '0.583333333333333500', 'sepalwidthcm': '0.375000000000000000', 'petallengthcm': '0.559322033898305000', 'petalwidthcm': '0.500000000000000100', 'species': 'Iris-versicolor'}, {'id': '76', 'sepallengthcm': '0.638888888888888800', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.576271186440678000', 'petalwidthcm': '0.541666666666666700', 'species': 'Iris-versicolor'}, {'id': '77', 'sepallengthcm': '0.694444444444444400', 'sepalwidthcm': '0.333333333333333260', 'petallengthcm': '0.644067796610169400', 'petalwidthcm': '0.541666666666666700', 'species': 'Iris-versicolor'}, {'id': '78', 'sepallengthcm': '0.666666666666666700', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.677966101694915200', 'petalwidthcm': '0.666666666666666700', 'species': 'Iris-versicolor'}, {'id': '79', 'sepallengthcm': '0.472222222222222300', 'sepalwidthcm': '0.375000000000000000', 'petallengthcm': '0.593220338983050800', 'petalwidthcm': '0.583333333333333400', 'species': 'Iris-versicolor'}, {'id': '80', 'sepallengthcm': '0.388888888888888840', 'sepalwidthcm': '0.250000000000000000', 'petallengthcm': '0.423728813559322000', 'petalwidthcm': '0.375000000000000000', 'species': 'Iris-versicolor'}, {'id': '81', 'sepallengthcm': '0.333333333333333260', 'sepalwidthcm': '0.166666666666666630', 'petallengthcm': '0.474576271186440630', 'petalwidthcm': '0.416666666666666700', 'species': 'Iris-versicolor'}, {'id': '82', 'sepallengthcm': '0.333333333333333260', 'sepalwidthcm': '0.166666666666666630', 'petallengthcm': '0.457627118644067800', 'petalwidthcm': '0.375000000000000000', 'species': 'Iris-versicolor'}, {'id': '83', 'sepallengthcm': '0.416666666666666500', 'sepalwidthcm': '0.291666666666666740', 'petallengthcm': '0.491525423728813470', 'petalwidthcm': '0.458333333333333300', 'species': 'Iris-versicolor'}, {'id': '84', 'sepallengthcm': '0.472222222222222300', 'sepalwidthcm': '0.291666666666666740', 'petallengthcm': '0.694915254237288100', 'petalwidthcm': '0.625000000000000100', 'species': 'Iris-versicolor'}, {'id': '85', 'sepallengthcm': '0.305555555555555600', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.593220338983050800', 'petalwidthcm': '0.583333333333333400', 'species': 'Iris-versicolor'}, {'id': '86', 'sepallengthcm': '0.472222222222222300', 'sepalwidthcm': '0.583333333333333300', 'petallengthcm': '0.593220338983050800', 'petalwidthcm': '0.625000000000000100', 'species': 'Iris-versicolor'}, {'id': '87', 'sepallengthcm': '0.666666666666666700', 'sepalwidthcm': '0.458333333333333260', 'petallengthcm': '0.627118644067796600', 'petalwidthcm': '0.583333333333333400', 'species': 'Iris-versicolor'}, {'id': '88', 'sepallengthcm': '0.555555555555555600', 'sepalwidthcm': '0.124999999999999890', 'petallengthcm': '0.576271186440678000', 'petalwidthcm': '0.500000000000000100', 'species': 'Iris-versicolor'}, {'id': '89', 'sepallengthcm': '0.361111111111110940', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.525423728813559300', 'petalwidthcm': '0.500000000000000100', 'species': 'Iris-versicolor'}, {'id': '90', 'sepallengthcm': '0.333333333333333260', 'sepalwidthcm': '0.208333333333333260', 'petallengthcm': '0.508474576271186400', 'petalwidthcm': '0.500000000000000100', 'species': 'Iris-versicolor'}, {'id': '91', 'sepallengthcm': '0.333333333333333260', 'sepalwidthcm': '0.250000000000000000', 'petallengthcm': '0.576271186440678000', 'petalwidthcm': '0.458333333333333300', 'species': 'Iris-versicolor'}, {'id': '92', 'sepallengthcm': '0.499999999999999800', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.610169491525423700', 'petalwidthcm': '0.541666666666666700', 'species': 'Iris-versicolor'}, {'id': '93', 'sepallengthcm': '0.416666666666666500', 'sepalwidthcm': '0.250000000000000000', 'petallengthcm': '0.508474576271186400', 'petalwidthcm': '0.458333333333333300', 'species': 'Iris-versicolor'}, {'id': '94', 'sepallengthcm': '0.194444444444444420', 'sepalwidthcm': '0.124999999999999890', 'petallengthcm': '0.389830508474576230', 'petalwidthcm': '0.375000000000000000', 'species': 'Iris-versicolor'}, {'id': '95', 'sepallengthcm': '0.361111111111110940', 'sepalwidthcm': '0.291666666666666740', 'petallengthcm': '0.542372881355932200', 'petalwidthcm': '0.500000000000000100', 'species': 'Iris-versicolor'}, {'id': '96', 'sepallengthcm': '0.388888888888888840', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.542372881355932200', 'petalwidthcm': '0.458333333333333300', 'species': 'Iris-versicolor'}, {'id': '97', 'sepallengthcm': '0.388888888888888840', 'sepalwidthcm': '0.375000000000000000', 'petallengthcm': '0.542372881355932200', 'petalwidthcm': '0.500000000000000100', 'species': 'Iris-versicolor'}, {'id': '98', 'sepallengthcm': '0.527777777777777900', 'sepalwidthcm': '0.375000000000000000', 'petallengthcm': '0.559322033898305000', 'petalwidthcm': '0.500000000000000100', 'species': 'Iris-versicolor'}, {'id': '99', 'sepallengthcm': '0.222222222222222100', 'sepalwidthcm': '0.208333333333333260', 'petallengthcm': '0.338983050847457600', 'petalwidthcm': '0.416666666666666700', 'species': 'Iris-versicolor'}, {'id': '100', 'sepallengthcm': '0.388888888888888840', 'sepalwidthcm': '0.333333333333333260', 'petallengthcm': '0.525423728813559300', 'petalwidthcm': '0.500000000000000100', 'species': 'Iris-versicolor'}, {'id': '101', 'sepallengthcm': '0.555555555555555600', 'sepalwidthcm': '0.541666666666666500', 'petallengthcm': '0.847457627118644000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-virginica'}, {'id': '102', 'sepallengthcm': '0.416666666666666500', 'sepalwidthcm': '0.291666666666666740', 'petallengthcm': '0.694915254237288100', 'petalwidthcm': '0.750000000000000000', 'species': 'Iris-virginica'}, {'id': '103', 'sepallengthcm': '0.777777777777777700', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.830508474576271200', 'petalwidthcm': '0.833333333333333500', 'species': 'Iris-virginica'}, {'id': '104', 'sepallengthcm': '0.555555555555555600', 'sepalwidthcm': '0.375000000000000000', 'petallengthcm': '0.779661016949152500', 'petalwidthcm': '0.708333333333333400', 'species': 'Iris-virginica'}, {'id': '105', 'sepallengthcm': '0.611111111111111200', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.813559322033898200', 'petalwidthcm': '0.875000000000000100', 'species': 'Iris-virginica'}, {'id': '106', 'sepallengthcm': '0.916666666666666500', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.949152542372881300', 'petalwidthcm': '0.833333333333333500', 'species': 'Iris-virginica'}, {'id': '107', 'sepallengthcm': '0.166666666666666740', 'sepalwidthcm': '0.208333333333333260', 'petallengthcm': '0.593220338983050800', 'petalwidthcm': '0.666666666666666700', 'species': 'Iris-virginica'}, {'id': '108', 'sepallengthcm': '0.833333333333333000', 'sepalwidthcm': '0.375000000000000000', 'petallengthcm': '0.898305084745762500', 'petalwidthcm': '0.708333333333333400', 'species': 'Iris-virginica'}, {'id': '109', 'sepallengthcm': '0.666666666666666700', 'sepalwidthcm': '0.208333333333333260', 'petallengthcm': '0.813559322033898200', 'petalwidthcm': '0.708333333333333400', 'species': 'Iris-virginica'}, {'id': '110', 'sepallengthcm': '0.805555555555555600', 'sepalwidthcm': '0.666666666666666700', 'petallengthcm': '0.864406779661016900', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-virginica'}, {'id': '111', 'sepallengthcm': '0.611111111111111200', 'sepalwidthcm': '0.500000000000000000', 'petallengthcm': '0.694915254237288100', 'petalwidthcm': '0.791666666666666700', 'species': 'Iris-virginica'}, {'id': '112', 'sepallengthcm': '0.583333333333333500', 'sepalwidthcm': '0.291666666666666740', 'petallengthcm': '0.728813559322033800', 'petalwidthcm': '0.750000000000000000', 'species': 'Iris-virginica'}, {'id': '113', 'sepallengthcm': '0.694444444444444400', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.762711864406779600', 'petalwidthcm': '0.833333333333333500', 'species': 'Iris-virginica'}, {'id': '114', 'sepallengthcm': '0.388888888888888840', 'sepalwidthcm': '0.208333333333333260', 'petallengthcm': '0.677966101694915200', 'petalwidthcm': '0.791666666666666700', 'species': 'Iris-virginica'}, {'id': '115', 'sepallengthcm': '0.416666666666666500', 'sepalwidthcm': '0.333333333333333260', 'petallengthcm': '0.694915254237288100', 'petalwidthcm': '0.958333333333333400', 'species': 'Iris-virginica'}, {'id': '116', 'sepallengthcm': '0.583333333333333500', 'sepalwidthcm': '0.500000000000000000', 'petallengthcm': '0.728813559322033800', 'petalwidthcm': '0.916666666666666600', 'species': 'Iris-virginica'}, {'id': '117', 'sepallengthcm': '0.611111111111111200', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.762711864406779600', 'petalwidthcm': '0.708333333333333400', 'species': 'Iris-virginica'}, {'id': '118', 'sepallengthcm': '0.944444444444444200', 'sepalwidthcm': '0.749999999999999800', 'petallengthcm': '0.966101694915254300', 'petalwidthcm': '0.875000000000000100', 'species': 'Iris-virginica'}, {'id': '119', 'sepallengthcm': '0.944444444444444200', 'sepalwidthcm': '0.250000000000000000', 'petallengthcm': '1.000000000000000000', 'petalwidthcm': '0.916666666666666600', 'species': 'Iris-virginica'}, {'id': '120', 'sepallengthcm': '0.472222222222222300', 'sepalwidthcm': '0.083333333333333370', 'petallengthcm': '0.677966101694915200', 'petalwidthcm': '0.583333333333333400', 'species': 'Iris-virginica'}, {'id': '121', 'sepallengthcm': '0.722222222222222300', 'sepalwidthcm': '0.500000000000000000', 'petallengthcm': '0.796610169491525400', 'petalwidthcm': '0.916666666666666600', 'species': 'Iris-virginica'}, {'id': '122', 'sepallengthcm': '0.361111111111110940', 'sepalwidthcm': '0.333333333333333260', 'petallengthcm': '0.661016949152542400', 'petalwidthcm': '0.791666666666666700', 'species': 'Iris-virginica'}, {'id': '123', 'sepallengthcm': '0.944444444444444200', 'sepalwidthcm': '0.333333333333333260', 'petallengthcm': '0.966101694915254300', 'petalwidthcm': '0.791666666666666700', 'species': 'Iris-virginica'}, {'id': '124', 'sepallengthcm': '0.555555555555555600', 'sepalwidthcm': '0.291666666666666740', 'petallengthcm': '0.661016949152542400', 'petalwidthcm': '0.708333333333333400', 'species': 'Iris-virginica'}, {'id': '125', 'sepallengthcm': '0.666666666666666700', 'sepalwidthcm': '0.541666666666666500', 'petallengthcm': '0.796610169491525400', 'petalwidthcm': '0.833333333333333500', 'species': 'Iris-virginica'}, {'id': '126', 'sepallengthcm': '0.805555555555555600', 'sepalwidthcm': '0.500000000000000000', 'petallengthcm': '0.847457627118644000', 'petalwidthcm': '0.708333333333333400', 'species': 'Iris-virginica'}, {'id': '127', 'sepallengthcm': '0.527777777777777900', 'sepalwidthcm': '0.333333333333333260', 'petallengthcm': '0.644067796610169400', 'petalwidthcm': '0.708333333333333400', 'species': 'Iris-virginica'}, {'id': '128', 'sepallengthcm': '0.499999999999999800', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.661016949152542400', 'petalwidthcm': '0.708333333333333400', 'species': 'Iris-virginica'}, {'id': '129', 'sepallengthcm': '0.583333333333333500', 'sepalwidthcm': '0.333333333333333260', 'petallengthcm': '0.779661016949152500', 'petalwidthcm': '0.833333333333333500', 'species': 'Iris-virginica'}, {'id': '130', 'sepallengthcm': '0.805555555555555600', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.813559322033898200', 'petalwidthcm': '0.625000000000000100', 'species': 'Iris-virginica'}, {'id': '131', 'sepallengthcm': '0.861111111111111200', 'sepalwidthcm': '0.333333333333333260', 'petallengthcm': '0.864406779661016900', 'petalwidthcm': '0.750000000000000000', 'species': 'Iris-virginica'}, {'id': '132', 'sepallengthcm': '1.000000000000000000', 'sepalwidthcm': '0.749999999999999800', 'petallengthcm': '0.915254237288135600', 'petalwidthcm': '0.791666666666666700', 'species': 'Iris-virginica'}, {'id': '133', 'sepallengthcm': '0.583333333333333500', 'sepalwidthcm': '0.333333333333333260', 'petallengthcm': '0.779661016949152500', 'petalwidthcm': '0.875000000000000100', 'species': 'Iris-virginica'}, {'id': '134', 'sepallengthcm': '0.555555555555555600', 'sepalwidthcm': '0.333333333333333260', 'petallengthcm': '0.694915254237288100', 'petalwidthcm': '0.583333333333333400', 'species': 'Iris-virginica'}, {'id': '135', 'sepallengthcm': '0.499999999999999800', 'sepalwidthcm': '0.250000000000000000', 'petallengthcm': '0.779661016949152500', 'petalwidthcm': '0.541666666666666700', 'species': 'Iris-virginica'}, {'id': '136', 'sepallengthcm': '0.944444444444444200', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.864406779661016900', 'petalwidthcm': '0.916666666666666600', 'species': 'Iris-virginica'}, {'id': '137', 'sepallengthcm': '0.555555555555555600', 'sepalwidthcm': '0.583333333333333300', 'petallengthcm': '0.779661016949152500', 'petalwidthcm': '0.958333333333333400', 'species': 'Iris-virginica'}, {'id': '138', 'sepallengthcm': '0.583333333333333500', 'sepalwidthcm': '0.458333333333333260', 'petallengthcm': '0.762711864406779600', 'petalwidthcm': '0.708333333333333400', 'species': 'Iris-virginica'}, {'id': '139', 'sepallengthcm': '0.472222222222222300', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.644067796610169400', 'petalwidthcm': '0.708333333333333400', 'species': 'Iris-virginica'}, {'id': '140', 'sepallengthcm': '0.722222222222222300', 'sepalwidthcm': '0.458333333333333260', 'petallengthcm': '0.745762711864406800', 'petalwidthcm': '0.833333333333333500', 'species': 'Iris-virginica'}, {'id': '141', 'sepallengthcm': '0.666666666666666700', 'sepalwidthcm': '0.458333333333333260', 'petallengthcm': '0.779661016949152500', 'petalwidthcm': '0.958333333333333400', 'species': 'Iris-virginica'}, {'id': '142', 'sepallengthcm': '0.722222222222222300', 'sepalwidthcm': '0.458333333333333260', 'petallengthcm': '0.694915254237288100', 'petalwidthcm': '0.916666666666666600', 'species': 'Iris-virginica'}, {'id': '143', 'sepallengthcm': '0.416666666666666500', 'sepalwidthcm': '0.291666666666666740', 'petallengthcm': '0.694915254237288100', 'petalwidthcm': '0.750000000000000000', 'species': 'Iris-virginica'}, {'id': '144', 'sepallengthcm': '0.694444444444444400', 'sepalwidthcm': '0.500000000000000000', 'petallengthcm': '0.830508474576271200', 'petalwidthcm': '0.916666666666666600', 'species': 'Iris-virginica'}, {'id': '145', 'sepallengthcm': '0.666666666666666700', 'sepalwidthcm': '0.541666666666666500', 'petallengthcm': '0.796610169491525400', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-virginica'}, {'id': '146', 'sepallengthcm': '0.666666666666666700', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.711864406779661000', 'petalwidthcm': '0.916666666666666600', 'species': 'Iris-virginica'}, {'id': '147', 'sepallengthcm': '0.555555555555555600', 'sepalwidthcm': '0.208333333333333260', 'petallengthcm': '0.677966101694915200', 'petalwidthcm': '0.750000000000000000', 'species': 'Iris-virginica'}, {'id': '148', 'sepallengthcm': '0.611111111111111200', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.711864406779661000', 'petalwidthcm': '0.791666666666666700', 'species': 'Iris-virginica'}, {'id': '149', 'sepallengthcm': '0.527777777777777900', 'sepalwidthcm': '0.583333333333333300', 'petallengthcm': '0.745762711864406800', 'petalwidthcm': '0.916666666666666600', 'species': 'Iris-virginica'}, {'id': '150', 'sepallengthcm': '0.444444444444444640', 'sepalwidthcm': '0.416666666666666740', 'petallengthcm': '0.694915254237288100', 'petalwidthcm': '0.708333333333333400', 'species': 'Iris-virginica'}]\n"
     ]
    }
   ],
   "source": [
    "# API endpoint URL\n",
    "API_URL = f\"http://localhost/api/database/{db_id}/table/{selected_table_id}/data?size=100000&page=0\"\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\"  # Specify the expected response format\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the API with the Accept header\n",
    "    response = requests.get(API_URL, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        dataset = response.json()\n",
    "        \n",
    "        \n",
    "        print( dataset)\n",
    "    else:\n",
    "        print(f\"Error: Received status code {response.status_code}\")\n",
    "        print(\"Response content:\", response.text)\n",
    "       \n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09557f94-325c-4bd6-882a-069a9e3c5ecd",
   "metadata": {},
   "source": [
    "replacing dynamic fetching of data When and if DBREPO isnt running (BACKUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce6e020d-cb80-49ec-8bcc-687b1e08885c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'iris_data.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 1. Read the JSON file id the API isnt available this data is saved locally but the data is from the API endpoint\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miris_data.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      3\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'iris_data.json'"
     ]
    }
   ],
   "source": [
    "# # 1. Read the JSON file id the API isnt available this data is saved locally but the data is from the API endpoint\n",
    "# with open(\"iris_data.json\", \"r\") as f:\n",
    "#     dataset = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcf2244-14dd-4e3d-b8cf-f7f3ba34f80f",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# üìÇ Setup MLflow\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbe91ec0-6447-4586-b7cc-2c1f74d4218f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter experiment name for MLflow:  efrgtr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/18 16:59:10 INFO mlflow.tracking.fluent: Experiment with name 'efrgtr' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/225271718569336729', creation_time=1747580350310, experiment_id='225271718569336729', last_update_time=1747580350310, lifecycle_stage='active', name='efrgtr', tags={}>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "# Ensure tracking directory exists\n",
    "project_dir = os.getcwd()\n",
    "mlrunlogs_dir = os.path.join(project_dir, \"mlrunlogs\")\n",
    "os.makedirs(mlrunlogs_dir, exist_ok=True)\n",
    "\n",
    "# Set MLflow tracking URI (local SQLite backend)\n",
    "mlflow_tracking_path = os.path.join(mlrunlogs_dir, \"mlflow.db\")\n",
    "mlflow.set_tracking_uri(\"mlrunlogs/mlflow.db\")\n",
    "\n",
    "# Prompt for experiment name\n",
    "experiment_name = input(\"Enter experiment name for MLflow: \").strip()\n",
    "if not experiment_name:\n",
    "    experiment_name = \"default_experiment\"\n",
    "    print(\"‚ö†Ô∏è No name entered. Using fallback:\", experiment_name)\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2c2c5f-cc36-41a3-9643-83ef95b9f55e",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# üîÑ Git Commit Hash for previous commit for metadata\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "838dd233-25dc-4725-974d-4da89c257782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import os\n",
    "\n",
    "def get_latest_git_commit(repo_path: str = \"C:/Users/reema/REPO\") -> dict:\n",
    "    \"\"\"\n",
    "    Returns the latest Git commit metadata from the given repo path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        repo = git.Repo(repo_path)\n",
    "        commit = repo.head.commit\n",
    "        commit_metadata = {\n",
    "            \"git_commit\": commit.hexsha,\n",
    "            \"git_author\": commit.author.name,\n",
    "            \"git_email\": commit.author.email,\n",
    "            \"git_commit_time\": str(commit.committed_datetime),\n",
    "            \"git_message\": commit.message.strip(),\n",
    "            \"git_branch\": repo.active_branch.name if not repo.head.is_detached else \"detached\"\n",
    "        }\n",
    "        return commit_metadata\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[‚ö†Ô∏è Git Error] Could not read Git repo at {repo_path}: {e}\")\n",
    "        return {\n",
    "            \"git_commit\": \"not available\",\n",
    "            \"git_author\": \"not available\",\n",
    "            \"git_email\": \"not available\",\n",
    "            \"git_commit_time\": \"not available\",\n",
    "            \"git_message\": \"not available\",\n",
    "            \"git_branch\": \"not available\"\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "repo_dir = \"C:/Users/reema/REPO\"\n",
    "git_metadata = get_latest_git_commit(repo_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430d15ef-3432-4e45-88fb-b7048a5b10a9",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# Make threadpoolctl safe so MLflow‚Äôs autologger won‚Äôt crash ‚îÄ‚îÄ‚îÄ\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9668451f-4352-4bdc-8b6b-bbe49074212a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/18 16:59:17 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2025/05/18 16:59:18 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ Patch threadpoolctl if needed to avoid autolog crashes ‚îÄ‚îÄ‚îÄ\n",
    "try:\n",
    "    import threadpoolctl\n",
    "    _original_threadpool_info = threadpoolctl.threadpool_info\n",
    "\n",
    "    def _safe_threadpool_info(*args, **kwargs):\n",
    "        try:\n",
    "            return _original_threadpool_info(*args, **kwargs)\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    threadpoolctl.threadpool_info = _safe_threadpool_info\n",
    "except ImportError:\n",
    "    pass  # If threadpoolctl isn't installed, we just skip this patch\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ Enable MLflow autologging (generic, works with sklearn and more) ‚îÄ‚îÄ‚îÄ\n",
    "import mlflow\n",
    "\n",
    "mlflow.autolog(\n",
    "    log_input_examples=True,\n",
    "    log_model_signatures=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b608670-96a5-42b0-b69b-263ac1e452eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import platform\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from subprocess import check_output, CalledProcessError\n",
    "\n",
    "def log_standard_metadata(\n",
    "    model_name: str,\n",
    "    model,\n",
    "    hyperparams: dict,\n",
    "    acc: float,\n",
    "    prec: float,\n",
    "    rec: float,\n",
    "    f1: float,\n",
    "    auc: float,\n",
    "    label_map: dict,\n",
    "    run_id: str,\n",
    "    test_size: float,\n",
    "    random_state: int,\n",
    "    id_cols: list,\n",
    "    target_col: str,\n",
    "    X,\n",
    "    y,\n",
    "    run_data=None\n",
    "):\n",
    "    # === Experiment Metadata ===\n",
    "    mlflow.set_tag(\"run_id\", run_id)  # [MLflow / DB anchor]\n",
    "    mlflow.set_tag(\"model_name\", model_name)  # [ML Metadata, FAIR]\n",
    "    mlflow.set_tag(\"model_architecture\", model.__class__.__name__)  # [MLSEA]\n",
    "    mlflow.set_tag(\"test_size\", test_size)  # [MLSEA, Reproducibility]\n",
    "    mlflow.set_tag(\"random_state\", random_state)  # [MLSEA, Reproducibility]\n",
    "\n",
    "    # === Evaluation Metrics ===\n",
    "    mlflow.set_tag(\"accuracy\", acc)\n",
    "    mlflow.set_tag(\"precision_macro\", prec)\n",
    "    mlflow.set_tag(\"recall_macro\", rec)\n",
    "    mlflow.set_tag(\"f1_macro\", f1)\n",
    "    mlflow.set_tag(\"roc_auc\", auc)\n",
    "\n",
    "    # === Hyperparameters and Label Encoding ===\n",
    "    mlflow.set_tag(\"hyperparameters\", json.dumps(hyperparams))  # [FAIR, MLSEA]\n",
    "    mlflow.set_tag(\"label_map\", json.dumps(label_map))  # [ML Preprocessing]\n",
    "\n",
    "    # === Preprocessing Snapshot ===\n",
    "    preprocessing_info = {\n",
    "        \"dropped_columns\": id_cols,\n",
    "        \"numeric_columns\": list(X.columns),\n",
    "        \"target_column\": target_col,\n",
    "        \"stratified\": False,\n",
    "        \"coercion_strategy\": \"Numeric cast (auto)\",\n",
    "        \"feature_engineering\": \"None\",\n",
    "        \"missing_value_strategy\": \"None\",\n",
    "        \"outlier_detection\": \"None\",\n",
    "        \"encoding_strategy\": \"LabelEncoder (target only)\",\n",
    "        \"scaling\": \"None\",\n",
    "        \"sampling\": \"None\",\n",
    "        \"feature_selection\": \"None\",\n",
    "        \"train_test_split\": {\"test_size\": test_size, \"random_state\": random_state},\n",
    "        \"imbalance_ratio\": str(dict(zip(*np.unique(y, return_counts=True)))),\n",
    "        \"preprocessing_timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    preprocessing_hash = hashlib.sha256(json.dumps(preprocessing_info).encode()).hexdigest()\n",
    "    mlflow.set_tag(\"preprocessing_info\", json.dumps(preprocessing_info))  # [MLSEA]\n",
    "    mlflow.set_tag(\"preprocessing_hash\", preprocessing_hash)\n",
    "\n",
    "    # === Reproducibility ===\n",
    "    mlflow.set_tag(\"model_serialization\", \"pickle\")  # [FAIR, MLSEA]\n",
    "    mlflow.set_tag(\"model_path\", f\"{model_name}.pkl\")\n",
    "\n",
    "    try:\n",
    "        sha = check_output([\"git\", \"rev-parse\", \"HEAD\"], text=True).strip()\n",
    "    except CalledProcessError:\n",
    "        sha = \"unknown\"\n",
    "    mlflow.set_tag(\"git_commit\", sha)\n",
    "\n",
    "    # === Compute Environment ===\n",
    "    compute_env = {\n",
    "        \"os\": f\"{platform.system()} {platform.release()}\",\n",
    "        \"cpu\": platform.processor(),\n",
    "        \"ram_gb\": round(psutil.virtual_memory().total / (1024 ** 3), 2),\n",
    "        \"python_version\": platform.python_version(),\n",
    "        \"sklearn_version\": sklearn.__version__,\n",
    "        \"pandas_version\": pd.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "    }\n",
    "    mlflow.set_tag(\"compute_environment\", json.dumps(compute_env))  # [Reproducibility]\n",
    "\n",
    "    # === Optional: Tag MLflow Justifications (previously logged manually) ===\n",
    "    if run_data:\n",
    "        for key, val in run_data.tags.items():\n",
    "            if key.startswith(\"justification_\"):\n",
    "                mlflow.set_tag(key, val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69f000f9-d0b6-41f7-92d3-4b605e4ecaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_reproducibility_txt_log(\n",
    "    model_name: str,\n",
    "    dataset_name: str,\n",
    "    dataset_version: str,\n",
    "    hyperparams: dict,\n",
    "    metrics: dict,\n",
    "    git_commit: str,\n",
    "    run_id: str,\n",
    "    architecture_file_path: str = \"provenance_architecture_description.txt\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a reproducibility log (YAML + architecture) and return the saved path.\n",
    "    This log combines:\n",
    "    - Model and dataset details\n",
    "    - Hyperparameters and evaluation metrics\n",
    "    - Git provenance info\n",
    "    - Reproduction steps\n",
    "    - Provenance architecture description\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_values(d):\n",
    "        \"\"\"Convert numpy floats to native floats.\"\"\"\n",
    "        return {k: float(v) if isinstance(v, (np.float32, np.float64)) else v for k, v in d.items()}\n",
    "\n",
    "    timestamp = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M UTC\")\n",
    "\n",
    "    repro_data = {\n",
    "        \"üìå Model Details\": {\n",
    "            \"Model Name\": model_name,\n",
    "            \"Dataset Name\": dataset_name,\n",
    "            \"Dataset Version\": dataset_version,\n",
    "            \"Run ID\": run_id,\n",
    "            \"Timestamp\": timestamp\n",
    "        },\n",
    "        \"üõ†Ô∏è Hyperparameters\": clean_values(hyperparams),\n",
    "        \"üìà Metrics\": clean_values(metrics),\n",
    "        \"üîó Git Info\": {\n",
    "            \"Commit Hash\": git_commit,\n",
    "            \"Reproduce With\": f\"git checkout {git_commit}\"\n",
    "        },\n",
    "        \"üöÄ Reproduction Guide\": [\n",
    "            \"1. Clone the repo and checkout the commit:\",\n",
    "            f\"   git checkout {git_commit}\",\n",
    "            \"2. Load and preprocess the dataset exactly as during training.\",\n",
    "            \"3. Load the model using MLflow:\",\n",
    "            f\"   mlflow.sklearn.load_model('runs:/{run_id}/model')\",\n",
    "            \"4. Run inference or evaluation using the same pipeline/script.\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # üîê Create and write to output file\n",
    "    save_dir = os.path.join(\"MODEL_PROVENANCE\", model_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    txt_path = os.path.join(save_dir, f\"{model_name}_reproducibility.txt\")\n",
    "\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as repro_file:\n",
    "        yaml.dump(repro_data, repro_file, allow_unicode=True, sort_keys=False, width=100)\n",
    "        repro_file.write(\"\\n\\n\")\n",
    "\n",
    "        if os.path.exists(architecture_file_path):\n",
    "            with open(architecture_file_path, \"r\", encoding=\"utf-8\") as arch_file:\n",
    "                architecture_description = arch_file.read()\n",
    "                repro_file.write(architecture_description)\n",
    "        else:\n",
    "            repro_file.write(\"[‚ö†Ô∏è Missing architecture description file]\\n\")\n",
    "\n",
    "    return txt_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a08a1f1-c3a6-45bd-97b1-92e2fade9ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_with_justification(log_func, key: str, value, context: str = \"\"):\n",
    "    \"\"\"\n",
    "    Log a value using the specified MLflow log function (e.g., mlflow.log_param),\n",
    "    then prompt the user for a justification and log it as a tag.\n",
    "    \"\"\"\n",
    "    log_func(key, value)\n",
    "    print(f\"\\nüìù Justification for `{key}` ({context})\")\n",
    "    user_reason = input(\"‚Üí Why did you choose this value? \")\n",
    "    mlflow.set_tag(f\"justification_{key}\", user_reason or \"No justification provided\")\n",
    "\n",
    "def log_justification(key: str, question: str):\n",
    "    \"\"\"\n",
    "    Prompt for a justification only (without logging a value), and log it as a tag.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìù Justification for `{key}`\")\n",
    "    user_reason = input(f\"‚Üí {question} \")\n",
    "    mlflow.set_tag(f\"justification_{key}\", user_reason or \"No justification provided\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00237086-0d9c-41b2-a780-b2322ecd69fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9058319a-adba-4a6b-93e9-d17080c0594d",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# üöÄ Start MLflow Run \n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14c62f08-a116-4060-9689-f69968e9f240",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your role (default: collaborator):  dsf\n",
      "Enter project ID (default: default_project):  fsdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Session Metadata:\n",
      "  session_id: a703d999-0a29-4cfb-8dab-48d1f14d1bff\n",
      "  username: reema\n",
      "  timestamp_utc: 2025-05-18T15:03:47.945054\n",
      "  hostname: Purplish\n",
      "  platform: Windows\n",
      "  os_version: 10.0.26100\n",
      "  python_version: 3.11.5\n",
      "  role: dsf\n",
      "  project_id: fsdf\n",
      "ML_EXP_Shapes: (150, 4) (150,)\n",
      "ML_EXP_Classes: ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n",
      "{'id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'name': 'Iris', 'description': None, 'tables': [{'id': 'e435fe7c-f889-40f5-9660-007597ad4a5b', 'name': 'iris_data_v2', 'alias': None, 'identifiers': [], 'owner': {'id': '55bad3e0-c815-103f-9c9e-43ec87b6b872', 'username': 'reema', 'name': 'reema dass', 'orcid': None, 'qualified_name': 'reema dass ‚Äî @reema', 'given_name': 'reema', 'family_name': 'dass'}, 'description': 'iris_data_v2', 'columns': [{'id': '9208e04f-b09c-4d43-9ddc-a41cffc55840', 'name': 'Id', 'alias': None, 'size': None, 'd': None, 'mean': 51, 'median': 51, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': 'e435fe7c-f889-40f5-9660-007597ad4a5b', 'ord': 0, 'internal_name': 'id', 'index_length': None, 'length': None, 'type': 'bigint', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 29, 'is_null_allowed': False}, {'id': '6285c88a-8cb7-42c3-afcc-d5623f96b0ad', 'name': 'SepalLengthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 5, 'median': 5, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': 'e435fe7c-f889-40f5-9660-007597ad4a5b', 'ord': 1, 'internal_name': 'sepallengthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 1, 'is_null_allowed': False}, {'id': 'dfe7abba-0c31-4bb6-95d8-48467b619dad', 'name': 'SepalWidthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 3, 'median': 3, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': 'e435fe7c-f889-40f5-9660-007597ad4a5b', 'ord': 2, 'internal_name': 'sepalwidthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 0, 'is_null_allowed': False}, {'id': '86f0cd65-72b6-42a4-b710-24e3026cb155', 'name': 'PetalLengthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 3, 'median': 3, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': 'e435fe7c-f889-40f5-9660-007597ad4a5b', 'ord': 3, 'internal_name': 'petallengthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 1, 'is_null_allowed': False}, {'id': 'ee6d4920-2c24-447e-86d7-cae81d85c091', 'name': 'PetalWidthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 1, 'median': 1, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': 'e435fe7c-f889-40f5-9660-007597ad4a5b', 'ord': 4, 'internal_name': 'petalwidthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 1, 'is_null_allowed': False}, {'id': 'fe728517-04fc-4cd2-9d57-d4b036d26db0', 'name': 'Species', 'alias': None, 'size': 255, 'd': None, 'mean': None, 'median': None, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': 'e435fe7c-f889-40f5-9660-007597ad4a5b', 'ord': 5, 'internal_name': 'species', 'index_length': None, 'length': None, 'type': 'varchar', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': None, 'is_null_allowed': False}], 'constraints': {'uniques': [{'id': 'feb9db84-3974-44df-8456-dfa5021db0cd', 'name': 'uk_iris_data_v2_0', 'table': {'id': 'e435fe7c-f889-40f5-9660-007597ad4a5b', 'name': 'iris_data_v2', 'description': 'iris_data_v2', 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'internal_name': 'iris_data_v2', 'is_versioned': True, 'is_public': True, 'is_schema_public': True, 'owned_by': '55bad3e0-c815-103f-9c9e-43ec87b6b872'}, 'columns': [{'id': '9208e04f-b09c-4d43-9ddc-a41cffc55840', 'name': 'Id', 'alias': None, 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': 'e435fe7c-f889-40f5-9660-007597ad4a5b', 'internal_name': 'id', 'type': 'bigint'}]}], 'checks': [], 'foreign_keys': [], 'primary_key': [{'id': '23a1fd6a-871f-4a00-bde2-1a51a2873b07', 'table': {'id': 'e435fe7c-f889-40f5-9660-007597ad4a5b', 'name': 'iris_data_v2', 'description': 'iris_data_v2', 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'internal_name': 'iris_data_v2', 'is_versioned': True, 'is_public': True, 'is_schema_public': True, 'owned_by': '55bad3e0-c815-103f-9c9e-43ec87b6b872'}, 'column': {'id': '9208e04f-b09c-4d43-9ddc-a41cffc55840', 'name': 'Id', 'alias': None, 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': 'e435fe7c-f889-40f5-9660-007597ad4a5b', 'internal_name': 'id', 'type': 'bigint'}}]}, 'created': '2025-05-18 09:29:15', 'last_retrieved': None, 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'internal_name': 'iris_data_v2', 'is_versioned': True, 'is_schema_public': True, 'queue_name': 'dbrepo', 'queue_type': 'quorum', 'routing_key': 'dbrepo.f6a329d0-0cd6-4308-8c89-ad5763b42324.e435fe7c-f889-40f5-9660-007597ad4a5b', 'is_public': True, 'num_rows': 100, 'data_length': 16384, 'max_data_length': 0, 'avg_row_length': 163}, {'id': '5aa89543-bd03-4118-bdc6-7c4d09e8ff75', 'name': 'iris_data', 'alias': None, 'identifiers': [], 'owner': {'id': '55bad3e0-c815-103f-9c9e-43ec87b6b872', 'username': 'reema', 'name': 'reema dass', 'orcid': None, 'qualified_name': 'reema dass ‚Äî @reema', 'given_name': 'reema', 'family_name': 'dass'}, 'description': 'iris data', 'columns': [{'id': '5f1b7208-823d-48f7-b529-10d28c03b95d', 'name': 'Id', 'alias': None, 'size': None, 'd': None, 'mean': 76, 'median': 76, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '5aa89543-bd03-4118-bdc6-7c4d09e8ff75', 'ord': 0, 'internal_name': 'id', 'index_length': None, 'length': None, 'type': 'bigint', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 43, 'is_null_allowed': False}, {'id': 'e8f4b88f-04f8-4270-8dcc-82cf53716963', 'name': 'SepalLengthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 6, 'median': 6, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '5aa89543-bd03-4118-bdc6-7c4d09e8ff75', 'ord': 1, 'internal_name': 'sepallengthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 1, 'is_null_allowed': False}, {'id': '38b10d14-9275-48e8-bdb3-056882bc0d89', 'name': 'SepalWidthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 3, 'median': 3, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '5aa89543-bd03-4118-bdc6-7c4d09e8ff75', 'ord': 2, 'internal_name': 'sepalwidthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 0, 'is_null_allowed': False}, {'id': 'bd9e25b2-876f-4eed-8860-30426c3fc7db', 'name': 'PetalLengthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 4, 'median': 4, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '5aa89543-bd03-4118-bdc6-7c4d09e8ff75', 'ord': 3, 'internal_name': 'petallengthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 2, 'is_null_allowed': False}, {'id': 'a19effb4-94f1-48a2-a2ed-4c306ad40118', 'name': 'PetalWidthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 1, 'median': 1, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '5aa89543-bd03-4118-bdc6-7c4d09e8ff75', 'ord': 4, 'internal_name': 'petalwidthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 1, 'is_null_allowed': False}, {'id': '5d373e57-3824-4fc2-a09c-8c67180d95c4', 'name': 'Species', 'alias': None, 'size': 255, 'd': None, 'mean': None, 'median': None, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '5aa89543-bd03-4118-bdc6-7c4d09e8ff75', 'ord': 5, 'internal_name': 'species', 'index_length': None, 'length': None, 'type': 'varchar', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': None, 'is_null_allowed': False}], 'constraints': {'uniques': [{'id': 'f40db558-06c7-4f0b-ab97-c45ab36127ed', 'name': 'uk_iris_data_0', 'table': {'id': '5aa89543-bd03-4118-bdc6-7c4d09e8ff75', 'name': 'iris_data', 'description': 'iris data', 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'internal_name': 'iris_data', 'is_versioned': True, 'is_public': True, 'is_schema_public': True, 'owned_by': '55bad3e0-c815-103f-9c9e-43ec87b6b872'}, 'columns': [{'id': '5f1b7208-823d-48f7-b529-10d28c03b95d', 'name': 'Id', 'alias': None, 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '5aa89543-bd03-4118-bdc6-7c4d09e8ff75', 'internal_name': 'id', 'type': 'bigint'}]}], 'checks': [], 'foreign_keys': [], 'primary_key': [{'id': '8c3895fc-262e-46db-8fb4-6f3098d29715', 'table': {'id': '5aa89543-bd03-4118-bdc6-7c4d09e8ff75', 'name': 'iris_data', 'description': 'iris data', 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'internal_name': 'iris_data', 'is_versioned': True, 'is_public': True, 'is_schema_public': True, 'owned_by': '55bad3e0-c815-103f-9c9e-43ec87b6b872'}, 'column': {'id': '5f1b7208-823d-48f7-b529-10d28c03b95d', 'name': 'Id', 'alias': None, 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '5aa89543-bd03-4118-bdc6-7c4d09e8ff75', 'internal_name': 'id', 'type': 'bigint'}}]}, 'created': '2025-05-18 09:24:19', 'last_retrieved': None, 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'internal_name': 'iris_data', 'is_versioned': True, 'is_schema_public': True, 'queue_name': 'dbrepo', 'queue_type': 'quorum', 'routing_key': 'dbrepo.f6a329d0-0cd6-4308-8c89-ad5763b42324.5aa89543-bd03-4118-bdc6-7c4d09e8ff75', 'is_public': True, 'num_rows': 150, 'data_length': 16384, 'max_data_length': 0, 'avg_row_length': 109}, {'id': '4f9cfe1e-db91-41cc-ba3e-5a49a8274b42', 'name': 'iris_data_v1', 'alias': None, 'identifiers': [], 'owner': {'id': '55bad3e0-c815-103f-9c9e-43ec87b6b872', 'username': 'reema', 'name': 'reema dass', 'orcid': None, 'qualified_name': 'reema dass ‚Äî @reema', 'given_name': 'reema', 'family_name': 'dass'}, 'description': 'iris_data_v1', 'columns': [{'id': 'a509056c-9d1b-4054-afba-4c464ee52244', 'name': 'Id', 'alias': None, 'size': None, 'd': None, 'mean': 76, 'median': 76, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '4f9cfe1e-db91-41cc-ba3e-5a49a8274b42', 'ord': 0, 'internal_name': 'id', 'index_length': None, 'length': None, 'type': 'bigint', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 43, 'is_null_allowed': False}, {'id': '3bec13be-a801-4b78-9162-39e89b27e8ae', 'name': 'SepalLengthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 6, 'median': 6, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '4f9cfe1e-db91-41cc-ba3e-5a49a8274b42', 'ord': 1, 'internal_name': 'sepallengthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 1, 'is_null_allowed': False}, {'id': 'b4073647-4641-4e3b-a453-2c8f042a7fe3', 'name': 'SepalWidthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 3, 'median': 3, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '4f9cfe1e-db91-41cc-ba3e-5a49a8274b42', 'ord': 2, 'internal_name': 'sepalwidthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 0, 'is_null_allowed': False}, {'id': '5305cc3a-6bb5-4652-afe2-0a765c2c583d', 'name': 'PetalLengthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 4, 'median': 4, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '4f9cfe1e-db91-41cc-ba3e-5a49a8274b42', 'ord': 3, 'internal_name': 'petallengthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 2, 'is_null_allowed': False}, {'id': '5dfb58b5-51aa-44b8-99dd-4bed7860557b', 'name': 'PetalWidthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 1, 'median': 1, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '4f9cfe1e-db91-41cc-ba3e-5a49a8274b42', 'ord': 4, 'internal_name': 'petalwidthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 1, 'is_null_allowed': False}, {'id': '2f7c5230-36e7-402d-b9fc-14d37e032d8d', 'name': 'Species', 'alias': None, 'size': 255, 'd': None, 'mean': None, 'median': None, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '4f9cfe1e-db91-41cc-ba3e-5a49a8274b42', 'ord': 5, 'internal_name': 'species', 'index_length': None, 'length': None, 'type': 'varchar', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': None, 'is_null_allowed': False}], 'constraints': {'uniques': [{'id': 'ee22b7f6-cc74-4bed-8460-b262abaa1840', 'name': 'uk_iris_data_v1_0', 'table': {'id': '4f9cfe1e-db91-41cc-ba3e-5a49a8274b42', 'name': 'iris_data_v1', 'description': 'iris_data_v1', 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'internal_name': 'iris_data_v1', 'is_versioned': True, 'is_public': True, 'is_schema_public': True, 'owned_by': '55bad3e0-c815-103f-9c9e-43ec87b6b872'}, 'columns': [{'id': 'a509056c-9d1b-4054-afba-4c464ee52244', 'name': 'Id', 'alias': None, 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '4f9cfe1e-db91-41cc-ba3e-5a49a8274b42', 'internal_name': 'id', 'type': 'bigint'}]}], 'checks': [], 'foreign_keys': [], 'primary_key': [{'id': '9926ca7a-740d-466e-bbf7-8b2bbdf64f3a', 'table': {'id': '4f9cfe1e-db91-41cc-ba3e-5a49a8274b42', 'name': 'iris_data_v1', 'description': 'iris_data_v1', 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'internal_name': 'iris_data_v1', 'is_versioned': True, 'is_public': True, 'is_schema_public': True, 'owned_by': '55bad3e0-c815-103f-9c9e-43ec87b6b872'}, 'column': {'id': 'a509056c-9d1b-4054-afba-4c464ee52244', 'name': 'Id', 'alias': None, 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '4f9cfe1e-db91-41cc-ba3e-5a49a8274b42', 'internal_name': 'id', 'type': 'bigint'}}]}, 'created': '2025-05-18 09:28:39', 'last_retrieved': None, 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'internal_name': 'iris_data_v1', 'is_versioned': True, 'is_schema_public': True, 'queue_name': 'dbrepo', 'queue_type': 'quorum', 'routing_key': 'dbrepo.f6a329d0-0cd6-4308-8c89-ad5763b42324.4f9cfe1e-db91-41cc-ba3e-5a49a8274b42', 'is_public': True, 'num_rows': 150, 'data_length': 16384, 'max_data_length': 0, 'avg_row_length': 109}, {'id': '1a4073f0-49ce-4ce6-8e6c-ad176e8fa871', 'name': 'iris_data_v4', 'alias': None, 'identifiers': [], 'owner': {'id': '55bad3e0-c815-103f-9c9e-43ec87b6b872', 'username': 'reema', 'name': 'reema dass', 'orcid': None, 'qualified_name': 'reema dass ‚Äî @reema', 'given_name': 'reema', 'family_name': 'dass'}, 'description': 'iris_data_v4', 'columns': [{'id': '7aedba01-fb5f-4794-93d7-2e624fa43762', 'name': 'Id', 'alias': None, 'size': None, 'd': None, 'mean': 76, 'median': 76, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '1a4073f0-49ce-4ce6-8e6c-ad176e8fa871', 'ord': 0, 'internal_name': 'id', 'index_length': None, 'length': None, 'type': 'bigint', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 43, 'is_null_allowed': False}, {'id': '282bfebb-8d39-41f0-b620-2a85bc623ff7', 'name': 'SepalLengthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 0, 'median': 0, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '1a4073f0-49ce-4ce6-8e6c-ad176e8fa871', 'ord': 1, 'internal_name': 'sepallengthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 0, 'is_null_allowed': False}, {'id': 'aad43499-3531-45f2-9389-7f65c388e087', 'name': 'SepalWidthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 0, 'median': 0, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '1a4073f0-49ce-4ce6-8e6c-ad176e8fa871', 'ord': 2, 'internal_name': 'sepalwidthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 0, 'is_null_allowed': False}, {'id': 'ef16c745-a43a-4008-ba96-92dcfd5d3878', 'name': 'PetalLengthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 0, 'median': 0, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '1a4073f0-49ce-4ce6-8e6c-ad176e8fa871', 'ord': 3, 'internal_name': 'petallengthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 0, 'is_null_allowed': False}, {'id': '5cbd27c4-3521-4f8e-b968-c6f6efa6fe34', 'name': 'PetalWidthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 0, 'median': 0, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '1a4073f0-49ce-4ce6-8e6c-ad176e8fa871', 'ord': 4, 'internal_name': 'petalwidthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 0, 'is_null_allowed': False}, {'id': 'a37ba198-a44c-413e-b888-da5836b44e66', 'name': 'Species', 'alias': None, 'size': 255, 'd': None, 'mean': None, 'median': None, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '1a4073f0-49ce-4ce6-8e6c-ad176e8fa871', 'ord': 5, 'internal_name': 'species', 'index_length': None, 'length': None, 'type': 'varchar', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': None, 'is_null_allowed': False}], 'constraints': {'uniques': [{'id': 'c0bd3d9a-b925-4230-8cc7-cfcbd17cb390', 'name': 'uk_iris_data_v4_0', 'table': {'id': '1a4073f0-49ce-4ce6-8e6c-ad176e8fa871', 'name': 'iris_data_v4', 'description': 'iris_data_v4', 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'internal_name': 'iris_data_v4', 'is_versioned': True, 'is_public': True, 'is_schema_public': True, 'owned_by': '55bad3e0-c815-103f-9c9e-43ec87b6b872'}, 'columns': [{'id': '7aedba01-fb5f-4794-93d7-2e624fa43762', 'name': 'Id', 'alias': None, 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '1a4073f0-49ce-4ce6-8e6c-ad176e8fa871', 'internal_name': 'id', 'type': 'bigint'}]}], 'checks': [], 'foreign_keys': [], 'primary_key': [{'id': '4550fad1-269f-4977-8cd9-ae1eb9bf394e', 'table': {'id': '1a4073f0-49ce-4ce6-8e6c-ad176e8fa871', 'name': 'iris_data_v4', 'description': 'iris_data_v4', 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'internal_name': 'iris_data_v4', 'is_versioned': True, 'is_public': True, 'is_schema_public': True, 'owned_by': '55bad3e0-c815-103f-9c9e-43ec87b6b872'}, 'column': {'id': '7aedba01-fb5f-4794-93d7-2e624fa43762', 'name': 'Id', 'alias': None, 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '1a4073f0-49ce-4ce6-8e6c-ad176e8fa871', 'internal_name': 'id', 'type': 'bigint'}}]}, 'created': '2025-05-18 09:30:26', 'last_retrieved': None, 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'internal_name': 'iris_data_v4', 'is_versioned': True, 'is_schema_public': True, 'queue_name': 'dbrepo', 'queue_type': 'quorum', 'routing_key': 'dbrepo.f6a329d0-0cd6-4308-8c89-ad5763b42324.1a4073f0-49ce-4ce6-8e6c-ad176e8fa871', 'is_public': True, 'num_rows': 150, 'data_length': 16384, 'max_data_length': 0, 'avg_row_length': 109}, {'id': '0c672781-25c3-438e-8fba-18c2c7f16886', 'name': 'iris_data_v3', 'alias': None, 'identifiers': [], 'owner': {'id': '55bad3e0-c815-103f-9c9e-43ec87b6b872', 'username': 'reema', 'name': 'reema dass', 'orcid': None, 'qualified_name': 'reema dass ‚Äî @reema', 'given_name': 'reema', 'family_name': 'dass'}, 'description': 'iris_data_v3', 'columns': [{'id': 'd9b9e855-5899-4b7a-8d23-09c40a10f0e3', 'name': 'Id', 'alias': None, 'size': None, 'd': None, 'mean': 76, 'median': 76, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '0c672781-25c3-438e-8fba-18c2c7f16886', 'ord': 0, 'internal_name': 'id', 'index_length': None, 'length': None, 'type': 'bigint', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 43, 'is_null_allowed': False}, {'id': 'd1f55ce2-1ed1-44e2-9839-8b5228b54a16', 'name': 'SepalLengthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 6, 'median': 6, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '0c672781-25c3-438e-8fba-18c2c7f16886', 'ord': 1, 'internal_name': 'sepallengthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 1, 'is_null_allowed': False}, {'id': 'bfd4a89d-1112-409d-b206-eca792b9d8f9', 'name': 'SepalWidthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 3, 'median': 3, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '0c672781-25c3-438e-8fba-18c2c7f16886', 'ord': 2, 'internal_name': 'sepalwidthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 0, 'is_null_allowed': False}, {'id': '9fe0bbb9-0461-4562-b8be-c4c99e083905', 'name': 'PetalLengthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 4, 'median': 4, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '0c672781-25c3-438e-8fba-18c2c7f16886', 'ord': 3, 'internal_name': 'petallengthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 2, 'is_null_allowed': False}, {'id': 'c54e7746-4179-4b31-9bd6-a0d17809c7ac', 'name': 'PetalWidthCm', 'alias': None, 'size': 40, 'd': 20, 'mean': 1, 'median': 1, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '0c672781-25c3-438e-8fba-18c2c7f16886', 'ord': 4, 'internal_name': 'petalwidthcm', 'index_length': None, 'length': None, 'type': 'decimal', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': 1, 'is_null_allowed': False}, {'id': '71249099-7722-4bec-b2bb-10a1e67861cd', 'name': 'Species', 'alias': None, 'size': 255, 'd': None, 'mean': None, 'median': None, 'concept': None, 'unit': None, 'description': None, 'enums': [], 'sets': [], 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '0c672781-25c3-438e-8fba-18c2c7f16886', 'ord': 5, 'internal_name': 'species', 'index_length': None, 'length': None, 'type': 'varchar', 'data_length': None, 'max_data_length': None, 'num_rows': None, 'val_min': None, 'val_max': None, 'std_dev': None, 'is_null_allowed': False}], 'constraints': {'uniques': [{'id': 'dfb10fdc-00e0-496e-b80f-bdd2089d2e58', 'name': 'uk_iris_data_v3_0', 'table': {'id': '0c672781-25c3-438e-8fba-18c2c7f16886', 'name': 'iris_data_v3', 'description': 'iris_data_v3', 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'internal_name': 'iris_data_v3', 'is_versioned': True, 'is_public': True, 'is_schema_public': True, 'owned_by': '55bad3e0-c815-103f-9c9e-43ec87b6b872'}, 'columns': [{'id': 'd9b9e855-5899-4b7a-8d23-09c40a10f0e3', 'name': 'Id', 'alias': None, 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '0c672781-25c3-438e-8fba-18c2c7f16886', 'internal_name': 'id', 'type': 'bigint'}]}], 'checks': [], 'foreign_keys': [], 'primary_key': [{'id': '9b74d026-7cfb-4144-9b74-780215371112', 'table': {'id': '0c672781-25c3-438e-8fba-18c2c7f16886', 'name': 'iris_data_v3', 'description': 'iris_data_v3', 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'internal_name': 'iris_data_v3', 'is_versioned': True, 'is_public': True, 'is_schema_public': True, 'owned_by': '55bad3e0-c815-103f-9c9e-43ec87b6b872'}, 'column': {'id': 'd9b9e855-5899-4b7a-8d23-09c40a10f0e3', 'name': 'Id', 'alias': None, 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'table_id': '0c672781-25c3-438e-8fba-18c2c7f16886', 'internal_name': 'id', 'type': 'bigint'}}]}, 'created': '2025-05-18 09:29:51', 'last_retrieved': None, 'database_id': 'f6a329d0-0cd6-4308-8c89-ad5763b42324', 'internal_name': 'iris_data_v3', 'is_versioned': True, 'is_schema_public': True, 'queue_name': 'dbrepo', 'queue_type': 'quorum', 'routing_key': 'dbrepo.f6a329d0-0cd6-4308-8c89-ad5763b42324.0c672781-25c3-438e-8fba-18c2c7f16886', 'is_public': True, 'num_rows': 150, 'data_length': 16384, 'max_data_length': 0, 'avg_row_length': 109}], 'views': [], 'container': {'id': '6cfb3b8e-1792-4e46-871a-f3d103527203', 'name': 'mariadb:11.3.2', 'image': {'id': 'd79cb089-363c-488b-9717-649e44d8fcc5', 'name': 'mariadb', 'version': '11.1.3', 'operators': [{'id': '2d24ddd7-33c9-11f0-bcbb-e2d84432ea81', 'value': '!=', 'documentation': 'https://mariadb.com/kb/en/not-equal/', 'display_name': 'Not equal operator'}, {'id': '2d24daab-33c9-11f0-bcbb-e2d84432ea81', 'value': '<', 'documentation': 'https://mariadb.com/kb/en/less-than/', 'display_name': 'Less-than operator'}, {'id': '2d24db68-33c9-11f0-bcbb-e2d84432ea81', 'value': '<=', 'documentation': 'https://mariadb.com/kb/en/less-than-or-equal/', 'display_name': 'Less than or equal operator'}, {'id': '2d24d8a3-33c9-11f0-bcbb-e2d84432ea81', 'value': '<=>', 'documentation': 'https://mariadb.com/kb/en/null-safe-equal/', 'display_name': 'NULL-safe equal operator'}, {'id': '2d24d4ff-33c9-11f0-bcbb-e2d84432ea81', 'value': '=', 'documentation': 'https://mariadb.com/kb/en/assignment-operators-assignment-operator/', 'display_name': 'Equal operator'}, {'id': '2d24dcca-33c9-11f0-bcbb-e2d84432ea81', 'value': '>', 'documentation': 'https://mariadb.com/kb/en/greater-than/', 'display_name': 'Greater-than operator'}, {'id': '2d24dd51-33c9-11f0-bcbb-e2d84432ea81', 'value': '>=', 'documentation': 'https://mariadb.com/kb/en/greater-than-or-equal/', 'display_name': 'Greater than or equal operator'}, {'id': '2d24df85-33c9-11f0-bcbb-e2d84432ea81', 'value': 'IN', 'documentation': 'https://mariadb.com/kb/en/in/', 'display_name': 'IN'}, {'id': '2d24e07f-33c9-11f0-bcbb-e2d84432ea81', 'value': 'IS NOT NULL', 'documentation': 'https://mariadb.com/kb/en/is-not-null/', 'display_name': 'IS NOT NULL'}, {'id': '2d24e0f7-33c9-11f0-bcbb-e2d84432ea81', 'value': 'IS NULL', 'documentation': 'https://mariadb.com/kb/en/is-null/', 'display_name': 'IS NULL'}, {'id': '2d24de7c-33c9-11f0-bcbb-e2d84432ea81', 'value': 'LIKE', 'documentation': 'https://mariadb.com/kb/en/like/', 'display_name': 'LIKE'}, {'id': '2d24dfff-33c9-11f0-bcbb-e2d84432ea81', 'value': 'NOT IN', 'documentation': 'https://mariadb.com/kb/en/not-in/', 'display_name': 'NOT IN'}, {'id': '2d24df05-33c9-11f0-bcbb-e2d84432ea81', 'value': 'NOT LIKE', 'documentation': 'https://mariadb.com/kb/en/not-like/', 'display_name': 'NOT LIKE'}, {'id': '2d24e1e5-33c9-11f0-bcbb-e2d84432ea81', 'value': 'NOT REGEXP', 'documentation': 'https://mariadb.com/kb/en/not-regexp/', 'display_name': 'NOT REGEXP'}, {'id': '2d24e16a-33c9-11f0-bcbb-e2d84432ea81', 'value': 'REGEXP', 'documentation': 'https://mariadb.com/kb/en/regexp/', 'display_name': 'REGEXP'}], 'default': False, 'data_types': [{'id': '2d23cc31-33c9-11f0-bcbb-e2d84432ea81', 'value': 'bigint', 'documentation': 'https://mariadb.com/kb/en/bigint/', 'display_name': 'BIGINT(size)', 'size_min': 0, 'size_max': None, 'size_default': None, 'size_required': False, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': None, 'is_quoted': False, 'is_buildable': True}, {'id': '2d23cf57-33c9-11f0-bcbb-e2d84432ea81', 'value': 'binary', 'documentation': 'https://mariadb.com/kb/en/binary/', 'display_name': 'BINARY(size)', 'size_min': 0, 'size_max': 255, 'size_default': 255, 'size_required': True, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': 'size in Bytes', 'is_quoted': False, 'is_buildable': True}, {'id': '2d23d188-33c9-11f0-bcbb-e2d84432ea81', 'value': 'bit', 'documentation': 'https://mariadb.com/kb/en/bit/', 'display_name': 'BIT(size)', 'size_min': 0, 'size_max': 64, 'size_default': None, 'size_required': False, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': None, 'is_quoted': False, 'is_buildable': True}, {'id': '2d23d262-33c9-11f0-bcbb-e2d84432ea81', 'value': 'blob', 'documentation': 'https://mariadb.com/kb/en/blob/', 'display_name': 'BLOB(size)', 'size_min': 0, 'size_max': 65535, 'size_default': None, 'size_required': False, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': 'size in Bytes', 'is_quoted': False, 'is_buildable': False}, {'id': '2d23d301-33c9-11f0-bcbb-e2d84432ea81', 'value': 'bool', 'documentation': 'https://mariadb.com/kb/en/bool/', 'display_name': 'BOOL', 'size_min': None, 'size_max': None, 'size_default': None, 'size_required': None, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': None, 'is_quoted': False, 'is_buildable': True}, {'id': '2d23d38a-33c9-11f0-bcbb-e2d84432ea81', 'value': 'char', 'documentation': 'https://mariadb.com/kb/en/char/', 'display_name': 'CHAR(size)', 'size_min': 0, 'size_max': 255, 'size_default': 255, 'size_required': False, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': None, 'is_quoted': False, 'is_buildable': True}, {'id': '2d23d3f0-33c9-11f0-bcbb-e2d84432ea81', 'value': 'date', 'documentation': 'https://mariadb.com/kb/en/date/', 'display_name': 'DATE', 'size_min': None, 'size_max': None, 'size_default': None, 'size_required': None, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': 'e.g. YYYY-MM-DD, YY-MM-DD, YYMMDD, YYYY/MM/DD', 'type_hint': 'min. 1000-01-01, max. 9999-12-31', 'is_quoted': True, 'is_buildable': True}, {'id': '2d23d4bf-33c9-11f0-bcbb-e2d84432ea81', 'value': 'datetime', 'documentation': 'https://mariadb.com/kb/en/datetime/', 'display_name': 'DATETIME(fsp)', 'size_min': 0, 'size_max': 6, 'size_default': None, 'size_required': None, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': 'e.g. YYYY-MM-DD HH:MM:SS, YY-MM-DD HH:MM:SS, YYYYMMDDHHMMSS, YYMMDDHHMMSS, YYYYMMDD, YYMMDD', 'type_hint': 'fsp=microsecond precision, min. 1000-01-01 00:00:00.0, max. 9999-12-31 23:59:59.9', 'is_quoted': True, 'is_buildable': True}, {'id': '2d23d569-33c9-11f0-bcbb-e2d84432ea81', 'value': 'decimal', 'documentation': 'https://mariadb.com/kb/en/decimal/', 'display_name': 'DECIMAL(size, d)', 'size_min': 0, 'size_max': 65, 'size_default': None, 'size_required': False, 'd_min': 0, 'd_max': 38, 'd_default': None, 'd_required': False, 'data_hint': None, 'type_hint': None, 'is_quoted': False, 'is_buildable': True}, {'id': '2d23d5f9-33c9-11f0-bcbb-e2d84432ea81', 'value': 'double', 'documentation': 'https://mariadb.com/kb/en/double/', 'display_name': 'DOUBLE(size, d)', 'size_min': None, 'size_max': None, 'size_default': None, 'size_required': False, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': False, 'data_hint': None, 'type_hint': None, 'is_quoted': False, 'is_buildable': True}, {'id': '2d23d67a-33c9-11f0-bcbb-e2d84432ea81', 'value': 'enum', 'documentation': 'https://mariadb.com/kb/en/enum/', 'display_name': 'ENUM(v1,v2,...)', 'size_min': None, 'size_max': None, 'size_default': None, 'size_required': None, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': 'e.g. value1, value2, ...', 'type_hint': None, 'is_quoted': True, 'is_buildable': True}, {'id': '2d23d6ec-33c9-11f0-bcbb-e2d84432ea81', 'value': 'float', 'documentation': 'https://mariadb.com/kb/en/float/', 'display_name': 'FLOAT(size)', 'size_min': None, 'size_max': None, 'size_default': None, 'size_required': False, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': None, 'is_quoted': False, 'is_buildable': True}, {'id': '2d23d79c-33c9-11f0-bcbb-e2d84432ea81', 'value': 'int', 'documentation': 'https://mariadb.com/kb/en/int/', 'display_name': 'INT(size)', 'size_min': None, 'size_max': None, 'size_default': None, 'size_required': False, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': 'size in Bytes', 'is_quoted': False, 'is_buildable': True}, {'id': '2d23d81f-33c9-11f0-bcbb-e2d84432ea81', 'value': 'longblob', 'documentation': 'https://mariadb.com/kb/en/longblob/', 'display_name': 'LONGBLOB', 'size_min': None, 'size_max': None, 'size_default': None, 'size_required': None, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': 'max. 3.999 GiB', 'is_quoted': False, 'is_buildable': True}, {'id': '2d23d8a4-33c9-11f0-bcbb-e2d84432ea81', 'value': 'longtext', 'documentation': 'https://mariadb.com/kb/en/longtext/', 'display_name': 'LONGTEXT', 'size_min': None, 'size_max': None, 'size_default': None, 'size_required': None, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': 'max. 3.999 GiB', 'is_quoted': True, 'is_buildable': True}, {'id': '2d23d937-33c9-11f0-bcbb-e2d84432ea81', 'value': 'mediumblob', 'documentation': 'https://mariadb.com/kb/en/mediumblob/', 'display_name': 'MEDIUMBLOB', 'size_min': None, 'size_max': None, 'size_default': None, 'size_required': None, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': 'max. 15.999 MiB', 'is_quoted': False, 'is_buildable': True}, {'id': '2d23d993-33c9-11f0-bcbb-e2d84432ea81', 'value': 'mediumint', 'documentation': 'https://mariadb.com/kb/en/mediumint/', 'display_name': 'MEDIUMINT', 'size_min': None, 'size_max': None, 'size_default': None, 'size_required': None, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': 'size in Bytes', 'is_quoted': False, 'is_buildable': True}, {'id': '2d23d9df-33c9-11f0-bcbb-e2d84432ea81', 'value': 'mediumtext', 'documentation': 'https://mariadb.com/kb/en/mediumtext/', 'display_name': 'MEDIUMTEXT', 'size_min': None, 'size_max': None, 'size_default': None, 'size_required': None, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': 'size in Bytes', 'is_quoted': True, 'is_buildable': True}, {'id': '2d23da2d-33c9-11f0-bcbb-e2d84432ea81', 'value': 'serial', 'documentation': 'https://mariadb.com/kb/en/bigint/', 'display_name': 'SERIAL', 'size_min': None, 'size_max': None, 'size_default': None, 'size_required': None, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': None, 'is_quoted': True, 'is_buildable': True}, {'id': '2d23da7a-33c9-11f0-bcbb-e2d84432ea81', 'value': 'set', 'documentation': 'https://mariadb.com/kb/en/set/', 'display_name': 'SET(v1,v2,...)', 'size_min': None, 'size_max': None, 'size_default': None, 'size_required': None, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': 'e.g. value1, value2, ...', 'type_hint': None, 'is_quoted': True, 'is_buildable': True}, {'id': '2d23dae0-33c9-11f0-bcbb-e2d84432ea81', 'value': 'smallint', 'documentation': 'https://mariadb.com/kb/en/smallint/', 'display_name': 'SMALLINT(size)', 'size_min': 0, 'size_max': None, 'size_default': None, 'size_required': False, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': 'size in Bytes', 'is_quoted': False, 'is_buildable': True}, {'id': '2d23db2d-33c9-11f0-bcbb-e2d84432ea81', 'value': 'text', 'documentation': 'https://mariadb.com/kb/en/text/', 'display_name': 'TEXT(size)', 'size_min': 0, 'size_max': None, 'size_default': None, 'size_required': False, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': 'size in Bytes', 'is_quoted': True, 'is_buildable': True}, {'id': '2d23db78-33c9-11f0-bcbb-e2d84432ea81', 'value': 'time', 'documentation': 'https://mariadb.com/kb/en/time/', 'display_name': 'TIME(fsp)', 'size_min': 0, 'size_max': 6, 'size_default': 0, 'size_required': False, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': 'e.g. HH:MM:SS, HH:MM, HHMMSS, H:M:S', 'type_hint': 'fsp=microsecond precision, min. 0, max. 6', 'is_quoted': True, 'is_buildable': True}, {'id': '2d23dbc9-33c9-11f0-bcbb-e2d84432ea81', 'value': 'timestamp', 'documentation': 'https://mariadb.com/kb/en/timestamp/', 'display_name': 'TIMESTAMP(fsp)', 'size_min': 0, 'size_max': 6, 'size_default': 0, 'size_required': False, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': 'e.g. YYYY-MM-DD HH:MM:SS, YY-MM-DD HH:MM:SS, YYYYMMDDHHMMSS, YYMMDDHHMMSS, YYYYMMDD, YYMMDD', 'type_hint': 'fsp=microsecond precision, min. 0, max. 6', 'is_quoted': True, 'is_buildable': True}, {'id': '2d23dc24-33c9-11f0-bcbb-e2d84432ea81', 'value': 'tinyblob', 'documentation': 'https://mariadb.com/kb/en/timestamp/', 'display_name': 'TINYBLOB', 'size_min': None, 'size_max': None, 'size_default': None, 'size_required': None, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': 'fsp=microsecond precision, min. 0, max. 6', 'type_hint': None, 'is_quoted': False, 'is_buildable': True}, {'id': '2d23dc74-33c9-11f0-bcbb-e2d84432ea81', 'value': 'tinyint', 'documentation': 'https://mariadb.com/kb/en/tinyint/', 'display_name': 'TINYINT(size)', 'size_min': 0, 'size_max': None, 'size_default': None, 'size_required': False, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': 'size in Bytes', 'type_hint': None, 'is_quoted': False, 'is_buildable': True}, {'id': '2d23dcc0-33c9-11f0-bcbb-e2d84432ea81', 'value': 'tinytext', 'documentation': 'https://mariadb.com/kb/en/tinytext/', 'display_name': 'TINYTEXT', 'size_min': None, 'size_max': None, 'size_default': None, 'size_required': None, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': 'max. 255 characters', 'type_hint': None, 'is_quoted': True, 'is_buildable': True}, {'id': '2d23dd19-33c9-11f0-bcbb-e2d84432ea81', 'value': 'year', 'documentation': 'https://mariadb.com/kb/en/year/', 'display_name': 'YEAR', 'size_min': 2, 'size_max': 4, 'size_default': None, 'size_required': False, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': 'e.g. YYYY, YY', 'type_hint': 'min. 1901, max. 2155', 'is_quoted': False, 'is_buildable': True}, {'id': '2d23dd71-33c9-11f0-bcbb-e2d84432ea81', 'value': 'varbinary', 'documentation': 'https://mariadb.com/kb/en/varbinary/', 'display_name': 'VARBINARY(size)', 'size_min': 0, 'size_max': None, 'size_default': None, 'size_required': True, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': None, 'is_quoted': False, 'is_buildable': True}, {'id': '2d23ddc3-33c9-11f0-bcbb-e2d84432ea81', 'value': 'varchar', 'documentation': 'https://mariadb.com/kb/en/varchar/', 'display_name': 'VARCHAR(size)', 'size_min': 0, 'size_max': 65532, 'size_default': 255, 'size_required': True, 'd_min': None, 'd_max': None, 'd_default': None, 'd_required': None, 'data_hint': None, 'type_hint': None, 'is_quoted': False, 'is_buildable': True}]}, 'quota': None, 'count': None, 'username': None, 'password': None, 'last_retrieved': None, 'internal_name': 'mariadb_11_3_2'}, 'accesses': [], 'identifiers': [], 'subsets': [], 'contact': {'id': '55bad3e0-c815-103f-9c9e-43ec87b6b872', 'username': 'reema', 'name': 'reema dass', 'orcid': None, 'qualified_name': 'reema dass ‚Äî @reema', 'given_name': 'reema', 'family_name': 'dass'}, 'owner': {'id': '55bad3e0-c815-103f-9c9e-43ec87b6b872', 'username': 'reema', 'name': 'reema dass', 'orcid': None, 'qualified_name': 'reema dass ‚Äî @reema', 'given_name': 'reema', 'family_name': 'dass'}, 'created': '2025-05-18 09:22:32', 'last_retrieved': None, 'dashboard_uid': 'bem8sq0qnka9sf', 'exchange_name': 'dbrepo', 'exchange_type': None, 'internal_name': 'iris_wmeb', 'is_public': True, 'is_schema_public': True, 'is_dashboard_enabled': True, 'preview_image': None}\n",
      "[{'timestamp': '2025-05-18T09:29:55.721Z', 'event': 'insert', 'total': 150}]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter test size (e.g., 0.2 for 20% test set):  fssf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid input. Defaulting to 0.2\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter random seed (e.g., 42):  fsvd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid input. Defaulting to 42\n",
      "Choose a model to train:\n",
      "1. random_forest\n",
      "2. decision_tree\n",
      "3. logistic_regression\n",
      "4. knn\n",
      "5. svm\n",
      "6. gradient_boosting\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter model number (default 1 for random_forest):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `n_estimators` (Hyperparameter configuration)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Why did you choose this value?  bd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `criterion` (Hyperparameter configuration)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Why did you choose this value?  fbd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `max_depth` (Hyperparameter configuration)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Why did you choose this value?  sffffffffffff\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `min_samples_split` (Hyperparameter configuration)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Why did you choose this value?  fvdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `min_samples_leaf` (Hyperparameter configuration)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Why did you choose this value?  dbfd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `max_features` (Hyperparameter configuration)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Why did you choose this value?  vcxbs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `bootstrap` (Hyperparameter configuration)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Why did you choose this value?  dssf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `oob_score` (Hyperparameter configuration)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Why did you choose this value?  fddfb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `class_weight` (Hyperparameter configuration)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Why did you choose this value?  fbdbd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `verbose` (Hyperparameter configuration)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Why did you choose this value?  fbbbd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `n_jobs` (Hyperparameter configuration)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Why did you choose this value?  dfbd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df1394dd45b43f781e6cf633990acd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `model_choice`\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Why did you choose this model (e.g., RandomForestClassifier) for this task?  fdd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `target_variable`\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Why did you choose this column as the prediction target?  gdbvd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `test_split`\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Why this train/test ratio (e.g., 80/20)?  dfvdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `metric_choice`\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Why did you use accuracy/f1/ROC-AUC as your evaluation metric?  dfbd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `threshold_accuracy`\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Was there a threshold for accuracy? Why?  dbdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `dataset_version`\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Why did you use this specific dataset version?  dbdb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `drop_column_X`\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Why did you drop any specific columns from the dataset?  dbdb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `experiment_name`\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Any context behind this experiment name or setup?  ddb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `model_limitations`\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Any known model limitations?  dbdb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `ethical_considerations`\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Any known model ethical considerations?  dbdbd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `intended_use`\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Known model intended use?  dbdb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Justification for `not_intended_for`\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚Üí Model not_intended_for?  db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reema\\anaconda3\\Lib\\site-packages\\shap\\plots\\_beeswarm.py:1153: UserWarning: The figure layout has changed to tight\n",
      "  pl.tight_layout()\n",
      "C:\\Users\\reema\\anaconda3\\Lib\\site-packages\\shap\\plots\\_beeswarm.py:761: UserWarning: The figure layout has changed to tight\n",
      "  pl.tight_layout(pad=0, w_pad=0, h_pad=0.0)\n",
      "C:\\Users\\reema\\AppData\\Local\\Temp\\ipykernel_27980\\1830696726.py:327: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Commit successful.\n",
      "üöÄ Push successful.\n",
      "‚ö†Ô∏è Commit c53df165 is not tagged with a version.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üîñ Enter version tag for this commit (or press Enter to skip):  v99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged tag: DOI_dataset_id\n",
      "‚úÖ Logged tag: DOI_dataset_title\n",
      "‚úÖ Logged tag: DOI_dataset_description\n",
      "‚úÖ Logged tag: DOI_dataset_creator\n",
      "‚úÖ Logged tag: DOI_dataset_publisher\n",
      "‚úÖ Logged tag: DOI_dataset_publication_date\n",
      "‚úÖ Logged tag: DOI_dataset_license\n",
      "‚úÖ Logged tag: DOI_dataset_keywords\n",
      "‚úÖ Logged tag: DOI_dataset_access_url\n",
      "‚úÖ Logged tag: DOI_dataset_documentation\n",
      "‚úÖ Logged tag: DOI_metadata_standard\n",
      "‚úÖ Logged tag: DOI_related_resources\n",
      "‚úÖ Logged tag: DOI_prov_entity\n",
      "‚úÖ Logged tag: DOI_prov_activity\n",
      "‚úÖ Logged tag: DOI_prov_agent_dataset_creator\n",
      "‚úÖ Logged tag: DOI_prov_used\n",
      "‚úÖ Logged tag: DOI_prov_wasDerivedFrom\n",
      "‚úÖ Logged tag: DOI_prov_wasAttributedTo\n",
      "‚úÖ Logged tag: DOI_prov_startedAtTime\n",
      "‚úÖ Logged tag: DOI_prov_role_dataset_creator\n",
      "‚úÖ Logged tag: DOI_prov_role_database_creator\n",
      "üìÅ Full metadata snapshot logged as: doi_metadata_snapshot.json\n",
      "üìÅ Run summary JSON logged at: C:\\Users\\reema\\REPO\\notebooks\\RQ_notebooks\\MODEL_PROVENANCE\\RandomForest_Iris_v20250518_170352\\RandomForest_Iris_v20250518_170352_run_summary.json\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    client = MlflowClient()\n",
    "    run_data = client.get_run(run.info.run_id).data\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Session Metadata ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    session_metadata = collect_session_metadata(prompt_fields=True)\n",
    "    mlflow.log_params(session_metadata)  # [PROV, Internal] Session and environment context\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Dataset Metadata ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    doi_metadata = extract_dataset_metadata_from_doi(\"10.24432/C56C76\")  # [FAIR, PROV, FAIR4ML]\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Experiment Start Time ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    start_time = datetime.now().isoformat()\n",
    "    mlflow.set_tag(\"startedAtTime\", start_time)  # [PROV] Activity start time\n",
    "\n",
    "    #######################################################################\n",
    "    ### Preprocessing #####################################################\n",
    "\n",
    "    # ‚îÄ‚îÄ Load into a DataFrame ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    df = pd.DataFrame(dataset)\n",
    "    original_row_count = df.shape[0]\n",
    "    mlflow.log_param(\"input_row_count\", original_row_count)  # [MLSEA] Input data size\n",
    "\n",
    "    # Log column names before transformation\n",
    "    mlflow.set_tag(\"raw_columns\", ','.join(df.columns))  # [FAIR4ML, Internal]\n",
    "\n",
    "    # ‚îÄ‚îÄ Generate row hashes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    before_hashes = set(df.astype(str).apply(lambda row: hash(tuple(row)), axis=1))\n",
    "    mlflow.set_tag(\"row_hash_tracking\", \"enabled\")  # [Internal] Used for provenance/repeatability\n",
    "\n",
    "    # ‚îÄ‚îÄ Extract target variable ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    target_col = df.columns[-1]\n",
    "    mlflow.set_tag(\"target_variable\", target_col)  # [FAIR4ML, MLSEA]\n",
    "\n",
    "    # ‚îÄ‚îÄ Separate features and labels ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    y = df[target_col]\n",
    "    X = df.drop(columns=[target_col])\n",
    "    mlflow.set_tag(\"feature_columns\", ','.join(X.columns))  # [FAIR4ML, MLSEA]\n",
    "\n",
    "    # ‚îÄ‚îÄ Drop ID columns (case-insensitive) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    id_cols = [c for c in X.columns if c.lower() == \"id\"]\n",
    "    if id_cols:\n",
    "        X = X.drop(columns=id_cols)\n",
    "        mlflow.set_tag(\"dropped_id_columns\", ','.join(id_cols))  # [Internal]\n",
    "\n",
    "    # ‚îÄ‚îÄ Convert columns to numeric where possible ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    numeric_conversion_count = 0\n",
    "    for c in X.columns:\n",
    "        try:\n",
    "            X[c] = pd.to_numeric(X[c])\n",
    "            numeric_conversion_count += 1\n",
    "        except Exception:\n",
    "            continue\n",
    "    mlflow.log_param(\"numeric_columns_converted\", numeric_conversion_count)  # [Internal, FAIR4ML]\n",
    "\n",
    "    # ‚îÄ‚îÄ Print diagnostic info ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    print(\"ML_EXP_Shapes:\", X.shape, y.shape)\n",
    "    mlflow.log_param(\"feature_matrix_shape\", str(X.shape))  # [MLSEA]\n",
    "    mlflow.log_param(\"label_vector_shape\", str(y.shape))    # [MLSEA]\n",
    "#######################################################################################################\n",
    "### 8) Label Encoding and Metadata Logging ############################################################\n",
    "\n",
    "# ‚îÄ‚îÄ Encode class labels numerically ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    print(\"ML_EXP_Classes:\", le.classes_)\n",
    "    \n",
    "    mlflow.set_tag(\"class_names\", ','.join(le.classes_))  # [FAIR4ML, MLSEA]\n",
    "    \n",
    "    # ‚îÄ‚îÄ Count rows and hash comparison before vs after preprocessing ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    count_end = df.shape[0]\n",
    "    after_hashes = set(df.astype(str).apply(lambda row: hash(tuple(row)), axis=1))\n",
    "    \n",
    "    n_insert = len(after_hashes - before_hashes)\n",
    "    n_delete = len(before_hashes - after_hashes)\n",
    "    \n",
    "    #######################################################################################################\n",
    "    ### Metadata Logging (Standardized Format) ############################################################\n",
    "    \n",
    "    # ‚îÄ‚îÄ Extended DB Metadata ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    db_meta = fetch_db_dataset_metadata(db_id, selected_table_id, selected_version, target_col, df.shape[0])  # [Internal]\n",
    "    \n",
    "    mlflow.set_tag(\"Internal_DBRepo_table_last_modified\", db_meta.get(\"dataset_publication_date\", \"unknown\"))\n",
    "  # [PROV]\n",
    "    \n",
    "    # ‚îÄ‚îÄ Row Count Metrics ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    mlflow.log_metric(\"row_count_start\", original_row_count)              # [MLSEA, FAIR4ML]\n",
    "    mlflow.log_metric(\"row_count_end\", count_end)                  # [MLSEA, FAIR4ML]\n",
    "    mlflow.log_metric(\"num_inserted_rows\", n_insert)               # [PROV]\n",
    "    mlflow.log_metric(\"num_deleted_rows\", n_delete)                # [PROV]\n",
    "    \n",
    "    # ‚îÄ‚îÄ Raw Data Source Metadata ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    mlflow.set_tag(\"data_source\", API_URL)                         # [FAIR]\n",
    "    mlflow.log_param(\"retrieval_time_utc\", datetime.utcnow().isoformat())  # [PROV]\n",
    "    mlflow.log_param(\"raw_row_count\", len(df))                     # [MLSEA]\n",
    "    mlflow.log_param(\"raw_columns\", df.columns.tolist())           # [FAIR4ML]\n",
    "    mlflow.log_param(\"dropped_columns\", id_cols)                   # [Internal]\n",
    "    \n",
    "    # ‚îÄ‚îÄ Post-Processing Metadata ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    mlflow.log_param(\"final_num_features\", X.shape[1])             # [MLSEA]\n",
    "    mlflow.log_param(\"final_feature_names\", X.columns.tolist())    # [FAIR4ML]\n",
    "    mlflow.set_tag(\"target_variable_encoded\", target_col)          # [FAIR4ML]\n",
    "    \n",
    "    # ‚îÄ‚îÄ Label Mapping as Artifact ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    label_map = {int(idx): cls for idx, cls in enumerate(le.classes_)}\n",
    "    buffer = io.StringIO()\n",
    "    json.dump(label_map, buffer, indent=2)\n",
    "    buffer.seek(0)\n",
    "    mlflow.log_text(buffer.getvalue(), artifact_file=\"label_mapping.json\")  # [FAIR4ML]\n",
    "    \n",
    "    # ‚îÄ‚îÄ Training Metadata ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_name = f\"RandomForest_Iris_v{ts}\"\n",
    "    mlflow.set_tag(\"model_name\", model_name)                       # [MLSEA]\n",
    "    \n",
    "    train_start_ts = datetime.now().isoformat()\n",
    "    mlflow.set_tag(\"training_start_time\", train_start_ts)          # [PROV]\n",
    "########################################################################################################\n",
    "### Model Parameters & Split Metadata ##################################################################\n",
    "\n",
    "# ‚îÄ‚îÄ Prompt test size and seed ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    try:\n",
    "        test_size = float(input(\"Enter test size (e.g., 0.2 for 20% test set): \"))\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Defaulting to 0.2\")\n",
    "        test_size = 0.2\n",
    "    \n",
    "    try:\n",
    "        random_state = int(input(\"Enter random seed (e.g., 42): \"))\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Defaulting to 42\")\n",
    "        random_state = 42\n",
    "    \n",
    "    # ‚îÄ‚îÄ Train/test split ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # ‚îÄ‚îÄ Log split config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    mlflow.log_param(\"test_size\", test_size)                     # [MLSEA]\n",
    "    mlflow.log_param(\"random_seed\", random_state)               # [PROV]\n",
    "    mlflow.log_param(\"n_train_samples\", X_train.shape[0])       # [FAIR4ML]\n",
    "    mlflow.log_param(\"n_test_samples\",  X_test.shape[0])        # [FAIR4ML]\n",
    "    mlflow.log_param(\"n_features\",      X_train.shape[1])       # [MLSEA]\n",
    "    \n",
    "    ########################################################################################################\n",
    "    ### Model Selection & Hyperparameters ##################################################################\n",
    "    \n",
    "    # ‚îÄ‚îÄ Define hyperparameters ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    ML_EXP_hyperparams = {\n",
    "        \"n_estimators\":       100,\n",
    "        \"criterion\":          \"entropy\",\n",
    "        \"max_depth\":          10,\n",
    "        \"min_samples_split\":  3,\n",
    "        \"min_samples_leaf\":   1,\n",
    "        \"max_features\":       \"sqrt\",\n",
    "        \"bootstrap\":          True,\n",
    "        \"oob_score\":          True,\n",
    "        \"class_weight\":       None,\n",
    "        \"verbose\":            1,\n",
    "        \"n_jobs\":             -1\n",
    "    }\n",
    "    \n",
    "    # ‚îÄ‚îÄ Model selection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    available_models = {\n",
    "        \"random_forest\": RandomForestClassifier,\n",
    "        \"decision_tree\": DecisionTreeClassifier,\n",
    "        \"logistic_regression\": LogisticRegression,\n",
    "        \"knn\": KNeighborsClassifier,\n",
    "        \"svm\": SVC,\n",
    "        \"gradient_boosting\": GradientBoostingClassifier\n",
    "    }\n",
    "    \n",
    "    # User prompt\n",
    "    print(\"Choose a model to train:\")\n",
    "    for i, name in enumerate(available_models.keys()):\n",
    "        print(f\"{i + 1}. {name}\")\n",
    "    \n",
    "    choice = input(\"Enter model number (default 1 for random_forest): \").strip()\n",
    "    choice = int(choice) if choice else 1\n",
    "    selected_key = list(available_models.keys())[choice - 1]\n",
    "    selected_model_class = available_models[selected_key]\n",
    "    mlflow.set_tag(\"selected_model\", selected_key)  # [FAIR4ML, MLSEA]\n",
    "    \n",
    "    # ‚îÄ‚îÄ Initialize model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    model = selected_model_class(**ML_EXP_hyperparams)\n",
    "    \n",
    "    # ‚îÄ‚îÄ Log hyperparameters with justification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    for key, val in ML_EXP_hyperparams.items():\n",
    "        log_with_justification(mlflow.log_param, key, val, context=\"Hyperparameter configuration\")  # [FAIR4ML, MLSEA]\n",
    "    \n",
    "    ########################################################################################################\n",
    "    ### Model Training & Evaluation ########################################################################\n",
    "    \n",
    "    # ‚îÄ‚îÄ Fit the model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    model.fit(X_train, y_train)\n",
    "    train_end_ts = datetime.now().isoformat()\n",
    "    mlflow.set_tag(\"training_end_time\", train_end_ts)  # [PROV]\n",
    "    \n",
    "    # ‚îÄ‚îÄ Predictions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # ‚îÄ‚îÄ Compute and log metrics ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    acc  = accuracy_score(y_test, y_pred)\n",
    "    auc  = roc_auc_score(y_test, y_proba, multi_class=\"ovr\")\n",
    "    prec = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    rec  = recall_score(y_test,  y_pred, average=\"macro\")\n",
    "    f1   = f1_score(y_test,      y_pred, average=\"macro\")\n",
    "    \n",
    "    mlflow.log_metric(\"accuracy\", acc)              # [MLSEA]\n",
    "    mlflow.log_metric(\"roc_auc\", auc)               # [MLSEA]\n",
    "    mlflow.log_metric(\"precision_macro\", prec)      # [MLSEA]\n",
    "    mlflow.log_metric(\"recall_macro\", rec)          # [MLSEA]\n",
    "    mlflow.log_metric(\"f1_macro\", f1)               # [MLSEA]\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "### Final Logging: Justifications, Metrics, Environment, Dataset Metadata #############################\n",
    "\n",
    "# ‚îÄ‚îÄ Prompt for and log justifications ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    log_justification(\"model_choice\", \"Why did you choose this model (e.g., RandomForestClassifier) for this task?\")\n",
    "    log_justification(\"target_variable\", \"Why did you choose this column as the prediction target?\")\n",
    "    log_justification(\"test_split\", \"Why this train/test ratio (e.g., 80/20)?\")\n",
    "    log_justification(\"metric_choice\", \"Why did you use accuracy/f1/ROC-AUC as your evaluation metric?\")\n",
    "    log_justification(\"threshold_accuracy\", \"Was there a threshold for accuracy? Why?\")\n",
    "    log_justification(\"dataset_version\", \"Why did you use this specific dataset version?\")\n",
    "    log_justification(\"drop_column_X\", \"Why did you drop any specific columns from the dataset?\")\n",
    "    log_justification(\"experiment_name\", \"Any context behind this experiment name or setup?\")\n",
    "    log_justification(\"model_limitations\", \"Any known model limitations?\")\n",
    "    log_justification(\"ethical_considerations\", \"Any known model ethical considerations?\")\n",
    "    log_justification(\"intended_use\", \"Known model intended use?\")\n",
    "    log_justification(\"not_intended_for\", \"Model not_intended_for?\")\n",
    "\n",
    "\n",
    "    # ‚îÄ‚îÄ Log model evaluation metrics ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    mlflow.log_metric(\"precision_macro\", prec)    # [MLSEA]\n",
    "    mlflow.log_metric(\"recall_macro\", rec)        # [MLSEA]\n",
    "    mlflow.log_metric(\"f1_macro\", f1)             # [MLSEA]\n",
    "    mlflow.log_metric(\"accuracy\", acc)            # [MLSEA]\n",
    "    mlflow.log_metric(\"roc_auc\", auc)             # [MLSEA]\n",
    "    \n",
    "    # ‚îÄ‚îÄ Log environment info ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    mlflow.log_params({\n",
    "        \"python_version\":       platform.python_version(),\n",
    "        \"os_platform\":          f\"{platform.system()} {platform.release()}\",\n",
    "        \"sklearn_version\":      sklearn.__version__,\n",
    "        \"pandas_version\":       pd.__version__,\n",
    "        \"numpy_version\":        np.__version__,\n",
    "        \"matplotlib_version\":   matplotlib.__version__,\n",
    "        \"seaborn_version\":      sns.__version__,\n",
    "        \"shap_version\":         shap.__version__,\n",
    "    })  # [PROV, Internal]\n",
    "    \n",
    "    # ‚îÄ‚îÄ Tag notebook name ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    mlflow.set_tag(\"notebook_name\", \"RQ1_2.ipynb\")  # [Internal]\n",
    "    \n",
    "    # ‚îÄ‚îÄ Dataset metadata tags ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    mlflow.set_tag(\"dataset_name\",    db_meta.get(\"dataset_name\", \"unknown\") )    # [FAIR4ML, PROV]\n",
    "    mlflow.set_tag(\"dataset_version\", selected_version)                                           # [FAIR4ML, Internal]\n",
    "    mlflow.set_tag(\"dataset_id\",      selected_table_id)  # [FAIR4ML, Internal]\n",
    "\n",
    "########################################################################################################\n",
    "### Plots: Feature Importance, ROC, PR, Confusion Matrix, SHAP #########################################\n",
    "\n",
    "# ‚îÄ‚îÄ Create plot output directory ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    plot_dir = os.path.join(\"ML_EXP_plots\", run.info.run_id) ##TODO test this path change\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    # ‚îÄ‚îÄ 1) Feature Importance Bar Chart ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = model.feature_importances_\n",
    "        feature_names = getattr(X_train, \"columns\", [f\"f{i}\" for i in range(X_train.shape[1])])\n",
    "        \n",
    "        fi_path = os.path.join(plot_dir, \"feature_importances.png\")\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.barplot(x=importances, y=feature_names)\n",
    "        plt.title(\"Feature Importances\")\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fi_path)\n",
    "        mlflow.log_artifact(fi_path)  # [MLSEA]\n",
    "        plt.close()\n",
    "    \n",
    "    # ‚îÄ‚îÄ 2) Multi-class ROC Curves ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    classes = np.unique(y_test)\n",
    "    y_test_bin = label_binarize(y_test, classes=classes)\n",
    "    \n",
    "    for idx, cls in enumerate(classes):\n",
    "        disp = RocCurveDisplay.from_predictions(y_test_bin[:, idx], y_proba[:, idx], name=f\"ROC for class {cls}\")\n",
    "        roc_path = os.path.join(plot_dir, f\"roc_curve_cls_{cls}.png\")\n",
    "        disp.figure_.savefig(roc_path)\n",
    "        mlflow.log_artifact(roc_path)  # [MLSEA]\n",
    "        plt.close(disp.figure_)\n",
    "    \n",
    "    # ‚îÄ‚îÄ 3) Multi-class Precision-Recall Curves ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    for idx, cls in enumerate(classes):\n",
    "        disp = PrecisionRecallDisplay.from_predictions(y_test_bin[:, idx], y_proba[:, idx], name=f\"PR curve for class {cls}\")\n",
    "        pr_path = os.path.join(plot_dir, f\"pr_curve_cls_{cls}.png\")\n",
    "        disp.figure_.savefig(pr_path)\n",
    "        mlflow.log_artifact(pr_path)  # [MLSEA]\n",
    "        plt.close(disp.figure_)\n",
    "    \n",
    "    # ‚îÄ‚îÄ 4) Confusion Matrix Plot ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    cm_path = os.path.join(plot_dir, \"confusion_matrix.png\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(cm_path)\n",
    "    mlflow.log_artifact(cm_path)  # [MLSEA]\n",
    "    plt.close()\n",
    "    \n",
    "    # ‚îÄ‚îÄ 5) SHAP Summary Plot ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    shap_path = os.path.join(plot_dir, \"shap_summary.png\")\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    \n",
    "    shap.summary_plot(shap_values, X_test, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(shap_path)\n",
    "    mlflow.log_artifact(shap_path)  # [FAIR4ML, MLSEA]\n",
    "    plt.close()\n",
    "    \n",
    "    ########################################################################################################\n",
    "    ### Final: Metadata Summary Logging ####################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    # log_standard_metadata(\n",
    "    #     model_name=model_name,\n",
    "    #     model=model,\n",
    "    #     hyperparams=ML_EXP_hyperparams,\n",
    "    #     acc=acc,\n",
    "    #     prec=prec,\n",
    "    #     rec=rec,\n",
    "    #     f1=f1,\n",
    "    #     auc=auc,\n",
    "    #     label_map=label_map,\n",
    "    #     run_id=run.info.run_id,\n",
    "    #     test_size=test_size,\n",
    "    #     random_state=random_state,\n",
    "    #     run_data=run_data\n",
    "    # )\n",
    "    log_standard_metadata(\n",
    "    model_name=model_name,\n",
    "    model=model,\n",
    "    hyperparams=ML_EXP_hyperparams,\n",
    "    acc=acc,\n",
    "    prec=prec,\n",
    "    rec=rec,\n",
    "    f1=f1,\n",
    "    auc=auc,\n",
    "    label_map=label_map,\n",
    "    run_id=run.info.run_id,\n",
    "    test_size=test_size,\n",
    "    random_state=random_state,\n",
    "    id_cols=id_cols,         # ‚úÖ list of dropped ID columns\n",
    "    target_col=target_col,   # ‚úÖ your target column, likely defined as df.columns[-1]\n",
    "    X=X,                     # ‚úÖ your features DataFrame\n",
    "    y=y,                     # ‚úÖ your labels array or Series\n",
    "    run_data=run_data        # optional but useful\n",
    "    )\n",
    "\n",
    "########################################################################################################\n",
    "### Export Model (.pkl) and Log as Artifact ############################################################\n",
    "\n",
    "# ‚îÄ‚îÄ Define output path ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    pkl_path = f\"Trained_models/{model_name}.pkl\"\n",
    "    os.makedirs(\"Trained_models\", exist_ok=True)  # Ensure the folder exists\n",
    "    \n",
    "    # ‚îÄ‚îÄ Serialize the trained model to disk ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    with open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    # ‚îÄ‚îÄ Log the serialized model to MLflow as an artifact ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    mlflow.log_artifact(pkl_path, artifact_path=model_name)  # [FAIR4ML, MLSEA]\n",
    "\n",
    "########################################################################################################\n",
    "### COMMIT: Git Integration + Provenance Logging #######################################################\n",
    "\n",
    "    def get_latest_commit_hash(repo_path=\".\"):\n",
    "        res = subprocess.run(\n",
    "            [\"git\", \"-C\", repo_path, \"rev-parse\", \"HEAD\"],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        return res.stdout.strip()\n",
    "    \n",
    "    def get_remote_url(repo_path=\".\", remote=\"origin\"):\n",
    "        res = subprocess.run(\n",
    "            [\"git\", \"-C\", repo_path, \"config\", \"--get\", f\"remote.{remote}.url\"],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        return res.stdout.strip()\n",
    "    \n",
    "    def make_commit_link(remote_url, commit_hash):\n",
    "        base = remote_url.rstrip(\".git\")\n",
    "        if base.startswith(\"git@\"):\n",
    "            base = base.replace(\":\", \"/\").replace(\"git@\", \"https://\")\n",
    "        return f\"{base}/commit/{commit_hash}\"\n",
    "    \n",
    "    def simple_commit_and_push_and_log(repo_path=\".\", message=\"Auto commit\", remote=\"origin\", branch=\"main\"):\n",
    "        status = subprocess.run([\"git\", \"-C\", repo_path, \"status\", \"--porcelain\"], capture_output=True, text=True)\n",
    "        if not status.stdout.strip():\n",
    "            print(\"üü° No changes to commit.\")\n",
    "            return None, None\n",
    "    \n",
    "        subprocess.run([\"git\", \"-C\", repo_path, \"add\", \"--all\"], capture_output=True, text=True)\n",
    "        commit = subprocess.run([\"git\", \"-C\", repo_path, \"commit\", \"-m\", message], capture_output=True, text=True)\n",
    "        if commit.returncode:\n",
    "            print(\"‚ùå git commit failed:\\n\", commit.stderr)\n",
    "            return None, None\n",
    "        print(\"‚úÖ Commit successful.\")\n",
    "    \n",
    "        push = subprocess.run([\"git\", \"-C\", repo_path, \"push\", \"-u\", remote, branch], capture_output=True, text=True)\n",
    "        if push.returncode:\n",
    "            print(\"‚ùå git push failed:\\n\", push.stderr)\n",
    "        else:\n",
    "            print(\"üöÄ Push successful.\")\n",
    "    \n",
    "        sha = get_latest_commit_hash(repo_path)\n",
    "        url = get_remote_url(repo_path, remote)\n",
    "        link = make_commit_link(url, sha)\n",
    "        return sha, link\n",
    "    \n",
    "    # ‚îÄ‚îÄ Perform commit and get commit SHA and link ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    sha, link = simple_commit_and_push_and_log(\n",
    "        repo_path=\".\",\n",
    "        message=\"Auto commit after successful training\"\n",
    "    )\n",
    "    \n",
    "    # ‚îÄ‚îÄ Ask for version tag and log it ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    def get_version_tag_for_commit(commit_hash, known_tags=None):\n",
    "        known_tags = known_tags or {}\n",
    "        version_tag = known_tags.get(commit_hash, \"untagged\")\n",
    "        if version_tag == \"untagged\":\n",
    "            print(f\"‚ö†Ô∏è Commit {commit_hash[:8]} is not tagged with a version.\")\n",
    "            user_input = input(\"üîñ Enter version tag for this commit (or press Enter to skip): \").strip()\n",
    "            version_tag = user_input if user_input else \"untagged\"\n",
    "        return commit_hash, version_tag\n",
    "    \n",
    "    commit, version_tag = get_version_tag_for_commit(sha)\n",
    "    mlflow.set_tag(\"GIT_code_version\", version_tag)  # [PROV]\n",
    "    mlflow.set_tag(\"model_version\", version_tag)  # [PROV]\n",
    "\n",
    "    \n",
    "    \n",
    "    # ‚îÄ‚îÄ Log author info ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    def get_git_author():\n",
    "        name = subprocess.check_output([\"git\", \"config\", \"user.name\"]).decode().strip()\n",
    "        email = subprocess.check_output([\"git\", \"config\", \"user.email\"]).decode().strip()\n",
    "        return name, email\n",
    "    \n",
    "    name, email = get_git_author()\n",
    "    mlflow.set_tag(\"GIT_user\", name)               # [PROV]\n",
    "    mlflow.set_tag(\"GIT_user_email\", email)        # [PROV]\n",
    "    \n",
    "    # ‚îÄ‚îÄ Log Git diff between this and previous commit ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    if sha and link:\n",
    "        previous_commit_hash = db_meta.get(\"code_commit_hash\", \"\")  # Fallback for comparison\n",
    "        if previous_commit_hash:\n",
    "            diff_text = subprocess.check_output(\n",
    "                [\"git\", \"-C\", \".\", \"diff\", previous_commit_hash, sha],\n",
    "                encoding=\"utf-8\", errors=\"ignore\"\n",
    "            )\n",
    "    \n",
    "            remote_url = get_remote_url(\".\")\n",
    "            remote_url = remote_url.rstrip(\".git\")\n",
    "            if remote_url.startswith(\"git@\"):\n",
    "                remote_url = remote_url.replace(\":\", \"/\").replace(\"git@\", \"https://\")\n",
    "    \n",
    "            previous_commit_url = f\"{remote_url}/commit/{previous_commit_hash}\"\n",
    "            current_commit_url  = f\"{remote_url}/commit/{sha}\"\n",
    "    \n",
    "            diff_data = {\n",
    "                \"GIT_previous_commit\":        previous_commit_hash,\n",
    "                \"GIT_previous_commit_url\":    previous_commit_url,\n",
    "                \"GIT_current_commit\":         sha,\n",
    "                \"GIT_current_commit_url\":     current_commit_url,\n",
    "                \"GIT_diff\":                   diff_text\n",
    "            }\n",
    "    \n",
    "            mlflow.log_dict(diff_data, artifact_file=\"GIT_commit_diff.json\")  # [PROV]\n",
    "            mlflow.set_tag(\"GIT_previous_commit_hash\", previous_commit_hash)\n",
    "            mlflow.set_tag(\"GIT_current_commit_hash\", sha)\n",
    "            mlflow.set_tag(\"GIT_current_commit_url\", link)\n",
    "########################################################################################################\n",
    "### Reproducibility Metadata Extraction + Text Log #####################################################\n",
    "\n",
    "# ‚îÄ‚îÄ Log all categorized metadata (FAIR, PROV, DBRepo, etc.) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # log_metadata_dict_to_mlflow(categorized_fields)  # [FAIR4ML, PROV, Internal]\n",
    "\n",
    "    log_metadata_dict_to_mlflow(\n",
    "        metadata=doi_metadata,\n",
    "        prefix=\"DOI_\",\n",
    "        snapshot_name=\"doi_metadata_snapshot.json\"\n",
    "    )\n",
    "    # ‚îÄ‚îÄ Retrieve full run metadata ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    run_id    = run.info.run_id\n",
    "    run_info  = client.get_run(run_id).info\n",
    "    run_data  = client.get_run(run_id).data\n",
    "    \n",
    "    params  = dict(run_data.params)\n",
    "    metrics = dict(run_data.metrics)\n",
    "    tags    = dict(run_data.tags)\n",
    "    \n",
    "    # ‚îÄ‚îÄ List all artifacts in the run ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    artifact_uri  = run_info.artifact_uri\n",
    "    artifact_meta = []\n",
    "    \n",
    "    def _gather(path=\"\"):\n",
    "        for af in client.list_artifacts(run_id, path):\n",
    "            if af.is_dir:\n",
    "                _gather(af.path)\n",
    "            else:\n",
    "                rel_path = af.path.lower()\n",
    "                if rel_path.endswith((\".json\", \".txt\", \".patch\")):\n",
    "                    artifact_meta.append({\"path\": af.path, \"type\": \"text\"})\n",
    "                elif rel_path.endswith((\".png\", \".jpg\", \".jpeg\", \".svg\")):\n",
    "                    artifact_meta.append({\"path\": af.path, \"type\": \"image\"})\n",
    "                else:\n",
    "                    artifact_meta.append({\"path\": af.path, \"type\": \"other\"})\n",
    "    \n",
    "    _gather()\n",
    "    \n",
    "    # ‚îÄ‚îÄ (Optional) Store artifact meta if needed ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    mlflow.log_dict({\"artifacts\": artifact_meta}, artifact_file=\"artifact_summary.json\")  # [Internal]\n",
    "    \n",
    "    # ‚îÄ‚îÄ Notebook directory (for trace/log location reference) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    notebook_dir = os.getcwd()\n",
    "    \n",
    "    ########################################################################################################\n",
    "    ### Generate Reproducibility Instructions ##############################################################\n",
    "    \n",
    "    # ‚îÄ‚îÄ Generate reproducibility .txt log with key details ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    repro_txt_path = generate_reproducibility_txt_log(\n",
    "        model_name=model_name,\n",
    "        dataset_name=db_meta.get(\"dataset_name\", \"unknown\"),\n",
    "        dataset_version=selected_version,\n",
    "        hyperparams=ML_EXP_hyperparams,\n",
    "        metrics={\n",
    "            \"accuracy\": acc,\n",
    "            \"f1_macro\": f1,\n",
    "            \"precision_macro\": prec,\n",
    "            \"recall_macro\": rec,\n",
    "            \"roc_auc\": auc\n",
    "        },\n",
    "        git_commit=sha,\n",
    "        run_id=run_id\n",
    "    )\n",
    "    \n",
    "    # ‚îÄ‚îÄ Log the .txt path to MLflow for traceability ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    mlflow.log_param(\"reproducibility_log_path\", repro_txt_path)  # [Internal, FAIR4ML]\n",
    "########################################################################################################\n",
    "### COMBINE: Export Full Run Summary as JSON ###########################################################\n",
    "\n",
    "# ‚îÄ‚îÄ Create output directory ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    summary_dir = os.path.join(os.getcwd(), \"MODEL_PROVENANCE\", model_name)\n",
    "    os.makedirs(summary_dir, exist_ok=True)\n",
    "    \n",
    "    # ‚îÄ‚îÄ Prepare run summary dict ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    summary = {\n",
    "        \"run_id\":         run_id,\n",
    "        \"run_name\":       run_info.run_name,\n",
    "        \"experiment_id\":  run_info.experiment_id,\n",
    "        \"start_time\":     run_info.start_time,\n",
    "        \"end_time\":       run_info.end_time,\n",
    "        \"params\":         params,\n",
    "        \"metrics\":        metrics,\n",
    "        \"tags\":           tags,\n",
    "        \"artifacts\":      artifact_meta\n",
    "    }\n",
    "    \n",
    "    # ‚îÄ‚îÄ Write summary to JSON file ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    summary_filename    = f\"{model_name}_run_summary.json\"\n",
    "    summary_local_path  = os.path.join(summary_dir, summary_filename)\n",
    "    \n",
    "    with open(summary_local_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    # ‚îÄ‚îÄ Log summary JSON to MLflow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    mlflow.log_artifact(summary_local_path, artifact_path=\"run_summaries\")  # [FAIR4ML, Internal]\n",
    "    print(\"üìÅ Run summary JSON logged at:\", summary_local_path)\n",
    "    \n",
    "    # ‚îÄ‚îÄ End MLflow run with PROV-O end timestamp ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    end_time = datetime.now().isoformat()\n",
    "    mlflow.set_tag(\"endedAtTime\", end_time)  # [PROV]\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e468bd63-075d-4d2b-899f-659468c9e0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\reema\\\\REPO\\\\notebooks\\\\RQ_notebooks\\\\MODEL_PROVENANCE\\\\RandomForest_Iris_v20250518_170352\\\\RandomForest_Iris_v20250518_170352_run_summary.json'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cee2f35d-1cb5-481a-aa26-3212cabca3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'requirements.txt'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "required_keywords = [\n",
    "    \"mlflow\", \"scikit-learn\", \"pandas\", \"numpy\", \"pyyaml\", \"seaborn\",\n",
    "    \"matplotlib\", \"shap\", \"rdflib\", \"requests\", \"python-dotenv\", \"gitpython\", \"psutil\", \"pyld\"\n",
    "]\n",
    "\n",
    "# Run pip freeze\n",
    "result = subprocess.run([\"pip\", \"freeze\"], stdout=subprocess.PIPE, text=True)\n",
    "all_packages = result.stdout.splitlines()\n",
    "\n",
    "# Filter based on matching names\n",
    "filtered = [pkg for pkg in all_packages if any(kw.lower() in pkg.lower() for kw in required_keywords)]\n",
    "\n",
    "# Save filtered requirements to file\n",
    "filtered_requirements_path = \"requirements.txt\"\n",
    "with open(filtered_requirements_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(filtered))\n",
    "\n",
    "filtered_requirements_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca333c-1c4f-4b79-8385-5de253b16511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #WORKS!!\n",
    "# database = client.create_database(\n",
    "#     name=\"Provenance_MetaData\",\n",
    "#     container_id=\"6cfb3b8e-1792-4e46-871a-f3d103527203\",\n",
    "#     is_public=True\n",
    "# )\n",
    "# print(f\"‚úÖ Database created: {database.id}\")\n",
    "# # resp = requests.get(\"http://localhost/api/container\", auth=(\"reema\", \"Toothless!26\"))\n",
    "# # print(resp.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe2dadb-3154-434d-8513-11f02017b936",
   "metadata": {},
   "source": [
    "Table creations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2cec661-0b1d-47ac-84a8-baa5bb2bed15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<ColumnType.CHAR: 'char'>, <ColumnType.VARCHAR: 'varchar'>, <ColumnType.BINARY: 'binary'>, <ColumnType.VARBINARY: 'varbinary'>, <ColumnType.TINYBLOB: 'tinyblob'>, <ColumnType.TINYTEXT: 'tinytext'>, <ColumnType.TEXT: 'text'>, <ColumnType.BLOB: 'blob'>, <ColumnType.MEDIUMTEXT: 'mediumtext'>, <ColumnType.MEDIUMBLOB: 'mediumblob'>, <ColumnType.LONGTEXT: 'longtext'>, <ColumnType.LONGBLOB: 'longblob'>, <ColumnType.ENUM: 'enum'>, <ColumnType.SERIAL: 'serial'>, <ColumnType.SET: 'set'>, <ColumnType.BIT: 'bit'>, <ColumnType.TINYINT: 'tinyint'>, <ColumnType.BOOL: 'bool'>, <ColumnType.SMALLINT: 'smallint'>, <ColumnType.MEDIUMINT: 'mediumint'>, <ColumnType.INT: 'int'>, <ColumnType.BIGINT: 'bigint'>, <ColumnType.FLOAT: 'float'>, <ColumnType.DOUBLE: 'double'>, <ColumnType.DECIMAL: 'decimal'>, <ColumnType.DATE: 'date'>, <ColumnType.DATETIME: 'datetime'>, <ColumnType.TIMESTAMP: 'timestamp'>, <ColumnType.TIME: 'time'>, <ColumnType.YEAR: 'year'>]\n"
     ]
    }
   ],
   "source": [
    "# from dbrepo.api.dto import ColumnType\n",
    "\n",
    "# print(list(ColumnType))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6823feb-5a8a-4d0f-9959-db4becf9c13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf769532-4ab7-4202-bf34-93b31557bb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Failed\n",
      "400\n",
      "{\"type\":\"about:blank\",\"title\":\"Bad Request\",\"status\":400,\"detail\":\"Failed to read request\",\"instance\":\"/api/database/ce4550bd-0fad-4a6b-894b-455a1decae5d/table/fbd07137-cc8f-42dc-b587-6be1ecad1001/data\",\"properties\":null}\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# import json\n",
    "\n",
    "# url = \"http://localhost/api/database/ce4550bd-0fad-4a6b-894b-455a1decae5d/table/fbd07137-cc8f-42dc-b587-6be1ecad1001/data\"\n",
    "# auth = (\"reema\", \"Toothless!26\")\n",
    "# headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# rows = [\n",
    "#     {\n",
    "#         \"runID\": \"run006\",\n",
    "#         \"sessionID\": \"sess006\",\n",
    "#         \"modelId\": \"model006\",\n",
    "#         \"datasetID\": \"data006\",\n",
    "#         \"git_commit\": \"abc123\",\n",
    "#         \"invenioID\": \"inv006\",\n",
    "#         \"timestamp\": \"2025-05-18T17:00:00Z\"\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# response = requests.post(url, auth=auth, headers=headers, json=rows)\n",
    "\n",
    "# if response.status_code == 200:\n",
    "#     print(\"‚úÖ Insert successful\")\n",
    "# else:\n",
    "#     print(\"‚ùå Failed\")\n",
    "#     print(response.status_code)\n",
    "#     print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5469841d-d44c-45e4-a53c-e32554595e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'runid': 'run001', 'sessionid': 'sess001', 'modelid': 'model001', 'datasetid': 'data001', 'git_commit': 'abc1234', 'invenioid': 'inv001', 'timestamp': '2025-05-18 14:30:00.0'}, {'runid': 'run002', 'sessionid': 'sess002', 'modelid': 'model002', 'datasetid': 'data002', 'git_commit': 'def5678', 'invenioid': 'inv002', 'timestamp': '2025-05-18 15:00:00.0'}, {'runid': 'run003', 'sessionid': 'sess003', 'modelid': 'model003', 'datasetid': 'data003', 'git_commit': 'ghi9012', 'invenioid': 'inv003', 'timestamp': '2025-05-18 15:30:00.0'}, {'runid': 'run004', 'sessionid': 'sess004', 'modelid': 'model004', 'datasetid': 'data004', 'git_commit': 'jkl3456', 'invenioid': 'inv004', 'timestamp': '2025-05-18 16:00:00.0'}, {'runid': 'run005', 'sessionid': 'sess005', 'modelid': 'model005', 'datasetid': 'data005', 'git_commit': 'mno7890', 'invenioid': 'inv005', 'timestamp': '2025-05-18 16:30:00.0'}]\n"
     ]
    }
   ],
   "source": [
    "# # API endpoint URL\n",
    "# db_id=\"ce4550bd-0fad-4a6b-894b-455a1decae5d\"\n",
    "# selected_table_id=\"fbd07137-cc8f-42dc-b587-6be1ecad1001\"\n",
    "# API_URL = f\"http://localhost/api/database/{db_id}/table/{selected_table_id}/data?size=100000&page=0\"\n",
    "\n",
    "# # Define the headers\n",
    "# headers = {\n",
    "#     \"Accept\": \"application/json\"  # Specify the expected response format\n",
    "# }\n",
    "\n",
    "# try:\n",
    "#     # Send a GET request to the API with the Accept header\n",
    "#     response = requests.get(API_URL, headers=headers)\n",
    "\n",
    "#     # Check if the request was successful\n",
    "#     if response.status_code == 200:\n",
    "#         # Parse the JSON response\n",
    "#         dataset = response.json()\n",
    "        \n",
    "        \n",
    "#         print( dataset)\n",
    "#     else:\n",
    "#         print(f\"Error: Received status code {response.status_code}\")\n",
    "#         print(\"Response content:\", response.text)\n",
    "       \n",
    "\n",
    "# except requests.exceptions.RequestException as e:\n",
    "#     print(f\"Request failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2231f52-11d9-4a6f-bbc0-25efc6ab5f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Row inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# import json\n",
    "\n",
    "# url = \"http://localhost/api/database/ce4550bd-0fad-4a6b-894b-455a1decae5d/table/53ffac33-9391-4cba-bd7b-5759a1c98201/data\"\n",
    "\n",
    "# headers = {\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# auth = (\"reema\", \"Toothless!26\")  # Replace with your actual credentials\n",
    "\n",
    "# # Payload matching the JSON format that worked in Postman\n",
    "# payload = {\n",
    "#     \"data\": {\n",
    "#         \"runid\": \"run0010000\",\n",
    "#         \"sessionid\": \"sess009\",\n",
    "#         \"modelid\": \"model009\",\n",
    "#         \"datasetid\": \"data009\",\n",
    "#         \"git_commit\": \"xyz999\",\n",
    "#         \"invenioid\": \"inv009\",\n",
    "#         \"timestamp\": \"2025-05-18T11:31:31.914+00:00\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# response = requests.post(url, headers=headers, auth=auth, json=payload)\n",
    "\n",
    "# if response.status_code == 201:\n",
    "#     print(\"‚úÖ Row inserted successfully!\")\n",
    "# else:\n",
    "#     print(\"‚ùå Failed to insert\")\n",
    "#     print(\"Status:\", response.status_code)\n",
    "#     print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f46eb8f-4ab6-471b-99fe-c6678607b5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-05-18T11:32:27.292+00:00'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2987b94d-deec-4a0b-b71c-62c4da98061c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [201]>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import requests\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Auth & headers\n",
    "# headers = {\"Content-Type\": \"application/json\"}\n",
    "# auth = (\"reema\", \"Toothless!26\")  # ‚ö†Ô∏è Never hardcode this in prod\n",
    "\n",
    "# # Timestamp conversion\n",
    "# def to_mysql_datetime(ts):\n",
    "#     return datetime.strptime(ts, \"%Y-%m-%d %H:%M:%S\").isoformat() + \"+00:00\"\n",
    "\n",
    "# # Define base URL (adjust table IDs per table)\n",
    "# BASE = \"http://localhost/api/database/ce4550bd-0fad-4a6b-894b-455a1decae5d/table\"\n",
    "\n",
    "# # Define table-specific endpoints\n",
    "# TABLES = {\n",
    "#     \"session_metadata\": \"3af934ed-a467-46e5-bb0a-495b2ff0efbf\",\n",
    "#     \"experiment_metadata\": \"53ffac33-9391-4cba-bd7b-5759a1c98201\",\n",
    "#     \"git_metadata\": \"ad546581-467e-4570-94eb-45eeb4f3019b\",\n",
    "#     \"dataset_metadata\": \"9119bded-19af-42b6-b27e-c9d229a9a7c2\",\n",
    "#     \"model_metadata\": \"26294b1f-4f4d-4a7a-917e-141fa048fb39\",\n",
    "# }\n",
    "\n",
    "# # 1. Session Metadata\n",
    "# requests.post(\n",
    "#     f\"{BASE}/{TABLES['session_metadata']}/data\",\n",
    "#     headers=headers,\n",
    "#     auth=auth,\n",
    "#     json={\n",
    "#         \"data\": {\n",
    "#             \"session_id\": \"sess_001pppp\",\n",
    "#             \"username\": \"user123\",\n",
    "#             \"timestamp\": to_mysql_datetime(\"2025-05-18 09:30:00\"),\n",
    "#             \"hostname\": \"host001\",\n",
    "#             \"platform\": \"Linux\"\n",
    "#         }\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # 2. Experiment Metadata\n",
    "# requests.post(\n",
    "#     f\"{BASE}/{TABLES['experiment_metadata']}/data\",\n",
    "#     headers=headers,\n",
    "#     auth=auth,\n",
    "#     json={\n",
    "#         \"data\": {\n",
    "#             \"runid\": \"run_001xppppp\",\n",
    "#             \"sessionid\": \"sess_001\",\n",
    "#             \"modelid\": \"model_001\",\n",
    "#             \"datasetid\": \"ds_001\",\n",
    "#             \"git_commit\": \"abc1234\",\n",
    "#             \"invenioid\": \"inv_001\",\n",
    "#             \"timestamp\": to_mysql_datetime(\"2025-05-18 09:45:00\")\n",
    "#         }\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # 3. Git Metadata\n",
    "# requests.post(\n",
    "#     f\"{BASE}/{TABLES['git_metadata']}/data\",\n",
    "#     headers=headers,\n",
    "#     auth=auth,\n",
    "#     json={\n",
    "#         \"data\": {\n",
    "#             \"commit_hash\": \"abc1234xppppp\",\n",
    "#             \"repo_url\": \"https://github.com/example/repo\",\n",
    "#             \"branch\": \"main\",\n",
    "#             \"author\": \"dev_user\",\n",
    "#             \"timestamp\": to_mysql_datetime(\"2025-05-18 09:20:00\"),\n",
    "#             \"version\": \"v2.3.1\"\n",
    "#         }\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # 4. Dataset Metadata\n",
    "# requests.post(\n",
    "#     f\"{BASE}/{TABLES['dataset_metadata']}/data\",\n",
    "#     headers=headers,\n",
    "#     auth=auth,\n",
    "#     json={\n",
    "#         \"data\": {\n",
    "#             \"dataset_id\": \"ds_001xpppppp\",\n",
    "#             \"table_name\": \"table_xyz\",\n",
    "#             \"detailed_type\": \"CSV\",\n",
    "#             \"classes\": 3,\n",
    "#             \"features\": 5,\n",
    "#             \"output_type\": \"categorical\",\n",
    "#             \"version\": \"v1.0\"\n",
    "#         }\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # 5. Model Metadata\n",
    "# requests.post(\n",
    "#     f\"{BASE}/{TABLES['model_metadata']}/data\",\n",
    "#     headers=headers,\n",
    "#     auth=auth,\n",
    "#     json={\n",
    "#         \"data\": {\n",
    "#             \"model_id\": \"model_001xppppppppp\",\n",
    "#             \"name\": \"RandomForest\",\n",
    "#             \"algo\": \"RandomForestClassifier\",\n",
    "#             \"features\": '[\"age\", \"income\", \"education\", \"gender\", \"marital_status\"]',\n",
    "#             \"label_snap\": \"income_group\",\n",
    "#             \"train_split\": 0.8,\n",
    "#             \"test_split\": 0.2,\n",
    "#             \"target_var\": \"income_group\",\n",
    "#             \"label_map\": '{\"0\":\"Low\",\"1\":\"Medium\",\"2\":\"High\"}',\n",
    "#             \"feature_select\": '[\"age\", \"income\", \"education\"]',\n",
    "#             \"imbalance_ratio\": 1.5\n",
    "#         }\n",
    "#     }\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f587f002-e685-46f4-8626-cee0d485a2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "378d771a-aa32-458a-b5bf-3bf75363010f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_upload', '_wrapper', 'analyse_datatypes', 'analyse_keys', 'analyse_table_statistics', 'create_container', 'create_database', 'create_database_access', 'create_identifier', 'create_subset', 'create_table', 'create_table_data', 'create_view', 'delete_container', 'delete_database_access', 'delete_table', 'delete_table_data', 'delete_view', 'endpoint', 'get_concepts', 'get_container', 'get_containers', 'get_database', 'get_database_access', 'get_databases', 'get_databases_count', 'get_identifier', 'get_identifier_data', 'get_identifiers', 'get_image', 'get_images', 'get_licenses', 'get_messages', 'get_ontologies', 'get_queries', 'get_subset', 'get_subset_data', 'get_subset_data_count', 'get_table', 'get_table_data', 'get_table_data_count', 'get_table_history', 'get_tables', 'get_units', 'get_user', 'get_users', 'get_view', 'get_view_data', 'get_view_data_count', 'get_views', 'import_table_data', 'password', 'publish_identifier', 'secure', 'update_database_access', 'update_database_owner', 'update_database_schema', 'update_database_visibility', 'update_identifier', 'update_subset', 'update_table_column', 'update_table_data', 'update_user', 'update_view', 'username', 'whoami']\n"
     ]
    }
   ],
   "source": [
    "from dbrepo.RestClient import RestClient\n",
    "client = RestClient(endpoint=\"http://localhost\", username=\"reema\", password=\"Toothless!26\")\n",
    "print(dir(client))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ff6e9-af91-490b-a280-24450a3b4082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d13600-db42-4010-93cd-ef77787a1565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82fe4cc5-396c-48ae-8561-df4e0a44fbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Git Metadata POST\n",
      "‚û°Ô∏è URL: http://localhost/api/database/ce4550bd-0fad-4a6b-894b-455a1decae5d/table/ad546581-467e-4570-94eb-45eeb4f3019b/data\n",
      "üì¶ Payload:\n",
      "{\n",
      "  \"commit_hash\": \"edb72cb2613a3001b07b1ae6e0d18d4a1023fd1e\",\n",
      "  \"repo_url\": \"https://archive.ics.uci.edu/dataset/53\",\n",
      "  \"branch\": \"main\",\n",
      "  \"author\": \"Reema George\",\n",
      "  \"timestamp\": \"2025-05-18T15:03:47+00:00\",\n",
      "  \"version\": \"v99\"\n",
      "}\n",
      "üì§ Status Code: 201\n",
      "üìù Response Text: \n",
      "\n",
      "üîç Dataset Metadata POST\n",
      "‚û°Ô∏è URL: http://localhost/api/database/ce4550bd-0fad-4a6b-894b-455a1decae5d/table/9119bded-19af-42b6-b27e-c9d229a9a7c2/data\n",
      "üì¶ Payload:\n",
      "{\n",
      "  \"dataset_id\": \"0c672781-25c3-438e-8fba-18c2c7f16886\",\n",
      "  \"table_name\": \"iris_data_v3\",\n",
      "  \"detailed_type\": \"CSV\",\n",
      "  \"classes\": 3,\n",
      "  \"features\": 4,\n",
      "  \"output_type\": \"categorical\",\n",
      "  \"version\": \"v3\"\n",
      "}\n",
      "üì§ Status Code: 201\n",
      "üìù Response Text: \n",
      "\n",
      "üîç MODEL METADATA POST\n",
      "‚û°Ô∏è URL: http://localhost/api/database/ce4550bd-0fad-4a6b-894b-455a1decae5d/table/26294b1f-4f4d-4a7a-917e-141fa048fb39/data\n",
      "üì¶ Payload:\n",
      "{\n",
      "  \"model_id\": \"model_iris\",\n",
      "  \"name\": \"RandomForest_Iris_v20250518_170352\",\n",
      "  \"algo\": \"RandomForestClassifier\",\n",
      "  \"features\": \"['sepallengthcm', 'sepalwidthcm', 'petallengthcm', 'petalwidthcm']\",\n",
      "  \"label_snap\": \"species\",\n",
      "  \"train_split\": 0.8,\n",
      "  \"test_split\": 0.2,\n",
      "  \"target_var\": \"species\",\n",
      "  \"label_map\": \"{\\\"0\\\": \\\"Iris-setosa\\\", \\\"1\\\": \\\"Iris-versicolor\\\", \\\"2\\\": \\\"Iris-virginica\\\"}\",\n",
      "  \"feature_select\": \"id,sepallengthcm,sepalwidthcm,petallengthcm,petalwidthcm\",\n",
      "  \"imbalance_ratio\": 1.0\n",
      "}\n",
      "üì§ Status Code: 201\n",
      "üìù Response Text: \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "auth = (\"reema\", \"Toothless!26\")\n",
    "BASE = \"http://localhost/api/database/633f5987-d116-42e8-97fc-36b9c25ade24/table\"\n",
    "TABLES = {\n",
    "    \"session_metadata\": \"a4b15637-b98c-41f1-910d-cedfe80ca53b\",\n",
    "    \"experiment_metadata\": \"0bf9cd0c-f0c0-4236-a412-30265f5ee1a6\",\n",
    "    \"git_metadata\": \"e3bb40a4-484c-4148-99a9-8189d02afacd\",\n",
    "    \"dataset_metadata\": \"78bce105-fe1b-4d02-a7af-68885030a15d\",\n",
    "    \"model_metadata\": \"99391154-5d55-4d28-9843-194508cd0c7e\"\n",
    "}\n",
    "\n",
    "# --- Load metadata file ---\n",
    "# with open(\"MODEL_PROVENANCE/b788db5d12174c28bc175589898f7f95/RandomForest_Iris_v20250516_193049_run_summary.json\", \"r\") as f:\n",
    "with open(summary_local_path, \"r\") as f:\n",
    "\n",
    "    meta = json.load(f)\n",
    "\n",
    "# --- Helper ---\n",
    "def to_mysql_datetime(ts):\n",
    "    return datetime.strptime(ts.split(\".\")[0], \"%Y-%m-%dT%H:%M:%S\").isoformat() + \"+00:00\"\n",
    "\n",
    "# --- Extract shared values ---\n",
    "run_id = meta[\"run_id\"]\n",
    "session_id = meta[\"params\"][\"session_id\"]\n",
    "dataset_id = meta[\"tags\"][\"dataset_id\"]\n",
    "model_id = \"model_\" + meta[\"tags\"][\"model_name\"].split(\"_\")[1].lower()\n",
    "git_commit = meta[\"tags\"][\"git_commit\"]\n",
    "git_version = meta[\"tags\"][\"GIT_code_version\"]\n",
    "timestamp_utc = meta[\"params\"][\"timestamp_utc\"]\n",
    "username = meta[\"params\"][\"username\"]\n",
    "platform = meta[\"params\"][\"platform\"]\n",
    "hostname = meta[\"params\"][\"hostname\"]\n",
    "target_var = meta[\"tags\"][\"target_variable\"]\n",
    "label_map = meta[\"tags\"][\"label_map\"]\n",
    "feature_list = meta[\"params\"][\"final_feature_names\"]\n",
    "dataset_name = meta[\"tags\"][\"dataset_name\"]\n",
    "dataset_version = meta[\"tags\"][\"dataset_version\"]\n",
    "estimator = meta[\"tags\"][\"estimator_name\"]\n",
    "feature_select = meta[\"tags\"][\"feature_columns\"]\n",
    "label_snap = meta[\"tags\"][\"target_variable_encoded\"]\n",
    "model_name = meta[\"tags\"][\"model_name\"]\n",
    "imbalance_ratio = 1.0 if \"imbalance_ratio\" not in meta[\"tags\"] else meta[\"tags\"][\"imbalance_ratio\"]\n",
    "\n",
    "# # --- 1. Session Metadata ---\n",
    "# session_payload = {\n",
    "#     \"session_id\": session_id,\n",
    "#     \"username\": username,\n",
    "#     \"timestamp\": to_mysql_datetime(timestamp_utc),\n",
    "#     \"hostname\": hostname,\n",
    "#     \"platform\": platform\n",
    "# }\n",
    "\n",
    "# session_url = f\"{BASE}/{TABLES['session_metadata']}/data\"\n",
    "\n",
    "# response = requests.post(\n",
    "#     session_url,\n",
    "#     headers=headers,\n",
    "#     auth=auth,\n",
    "#     json={\"data\": session_payload}\n",
    "# )\n",
    "\n",
    "# # --- Logging ---\n",
    "# print(\"\\nüîç Session Metadata POST\")\n",
    "# print(\"‚û°Ô∏è URL:\", session_url)\n",
    "# print(\"üì¶ Payload:\")\n",
    "# print(json.dumps(session_payload, indent=2))\n",
    "# print(\"üì§ Status Code:\", response.status_code)\n",
    "# print(\"üìù Response Text:\", response.text)\n",
    "\n",
    "# # # --- 2. Experiment Metadata ---\n",
    "# exp_payload = {\n",
    "#     \"runid\": run_id,\n",
    "#     \"sessionid\": session_id,\n",
    "#     \"modelid\": model_id,\n",
    "#     \"datasetid\": dataset_id,\n",
    "#     \"git_commit\": git_commit,\n",
    "#     \"invenioid\": meta[\"tags\"].get(\"DOI_dataset_id\"),\n",
    "#     \"timestamp\": to_mysql_datetime(timestamp_utc)\n",
    "# }\n",
    "\n",
    "# exp_url = f\"{BASE}/{TABLES['experiment_metadata']}/data\"\n",
    "\n",
    "# response = requests.post(\n",
    "#     exp_url,\n",
    "#     headers=headers,\n",
    "#     auth=auth,\n",
    "#     json={\"data\": exp_payload}\n",
    "# )\n",
    "\n",
    "# # --- Logging ---\n",
    "# print(\"\\nüîç Experiment Metadata POST\")\n",
    "# print(\"‚û°Ô∏è URL:\", exp_url)\n",
    "# print(\"üì¶ Payload:\")\n",
    "# print(json.dumps(exp_payload, indent=2))\n",
    "# print(\"üì§ Status Code:\", response.status_code)\n",
    "# print(\"üìù Response Text:\", response.text)\n",
    "\n",
    "\n",
    "# --- 3. Git Metadata ---\n",
    "git_payload = {\n",
    "    \"commit_hash\": git_commit,\n",
    "    \"repo_url\": meta[\"tags\"][\"DOI_prov_used\"],\n",
    "    \"branch\": \"main\",  # assumed\n",
    "    \"author\": meta[\"tags\"][\"GIT_user\"],\n",
    "    \"timestamp\": to_mysql_datetime(timestamp_utc),\n",
    "    \"version\": git_version\n",
    "}\n",
    "\n",
    "git_url = f\"{BASE}/{TABLES['git_metadata']}/data\"\n",
    "\n",
    "response = requests.post(\n",
    "    git_url,\n",
    "    headers=headers,\n",
    "    auth=auth,\n",
    "    json={\"data\": git_payload}\n",
    ")\n",
    "\n",
    "# --- Logging ---\n",
    "print(\"\\nüîç Git Metadata POST\")\n",
    "print(\"‚û°Ô∏è URL:\", git_url)\n",
    "print(\"üì¶ Payload:\")\n",
    "print(json.dumps(git_payload, indent=2))\n",
    "print(\"üì§ Status Code:\", response.status_code)\n",
    "print(\"üìù Response Text:\", response.text)\n",
    "\n",
    "# --- Build payload dynamically ---\n",
    "dataset_payload = {\n",
    "    \"dataset_id\": dataset_id,\n",
    "    \"table_name\": dataset_name,\n",
    "    \"detailed_type\": \"CSV\",\n",
    "    \"classes\": 3,\n",
    "    \"features\": int(meta[\"params\"][\"final_num_features\"]),\n",
    "    \"output_type\": \"categorical\",\n",
    "    \"version\": dataset_version\n",
    "}\n",
    "\n",
    "# --- Build URL ---\n",
    "dataset_url = f\"{BASE}/{TABLES['dataset_metadata']}/data\"\n",
    "\n",
    "# --- Send POST request ---\n",
    "response = requests.post(\n",
    "    dataset_url,\n",
    "    headers=headers,\n",
    "    auth=auth,\n",
    "    json={\"data\": dataset_payload}\n",
    ")\n",
    "\n",
    "# --- Log request/response ---\n",
    "print(\"\\nüîç Dataset Metadata POST\")\n",
    "print(\"‚û°Ô∏è URL:\", dataset_url)\n",
    "print(\"üì¶ Payload:\")\n",
    "print(json.dumps(dataset_payload, indent=2))\n",
    "print(\"üì§ Status Code:\", response.status_code)\n",
    "print(\"üìù Response Text:\", response.text)\n",
    "\n",
    "\n",
    "# -- Log POST request for model metadata --\n",
    "model_payload = {\n",
    "    \"model_id\": model_id,\n",
    "    \"name\": model_name,\n",
    "    \"algo\": estimator,\n",
    "    \"features\": feature_list,\n",
    "    \"label_snap\": label_snap,\n",
    "    \"train_split\": float(meta[\"params\"][\"n_train_samples\"]) / int(meta[\"params\"][\"input_row_count\"]),\n",
    "    \"test_split\": float(meta[\"params\"][\"n_test_samples\"]) / int(meta[\"params\"][\"input_row_count\"]),\n",
    "    \"target_var\": target_var,\n",
    "    \"label_map\": label_map,\n",
    "    \"feature_select\": feature_select,\n",
    "    \"imbalance_ratio\": imbalance_ratio\n",
    "}\n",
    "\n",
    "model_url = f\"{BASE}/{TABLES['model_metadata']}/data\"\n",
    "response = requests.post(\n",
    "    model_url,\n",
    "    headers=headers,\n",
    "    auth=auth,\n",
    "    json={\"data\": model_payload}\n",
    ")\n",
    "\n",
    "# --- Logging ---\n",
    "print(\"\\nüîç MODEL METADATA POST\")\n",
    "print(\"‚û°Ô∏è URL:\", model_url)\n",
    "print(\"üì¶ Payload:\")\n",
    "print(json.dumps(model_payload, indent=2))\n",
    "print(\"üì§ Status Code:\", response.status_code)\n",
    "print(\"üìù Response Text:\", response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f3d37-d830-40b8-a147-dcdc1c491fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a8bcc-5a5c-47dd-92c8-5e4ddd1fbcc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af02016-3e12-4745-a612-de0ac1bc4411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d0218-dac9-4fac-a473-46b15cac3cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33849487-1ea4-4f68-8760-7c9c57842243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67cdcc-6b6a-404c-8253-2598bc885cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8056340b-3c7f-4625-bd09-8bcba40e1bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54718d49-b55a-407d-a158-a9c60768453f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd04063-9e56-43e2-9bdd-2a5e4ed09ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2041c098-a708-4bd3-a69a-c800e1de10f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ff418-8279-4992-9b32-56b713bb182e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a080f-9910-4c67-a285-544c5bee3604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9fc11-4c3f-4db0-9ea4-17dc81b33493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d96de-3d46-43f9-bb67-8a3e3851237c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a5e3bbb-0288-47d0-9dc4-2855d7e4801a",
   "metadata": {},
   "source": [
    "1. Standards-compliant export (JSON-LD + Turtle)\n",
    "I already have your plain run_summary.json , wrap it in a JSON-LD context that maps your fields into PROV-O terms, then use rdflib to emit Turtle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ed1cfb-930a-4f17-a48f-30e4cffb7f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON file\n",
    "json_path = \"/mnt/data/REPO/notebooks/RQ_notebooks/MODEL_PROVENANCE/RandomForest_Iris_v20250425_125653/RandomForest_Iris_v20250425_125653_run_summary.json\"\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract justification tags\n",
    "justifications = {\n",
    "    k: v for k, v in data.get(\"tags\", {}).items()\n",
    "    if k.startswith(\"justification_\")\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "justification_df = pd.DataFrame([\n",
    "    {\"Decision\": k.replace(\"justification_\", \"\"), \"Justification\": v}\n",
    "    for k, v in justifications.items()\n",
    "])\n",
    "\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Researcher Justifications\", dataframe=justification_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf88da4-69f8-4982-a594-28cf25e4f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iso8601(ms):\n",
    "    \"\"\"Convert milliseconds since epoch to ISO8601 UTC.\"\"\"\n",
    "    return datetime.fromtimestamp(ms / 1000, tz=timezone.utc).isoformat()\n",
    "\n",
    "for json_path in glob.glob(\"MODEL_PROVENANCE/*/*_run_summary.json\"):\n",
    "    basename   = os.path.basename(json_path)\n",
    "    model_name = basename.rsplit(\"_run_summary.json\", 1)[0]\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        summary = json.load(f)\n",
    "\n",
    "    #‚Äì‚Äì Minimal override context: keep all your flat fields as-is,\n",
    "    #‚Äì‚Äì and only map the actual PROV terms to their IRIs.\n",
    "    ctx = {\n",
    "        # keep these flat\n",
    "        \"run_id\":       { \"@id\": \"run_id\" },\n",
    "        \"run_name\":     { \"@id\": \"run_name\" },\n",
    "        \"experiment_id\":{ \"@id\": \"experiment_id\" },\n",
    "        \"params\":       { \"@id\": \"params\" },\n",
    "        \"metrics\":      { \"@id\": \"metrics\" },\n",
    "        \"artifacts\":    { \"@id\": \"artifacts\" },\n",
    "        \"tags\":         { \"@id\": \"tags\" },\n",
    "\n",
    "        # provenance namespace\n",
    "        \"prov\": \"http://www.w3.org/ns/prov#\",\n",
    "        \"xsd\":  \"http://www.w3.org/2001/XMLSchema#\",\n",
    "\n",
    "        # map your timestamp fields into PROV\n",
    "        \"start_time\": { \"@id\": \"prov:startedAtTime\", \"@type\": \"xsd:dateTime\" },\n",
    "        \"end_time\":   { \"@id\": \"prov:endedAtTime\",   \"@type\": \"xsd:dateTime\" },\n",
    "\n",
    "        # PROV-used/generated\n",
    "        \"used\":      { \"@id\": \"prov:used\",      \"@type\": \"@id\" },\n",
    "        \"generated\": { \"@id\": \"prov:generated\", \"@type\": \"@id\" },\n",
    "\n",
    "        # JSON-LD boilerplate\n",
    "        \"@id\":   \"@id\",\n",
    "        \"@type\": \"@type\"\n",
    "    }\n",
    "\n",
    "    #‚Äì‚Äì Build JSON-LD document, re-using your original keys verbatim\n",
    "    doc = {\n",
    "        \"@context\":      ctx,\n",
    "        \"run_id\":        summary[\"run_id\"],\n",
    "        \"run_name\":      summary.get(\"run_name\"),\n",
    "        \"experiment_id\": summary.get(\"experiment_id\"),\n",
    "        \"params\":        summary.get(\"params\", {}),\n",
    "        \"metrics\":       summary.get(\"metrics\", {}),\n",
    "        \"artifacts\":     summary.get(\"artifacts\", []),\n",
    "        \"tags\":          summary.get(\"tags\", {}),\n",
    "\n",
    "        # PROV fields:\n",
    "        \"start_time\": iso8601(summary[\"start_time\"])\n",
    "    }\n",
    "\n",
    "    if summary.get(\"end_time\") is not None:\n",
    "        doc[\"end_time\"] = iso8601(summary[\"end_time\"])\n",
    "\n",
    "    # for used/generated, just point at your dataset/model URIs\n",
    "    # (or blank-node them if you prefer richer structure)\n",
    "    doc[\"used\"] = summary.get(\"tags\", {}).get(\"dataset_uri\") or []\n",
    "    doc[\"generated\"] = [\n",
    "        art.get(\"uri\") or art.get(\"path\")\n",
    "        for art in summary.get(\"artifacts\", [])\n",
    "    ]\n",
    "\n",
    "    #‚Äì‚Äì write JSON-LD\n",
    "    out_jsonld = os.path.join(\"MODEL_PROVENANCE\", model_name, f\"{model_name}.jsonld\")\n",
    "    with open(out_jsonld, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(doc, f, indent=2)\n",
    "\n",
    "    #‚Äì‚Äì parse & serialize to Turtle\n",
    "    g = Graph().parse(data=json.dumps(doc), format=\"json-ld\")\n",
    "    out_ttl = os.path.join(\"MODEL_PROVENANCE\", model_name, f\"{model_name}.ttl\")\n",
    "    g.serialize(destination=out_ttl, format=\"turtle\")\n",
    "\n",
    "    print(f\"Converted {basename} ‚Üí {os.path.basename(out_jsonld)}, {os.path.basename(out_ttl)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d6d524-01da-4f20-8131-0d4a3ac005e2",
   "metadata": {},
   "source": [
    "This code programatically, finds diff between generated Json file and created JsonLD and .TTL file to make it easier to understand if there is any discrepency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a420c0-230d-41c0-9b63-f3dbbca1e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_as_dict(path):\n",
    "    if path.endswith((\".ttl\", \".turtle\")):\n",
    "        g = Graph()\n",
    "        g.parse(path, format=\"turtle\")\n",
    "        # normalize to JSON-LD dict\n",
    "        return json.loads(g.serialize(format=\"json-ld\", indent=2))\n",
    "    else:\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "def compare_json(a, b, path=\"\"):\n",
    "    diffs = []\n",
    "    if isinstance(a, dict) and isinstance(b, dict):\n",
    "        all_keys = set(a) | set(b)\n",
    "        for k in all_keys:\n",
    "            new_path = f\"{path}/{k}\" if path else k\n",
    "            if k not in a:\n",
    "                diffs.append({\"path\": new_path, \"type\": \"added\",   \"a\": None,    \"b\": b[k]})\n",
    "            elif k not in b:\n",
    "                diffs.append({\"path\": new_path, \"type\": \"removed\", \"a\": a[k],   \"b\": None})\n",
    "            else:\n",
    "                diffs.extend(compare_json(a[k], b[k], new_path))\n",
    "    elif isinstance(a, list) and isinstance(b, list):\n",
    "        for i, (ia, ib) in enumerate(zip(a, b)):\n",
    "            diffs.extend(compare_json(ia, ib, f\"{path}[{i}]\"))\n",
    "        # handle length mismatches\n",
    "        if len(a) < len(b):\n",
    "            for i in range(len(a), len(b)):\n",
    "                diffs.append({\"path\": f\"{path}[{i}]\", \"type\": \"added\",   \"a\": None,  \"b\": b[i]})\n",
    "        elif len(a) > len(b):\n",
    "            for i in range(len(b), len(a)):\n",
    "                diffs.append({\"path\": f\"{path}[{i}]\", \"type\": \"removed\", \"a\": a[i],  \"b\": None})\n",
    "    else:\n",
    "        if a != b:\n",
    "            diffs.append({\"path\": path, \"type\": \"changed\", \"a\": a, \"b\": b})\n",
    "    return diffs\n",
    "\n",
    "\n",
    "# Define base directory\n",
    "base_dir = os.path.join(\"MODEL_PROVENANCE\", model_name)\n",
    "\n",
    "# Build full paths for the files to compare\n",
    "summary_json    = os.path.join(base_dir, f\"{model_name}_run_summary.json\")\n",
    "turtle_file     = os.path.join(base_dir, f\"{model_name}.ttl\")\n",
    "jsonld_file     = os.path.join(base_dir, f\"{model_name}.jsonld\")\n",
    "\n",
    "# Load files\n",
    "a = load_as_dict(summary_json)\n",
    "b = load_as_dict(turtle_file)\n",
    "c = load_as_dict(summary_json)\n",
    "d = load_as_dict(jsonld_file)\n",
    "\n",
    "# Perform comparisons\n",
    "diffs_jsonld_vs_ttl = compare_json(a, b)\n",
    "diffs_json_vs_jsonld = compare_json(c, d)\n",
    "\n",
    "# Build DataFrames for interactive inspection\n",
    "df1 = pd.DataFrame(diffs_jsonld_vs_ttl)\n",
    "df2 = pd.DataFrame(diffs_json_vs_jsonld)\n",
    "\n",
    "# --- Summaries & Filtering ---------------------------------------\n",
    "\n",
    "def summarize_and_preview(df, preview_n=10):\n",
    "    print(\"Change summary:\")\n",
    "    print(df['type'].value_counts().to_string(), \"\\n\")\n",
    "    \n",
    "    print(f\"First {preview_n} ‚Äòchanged‚Äô entries:\")\n",
    "    # print(df[df['type']==\"changed\"].head(preview_n).to_string(index=False), \"\\n\")\n",
    "    \n",
    "    # Top‚Äêlevel (one slash) adds/removes\n",
    "    top = df[df['path'].str.count(\"/\") == 1]\n",
    "    print(\"Top-level adds/removes:\")\n",
    "    print(top[top['type'].isin(['added','removed'])].to_string(index=False))\n",
    "\n",
    "print(\"== JSON-LD vs TTL ==\")\n",
    "summarize_and_preview(df1)\n",
    "\n",
    "print(\"\\n== JSON vs JSON-LD ==\")\n",
    "summarize_and_preview(df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af9d6e-c683-45f9-bac1-296611b4d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all the removed paths (in JSON but not in JSON-LD)\n",
    "print(\"Removed in JSON-LD comparison:\")\n",
    "print(df2[df2['type']==\"removed\"][['path']].to_string(index=False))\n",
    "\n",
    "# show all the added paths (in JSON-LD but not in JSON)\n",
    "print(\"\\nAdded in JSON-LD comparison:\")\n",
    "print(df2[df2['type']==\"added\"][['path']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d6f92a-5cd9-4c78-9c2a-0cd3247137c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all the removed paths (in JSON but not in JSON-LD)\n",
    "print(\"Removed in .ttl comparison:\")\n",
    "print(df1[df1['type']==\"removed\"][['path']].to_string(index=False))\n",
    "\n",
    "# show all the added paths (in JSON-LD but not in JSON)\n",
    "print(\"\\nAdded in .ttl comparison:\")\n",
    "print(df1[df1['type']==\"added\"][['path']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efd0d0-9277-4efa-88cf-d2fd1b90d74c",
   "metadata": {},
   "source": [
    "Checks for completeness and mapping and time taken, needs work #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a13eb-7679-4f4c-b346-24f25da72cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚îÄ‚îÄ User configuration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "# Which keys must appear in every run_summary.json?\n",
    "REQUIRED_TOPLEVEL = {\n",
    "    \"run_id\", \"start_time\", \"end_time\",\n",
    "    \"params\", \"metrics\", \"tags\", \"artifacts\"\n",
    "}\n",
    "\n",
    "# A couple of sub-fields we also want to spot-check:\n",
    "REQUIRED_PARAMS  = {\"random_state\"}\n",
    "REQUIRED_METRICS = {\"accuracy\"}\n",
    "\n",
    "JSON_SUMMARIES = glob.glob(\"MODEL_PROVENANCE/*_run_summary.json\")\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def iso8601(ms):\n",
    "    return datetime.fromtimestamp(ms/1000, tz=timezone.utc).isoformat()\n",
    "\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def write_json(path, obj):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "\n",
    "def convert_to_jsonld_and_ttl(summary, basename):\n",
    "    # build @context\n",
    "    ctx = {\n",
    "        \"prov\":    \"http://www.w3.org/ns/prov#\",\n",
    "        \"xsd\":     \"http://www.w3.org/2001/XMLSchema#\",\n",
    "        \"run\":     \"prov:Activity\",\n",
    "        \"start\":   \"prov:startedAtTime\",\n",
    "        \"end\":     \"prov:endedAtTime\",\n",
    "        \"used\":    \"prov:used\",\n",
    "        \"gen\":     \"prov:generated\",\n",
    "        \"param\":   \"prov:hadParameter\",\n",
    "        \"metric\":  \"prov:hadQuality\",\n",
    "        \"entity\":  \"prov:Entity\",\n",
    "        \"label\":   \"prov:label\",\n",
    "        \"value\":   \"prov:value\",\n",
    "        \"version\": \"prov:hadRevision\",\n",
    "        \"id\":      \"@id\",\n",
    "        \"type\":    \"@type\"\n",
    "    }\n",
    "\n",
    "    jsonld = {\n",
    "        \"@context\": ctx,\n",
    "        \"@id\":      f\"urn:run:{summary['run_id']}\",\n",
    "        \"@type\":    \"run\",\n",
    "        \"start\": {\n",
    "            \"@type\":  \"xsd:dateTime\",\n",
    "            \"@value\": iso8601(summary[\"start_time\"])\n",
    "        }\n",
    "    }\n",
    "    if summary.get(\"end_time\") is not None:\n",
    "        jsonld[\"end\"] = {\n",
    "            \"@type\":  \"xsd:dateTime\",\n",
    "            \"@value\": iso8601(summary[\"end_time\"])\n",
    "        }\n",
    "\n",
    "    # params\n",
    "    jsonld[\"param\"] = [\n",
    "        {\"@type\":\"entity\",\"label\":k,\"value\":str(v)}\n",
    "        for k,v in summary.get(\"params\",{}).items()\n",
    "    ]\n",
    "    # metrics\n",
    "    jsonld[\"metric\"] = [\n",
    "        {\"@type\":\"entity\",\"label\":k,\n",
    "         \"value\":{\"@type\":\"xsd:decimal\",\"@value\":v}}\n",
    "        for k,v in summary.get(\"metrics\",{}).items()\n",
    "    ]\n",
    "    # artifacts\n",
    "    jsonld[\"gen\"] = [\n",
    "        {\n",
    "            \"@type\":\"entity\",\n",
    "            \"label\": art.get(\"path\") or art.get(\"label\"),\n",
    "            \"prov:location\": (\n",
    "                art.get(\"uri\")\n",
    "                or (art.get(\"content\",\"\")[:30]+\"‚Ä¶\")\n",
    "                if isinstance(art.get(\"content\"),str)\n",
    "                else \"\"\n",
    "            )\n",
    "        }\n",
    "        for art in summary.get(\"artifacts\",[])\n",
    "    ]\n",
    "    # dataset used\n",
    "    jsonld[\"used\"] = {\n",
    "        \"@type\":\"entity\",\n",
    "        \"label\": summary[\"tags\"].get(\"dataset_name\"),\n",
    "        \"version\": summary[\"tags\"].get(\"dataset_version\")\n",
    "    }\n",
    "\n",
    "    # write JSON-LD\n",
    "    out_jsonld = f\"MODEL_PROVENANCE/{basename}.jsonld\"\n",
    "    write_json(out_jsonld, jsonld)\n",
    "\n",
    "    # serialize TTL\n",
    "    g = Graph().parse(data=json.dumps(jsonld), format=\"json-ld\")\n",
    "    out_ttl = f\"MODEL_PROVENANCE/{basename}.ttl\"\n",
    "    g.serialize(destination=out_ttl, format=\"turtle\")\n",
    "\n",
    "    return out_jsonld, out_ttl\n",
    "\n",
    "\n",
    "def normalize_jsonld(js):\n",
    "    \"\"\"Simple deep-sort so compare_json doesn‚Äôt trip over ordering.\"\"\"\n",
    "    if isinstance(js, dict):\n",
    "        return {k: normalize_jsonld(js[k]) for k in sorted(js)}\n",
    "    if isinstance(js, list):\n",
    "        return sorted((normalize_jsonld(el) for el in js),\n",
    "                      key=lambda x: json.dumps(x, sort_keys=True))\n",
    "    return js\n",
    "\n",
    "\n",
    "def diff_roundtrip(orig_json, jsonld_path, ttl_path):\n",
    "    orig = load_json(orig_json)\n",
    "    ld   = load_json(jsonld_path)\n",
    "\n",
    "    # parse TTL back to JSON-LD\n",
    "    g = Graph().parse(ttl_path, format=\"turtle\")\n",
    "    ttl_as_ld = json.loads(g.serialize(format=\"json-ld\"))\n",
    "\n",
    "    # normalize\n",
    "    nl = normalize_jsonld(ld)\n",
    "    nt = normalize_jsonld(ttl_as_ld)\n",
    "\n",
    "    return {\n",
    "        \"orig_vs_jsonld\":   compare_json(orig, ld),\n",
    "        \"jsonld_vs_ttl_ld\": compare_json(nl, nt)\n",
    "    }\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Main flow ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def main():\n",
    "    ok = 0\n",
    "    total = len(JSON_SUMMARIES)\n",
    "    missing_reports = []\n",
    "    cases = {}  # store diff results per run\n",
    "\n",
    "    for js_path in JSON_SUMMARIES:\n",
    "        summary = load_json(js_path)\n",
    "        base    = os.path.basename(js_path).split(\"_run_summary.json\")[0]\n",
    "\n",
    "        # 1) completeness check\n",
    "        if not REQUIRED_TOPLEVEL.issubset(summary):\n",
    "            missing = REQUIRED_TOPLEVEL - set(summary)\n",
    "            missing_reports.append((js_path, f\"missing fields {missing}\"))\n",
    "            continue\n",
    "\n",
    "        if not (REQUIRED_PARAMS <= summary[\"params\"].keys()):\n",
    "            missing_reports.append((js_path, f\"params missing {REQUIRED_PARAMS - summary['params'].keys()}\"))\n",
    "            continue\n",
    "\n",
    "        if not (REQUIRED_METRICS <= summary[\"metrics\"].keys()):\n",
    "            missing_reports.append((js_path, f\"metrics missing {REQUIRED_METRICS - summary['metrics'].keys()}\"))\n",
    "            continue\n",
    "\n",
    "        ok += 1\n",
    "\n",
    "        # 2) convert\n",
    "        jsonld_path, ttl_path = convert_to_jsonld_and_ttl(summary, base)\n",
    "\n",
    "        # 3) diff\n",
    "        diffs = diff_roundtrip(js_path, jsonld_path, ttl_path)\n",
    "        cases[base] = diffs\n",
    "        print(f\"\\n‚îÄ‚îÄ {base} diffs ‚îÄ‚îÄ\")\n",
    "        print(\"  ‚Ä¢ JSON ‚Üí JSON-LD:\", len(diffs[\"orig_vs_jsonld\"]), \"differences\")\n",
    "        print(\"  ‚Ä¢ JSON-LD ‚Üí TTL ‚Üí JSON-LD:\", len(diffs[\"jsonld_vs_ttl_ld\"]), \"differences\")\n",
    "\n",
    "    # 4) completeness summary\n",
    "    completeness_pct = (100 * ok / total) if total else 0\n",
    "    print(f\"\\n{ok}/{total} runs passed completeness checks ({completeness_pct:.1f}%).\")\n",
    "    if missing_reports:\n",
    "        print(\"\\nFailures:\")\n",
    "        for path, reason in missing_reports:\n",
    "            print(f\" ‚Ä¢ {path}: {reason}\")\n",
    "\n",
    "    # 5) integrity check\n",
    "    total_runs = len(cases)\n",
    "    zero_diff_runs = sum(\n",
    "        1\n",
    "        for diffs in cases.values()\n",
    "        if not diffs[\"orig_vs_jsonld\"] and not diffs[\"jsonld_vs_ttl_ld\"]\n",
    "    )\n",
    "    integrity_pct = (100 * zero_diff_runs / total_runs) if total_runs else 0\n",
    "    print(f\"\\nMapping integrity: {zero_diff_runs}/{total_runs} runs have zero diffs ‚Äî {integrity_pct:.1f}%\")\n",
    "\n",
    "    # 6) overall quality score\n",
    "    overall_score = (completeness_pct + integrity_pct) / 2\n",
    "    print(f\"Overall quality score: {overall_score:.1f}%\")\n",
    "\n",
    "    # 7) Benchmark your training fn\n",
    "    print(\"\\nBenchmarking train_and_log() overhead:\")\n",
    "    def train_and_log(use_mlflow=False):\n",
    "        # ‚Üê your real instrumentation + fit logic here\n",
    "        time.sleep(0.5 + (0.1 if use_mlflow else 0))  # stub\n",
    "        return\n",
    "\n",
    "    for flag in (False, True):\n",
    "        start = time.time()\n",
    "        train_and_log(use_mlflow=flag)\n",
    "        elapsed = time.time() - start\n",
    "        label = \"With MLflow\" if flag else \"No MLflow\"\n",
    "        print(f\"  ‚Ä¢ {label:10s}: {elapsed:.3f}s\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5883f673-371e-415e-a73e-5c9c88b56fb1",
   "metadata": {},
   "source": [
    "RQ2  implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d07ac1c-ea80-4787-bcb9-da047d12167d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load all run summary JSON files\n",
    "files = glob.glob(\"MODEL_PROVENANCE/*/*_run_summary.json\")\n",
    "rows = []\n",
    "for f in files:\n",
    "    with open(f) as fh:\n",
    "        summary = json.load(fh)\n",
    "    # Flatten parameters and metrics\n",
    "    row = {\"run_id\": summary[\"run_id\"]}\n",
    "    row.update({f\"param_{k}\": v for k, v in summary.get(\"params\", {}).items()})\n",
    "    row.update({f\"metric_{k}\": v for k, v in summary.get(\"metrics\", {}).items()})\n",
    "    row.update({f\"tag_{k}\": v for k, v in summary.get(\"tags\", {}).items()})\n",
    "    rows.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba148da6-6ce5-45cf-a985-f164a53c969b",
   "metadata": {},
   "source": [
    "1) Tracing preprocessing steps\n",
    ":\n",
    "Here are the top 4 Iris‚Äêfocused preprocessing‚Äêtracing use cases I‚Äôd tackle first:\n",
    "\n",
    "Reconstruct a run‚Äôs exact preprocessing\n",
    "Fetch a run‚Äôs run_id, columns_raw, dropped_columns, feature_names and test_size so you can replay the exact data pull & split.\n",
    "\n",
    "Feature‚Äêdrop impact analysis\n",
    "Identify runs where one or more measurements (e.g. petalwidthcm) were dropped and compare their test accuracies.\n",
    "\n",
    "Best feature subset discovery\n",
    "Group runs by which features they used (sepals only vs petals only vs both) and rank them by test F1 or accuracy.\n",
    "\n",
    "Common steps in high-accuracy runs\n",
    "Filter for runs with accuracy_score_X_test ‚â• 0.95 and list the shared preprocessing settings (dropped columns, test_size, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e147555-afbf-4bba-b6da-7e90ff391920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Helper to get the ‚Äúofficial‚Äù feature_names from your summary DF\n",
    "# def _get_all_features(df):\n",
    "#     # assumes every row has the same param_feature_names\n",
    "#     raw = df.loc[0, 'param_feature_names']\n",
    "#     return ast.literal_eval(raw)\n",
    "\n",
    "# # Train & eval RF on just these columns of Iris\n",
    "# def evaluate_subset(features, test_size=0.2, random_state=42, n_estimators=200):\n",
    "#     iris = load_iris()\n",
    "#     X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "#     # map sklearn‚Äôs names to your param names, e.g. \"sepal length (cm)\" ‚Üí \"sepallengthcm\"\n",
    "#     canon = _get_all_features(df)\n",
    "#     mapping = dict(zip(iris.feature_names, canon))\n",
    "#     X = X.rename(columns=mapping)\n",
    "#     X_sub = X[features]\n",
    "#     y = iris.target\n",
    "#     Xtr, Xte, ytr, yte = train_test_split(X_sub, y, test_size=test_size, random_state=random_state)\n",
    "#     m = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "#     m.fit(Xtr, ytr)\n",
    "#     return accuracy_score(yte, m.predict(Xte))\n",
    "# def trace_preprocessing(df, run_id=None):\n",
    "#     cols = ['run_id',\n",
    "#             'param_dataset.title',\n",
    "#             'param_columns_raw',\n",
    "#             'param_dropped_columns',\n",
    "#             'param_feature_names',\n",
    "#             'param_dataset.authors', 'param_dataset.doi', 'param_dataset.published',\n",
    "#             'param_test_size',\n",
    "#             'param_criterion',\n",
    "#             'param_max_depth','param_max_leaf_nodes', 'param_max_samples',\n",
    "#            'metric_accuracy','metric_f1_macro','metric_roc_auc']\n",
    "#     if run_id is None:\n",
    "#         subset = df.loc[:, cols]\n",
    "#     else:\n",
    "#         subset = df.loc[df['run_id'] == run_id, cols]\n",
    "#     return subset.to_dict(orient='records')\n",
    "\n",
    "\n",
    "# def drop_impact(df, feature, **_):\n",
    "#     all_feats = _get_all_features(df)\n",
    "#     baseline = evaluate_subset(all_feats)\n",
    "#     without = [f for f in all_feats if f!=feature]\n",
    "#     dropped = evaluate_subset(without)\n",
    "#     return {\n",
    "#       'dropped_feature': feature,\n",
    "#       'baseline_acc': baseline,\n",
    "#       'dropped_acc': dropped,\n",
    "#       'impact': baseline - dropped\n",
    "#     }\n",
    "\n",
    "# def drop_impact_all(df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "#     \"\"\"\n",
    "#     Compute drop-impact for every feature in the dataset.\n",
    "#     Returns list of dicts with dropped_feature, baseline_acc, dropped_acc, impact.\n",
    "#     \"\"\"\n",
    "#     feats = _get_all_features(df)\n",
    "#     baseline = evaluate_subset(feats)\n",
    "#     summary = []\n",
    "#     for feat in feats:\n",
    "#         without = [f for f in feats if f != feat]\n",
    "#         acc = evaluate_subset(without)\n",
    "#         summary.append({\n",
    "#             'dropped_feature': feat,\n",
    "#             'baseline_acc': baseline,\n",
    "#             'dropped_acc': acc,\n",
    "#             'impact': round(baseline - acc, 4)\n",
    "#         })\n",
    "#     return summary\n",
    "\n",
    "# def best_feature_subset(df, features, **_):\n",
    "#     acc = evaluate_subset(features)\n",
    "#     return {'features': features, 'accuracy': acc}\n",
    "\n",
    "# def common_high_accuracy(df: pd.DataFrame, threshold: float = 0.95) -> List[Dict[str, Any]]:\n",
    "#     \"\"\"\n",
    "#     Filter runs with test accuracy >= threshold and list unique shared preprocessing settings.\n",
    "#     \"\"\"\n",
    "#     high = df[df['metric_accuracy_score_X_test'] >= threshold]\n",
    "#     cols = ['param_dropped_columns', 'param_test_size', 'param_feature_names']\n",
    "#     return high[cols].drop_duplicates().to_dict(orient='records')\n",
    "\n",
    "\n",
    "# # --------------------------------------------\n",
    "# # Use Case Registry with parameter order for minimal input\n",
    "# # --------------------------------------------\n",
    "# USE_CASES = {\n",
    "#     'trace_preprocessing': {\n",
    "#         'func': trace_preprocessing,\n",
    "#         'required_params': [],            # none strictly required\n",
    "#         'optional_params': ['run_id'],    # run_id can be supplied or not\n",
    "#     },\n",
    "#     'drop_impact': {\n",
    "#         'func': drop_impact,\n",
    "#         'required_params': ['feature'],\n",
    "#         'optional_params': [],\n",
    "#     },\n",
    "#      'drop_impact_all': {\n",
    "#         'func': drop_impact_all,\n",
    "#         'required_params': [],\n",
    "#         'optional_params': [],\n",
    "#     },\n",
    "#     'best_feature_subset': {\n",
    "#         'func': best_feature_subset,\n",
    "#         'required_params': ['features'],\n",
    "#         'optional_params': [],\n",
    "#     },\n",
    "#     'common_high_accuracy': {\n",
    "#         'func': common_high_accuracy,\n",
    "#         'required_params': ['threshold'],\n",
    "#         'optional_params': [],\n",
    "#     },\n",
    "# }\n",
    "\n",
    "\n",
    "# def call_use_case(df, use_case_name, **kwargs):\n",
    "#     if use_case_name not in USE_CASES:\n",
    "#         raise ValueError(f\"Unknown use case: {use_case_name}\")\n",
    "#     case = USE_CASES[use_case_name]\n",
    "#     func = case['func']\n",
    "#     # check required\n",
    "#     missing = [p for p in case['required_params'] if p not in kwargs]\n",
    "#     if missing:\n",
    "#         raise ValueError(f\"{use_case_name} missing required params: {missing}\")\n",
    "#     # build args\n",
    "#     args = {p: kwargs[p] for p in case['required_params']}\n",
    "#     for p in case['optional_params']:\n",
    "#         args[p] = kwargs.get(p)\n",
    "#     return func(df, **args)\n",
    "\n",
    "# # --------------------------------------------\n",
    "# # Example Usage\n",
    "# # --------------------------------------------\n",
    "# if __name__ == '__main__':\n",
    "#    # # 1) trace_preprocessing for all runs\n",
    "#     print(call_use_case(df, 'trace_preprocessing'))\n",
    "    \n",
    "#     # 2) trace_preprocessing for a single run_id\n",
    "#     print(call_use_case(df, 'trace_preprocessing', run_id='361daa12f99f4129a06cd20b78dd6fa7'))\n",
    "\n",
    "#     # 5) common_high_accuracy\n",
    "#     print(call_use_case(df, 'common_high_accuracy', threshold=0.99))\n",
    "\n",
    "#     # 4) Best‚Äêsubset on just sepals:\n",
    "#     print(call_use_case(df, 'best_feature_subset', features=['sepallengthcm','sepalwidthcm']))\n",
    "\n",
    "#     # 3) Drop‚Äêimpact for ‚Äúpetallengthcm‚Äù:\n",
    "#     print(call_use_case(df, 'drop_impact', feature='petallengthcm'))\n",
    "\n",
    "#     print(call_use_case(df, 'drop_impact_all'))  # summary for all features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f912d6-0e84-4155-858a-9668bef63f6e",
   "metadata": {},
   "source": [
    " ‚Ä¢ Detecting models trained with deprecated code versions\n",
    " ‚Ä¢ Mapping models to specific datasets used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a02c9a-5459-478f-a3c5-7f7a58ff22b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_deprecated_code(df: pd.DataFrame, deprecated_commits: List[str], **_) -> List[Dict[str, Any]]:\n",
    "    # we know the column is called tag_git_current_commit_hash\n",
    "    commit_col = 'tag_git_current_commit_hash'\n",
    "    if commit_col not in df.columns:\n",
    "        raise KeyError(f\"Missing {commit_col} in DataFrame\")\n",
    "    out = df[df[commit_col].isin(deprecated_commits)]\n",
    "    # include run_id and notebook/runName for context\n",
    "    cols = ['run_id', commit_col, 'tag_notebook_name', 'tag_mlflow.runName']\n",
    "    # drop any that don‚Äôt exist\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    return out[cols].to_dict(orient='records')\n",
    "\n",
    "\n",
    "def map_model_dataset(df: pd.DataFrame, **_) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For each run, return its model name (or run_id) alongside the dataset\n",
    "    title, DOI, published date and publisher.\n",
    "    \"\"\"\n",
    "    # pick whichever model-name column you have\n",
    "    model_col = 'tag_model_name' if 'tag_model_name' in df.columns else 'param_model_name'\n",
    "    cols = [\n",
    "        'run_id',\n",
    "        model_col,\n",
    "        'param_dataset.title',\n",
    "        'param_dataset.doi',\n",
    "        'param_dataset.published',\n",
    "        'param_dataset.publisher'\n",
    "    ]\n",
    "    # filter out any columns that don‚Äôt actually exist\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    return df[cols].to_dict(orient='records')\n",
    "\n",
    "# --------------------------------------------\n",
    "# Extend Use-Case Registry\n",
    "# --------------------------------------------\n",
    "USE_CASES.update({\n",
    "    'detect_deprecated_code': {\n",
    "        'func': detect_deprecated_code,\n",
    "        'required_params': ['deprecated_commits'],\n",
    "        'optional_params': []\n",
    "    },\n",
    "    'map_model_dataset': {\n",
    "        'func': map_model_dataset,\n",
    "        'required_params': [],\n",
    "        'optional_params': []\n",
    "    },\n",
    "})\n",
    "# 1) Detect runs on deprecated commits:\n",
    "deprecated = [\n",
    "    \"a07434af4f547af2daab044d6873eb7081162293\",\n",
    "    \"d329c92495e196ec0f39fbb19dfdd367131a77d9\"\n",
    "]\n",
    "# print(call_use_case(df, \"detect_deprecated_code\", deprecated_commits=deprecated))\n",
    "pprint(call_use_case(df, 'map_model_dataset'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52607ad-5849-4a2d-97ef-e8fc1ca16dc7",
   "metadata": {},
   "source": [
    "Goal: Notify collaborators who have forked the GitHub repo if their fork is outdated (i.e., behind the current commit used to train a model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29c8ad9-00bb-4c1e-ac3b-ee6861991acd",
   "metadata": {},
   "source": [
    "üß† What We Need\n",
    "Current training run‚Äôs Git commit hash\n",
    "\n",
    "GitHub API to fetch all forks of your repo\n",
    "\n",
    "Compare each fork‚Äôs main or master branch head commit\n",
    "\n",
    "Create an issue on their fork or on your repo tagging them if they‚Äôre behind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72bed50-fb56-442d-a21e-bb7991892d07",
   "metadata": {},
   "source": [
    ": Notify via issues on your own repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f147c-9d0a-4d7f-a4ab-545d1e2375fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def notify_outdated_forks():\n",
    "    load_dotenv()\n",
    "    token     = os.getenv(\"THESIS_TOKEN\")\n",
    "    owner     = \"reema-dass26\"\n",
    "    repo      = \"REPO\"\n",
    "\n",
    "    if not token:\n",
    "        print(\"‚ö†Ô∏è GITHUB_TOKEN not set.\")\n",
    "        return\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {token}\",\n",
    "        \"Accept\":        \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "\n",
    "    # 1) Get latest upstream commit\n",
    "    main_commits = requests.get(\n",
    "        f\"https://api.github.com/repos/{owner}/{repo}/commits\",\n",
    "        headers=headers,\n",
    "        params={\"per_page\": 1}\n",
    "    )\n",
    "    main_commits.raise_for_status()\n",
    "    new_commit_hash = main_commits.json()[0][\"sha\"]\n",
    "    print(f\"Latest upstream commit: {new_commit_hash}\")\n",
    "\n",
    "    # 2) List forks\n",
    "    forks_resp = requests.get(f\"https://api.github.com/repos/{owner}/{repo}/forks\", headers=headers)\n",
    "    forks_resp.raise_for_status()\n",
    "    forks = forks_resp.json()\n",
    "\n",
    "    # 3) Compare each fork\n",
    "    outdated = []\n",
    "    for fork in forks:\n",
    "        fork_owner = fork[\"owner\"][\"login\"]\n",
    "        fork_comm = requests.get(\n",
    "            fork[\"url\"] + \"/commits\",\n",
    "            headers=headers,\n",
    "            params={\"per_page\": 1}\n",
    "        )\n",
    "        if fork_comm.status_code != 200:\n",
    "            print(f\"¬†¬†‚Äì could not fetch commits for {fork_owner}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        fork_sha = fork_comm.json()[0][\"sha\"]\n",
    "        if fork_sha != new_commit_hash:\n",
    "            outdated.append(f\"@{fork_owner}\")\n",
    "\n",
    "    # 4) Open an issue if any are behind\n",
    "    if outdated:\n",
    "        title = \"üîî Notification: Your fork is behind the latest commit\"\n",
    "        body  = (\n",
    "            f\"Hi {' '.join(outdated)},\\n\\n\"\n",
    "            f\"The main repository has been updated to commit `{new_commit_hash}`.\\n\"\n",
    "            \"Please consider pulling the latest changes to stay in sync.\\n\\n\"\n",
    "            \"Thanks!\"\n",
    "        )\n",
    "        issues_url = f\"https://api.github.com/repos/{owner}/{repo}/issues\"\n",
    "        resp = requests.post(\n",
    "        issues_url,\n",
    "        headers=headers,\n",
    "        json={\"title\": title, \"body\": body}\n",
    "    )\n",
    "\n",
    "    # DEBUGGING OUTPUT\n",
    "    print(f\"‚Üí POST {issues_url}\")\n",
    "    print(\"‚Üí Status code:\", resp.status_code)\n",
    "    print(\"‚Üí Response headers:\", resp.headers)\n",
    "    try:\n",
    "        data = resp.json()\n",
    "        print(\"‚Üí Response JSON:\", data)\n",
    "        print(\"‚Üí html_url field:\", data.get(\"html_url\"))\n",
    "    except ValueError:\n",
    "        print(\"‚Üí No JSON response body; raw text:\", resp.text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    answer = input(\"Do you want to notify collaborators whose forks are behind? (y/N): \").strip().lower()\n",
    "    if answer in (\"y\", \"yes\"):\n",
    "        notify_outdated_forks()\n",
    "    else:\n",
    "        print(\"No action taken.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda31f16-fbe9-40ce-ac1b-9ebc898c8820",
   "metadata": {},
   "source": [
    "INVENIO INTEGRETION to upload the necessary files and publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e5a7fc-3b03-45c8-bc90-817ea5ba7352",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# TEST CODE FOR INVENIO INTEGRETION\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# API_BASE = \"https://127.0.0.1:5000\"\n",
    "# TOKEN    = \"8LnqJuz3TsBHffnDJ3isPLHYHtRbWrC0M667Nb5haEbnXpWqGbFRyfDApymr\"\n",
    "\n",
    "# # 1) Test read‚Äêscope by listing records (no size param or size=1)\n",
    "# resp = requests.get(\n",
    "#     f\"{API_BASE}/api/records\",\n",
    "#     headers={\"Authorization\": f\"Bearer {TOKEN}\"},\n",
    "#     verify=False\n",
    "# )\n",
    "# print(resp.status_code)\n",
    "# # should be 200 and a JSON page of records\n",
    "\n",
    "# # or explicitly:\n",
    "# resp = requests.get(\n",
    "#     f\"{API_BASE}/api/records?size=1\",\n",
    "#     headers={\"Authorization\": f\"Bearer {TOKEN}\"},\n",
    "#     verify=False\n",
    "# )\n",
    "# print(resp.status_code, resp.json())\n",
    "# #################################################################################################\n",
    "# API_BASE = \"https://127.0.0.1:5000\"\n",
    "# TOKEN    = \"8LnqJuz3TsBHffnDJ3isPLHYHtRbWrC0M667Nb5haEbnXpWqGbFRyfDApymr\"\n",
    "\n",
    "# resp = requests.options(\n",
    "#     f\"{API_BASE}/api/records\",\n",
    "#     headers={\"Authorization\": f\"Bearer {TOKEN}\"},\n",
    "#     verify=False\n",
    "# )\n",
    "# print(\"Allowed methods:\", resp.headers.get(\"Allow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745dc1c9-ed88-45dc-bd8c-1065c9c17aa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "API_BASE   = \"https://127.0.0.1:5000\"\n",
    "TOKEN      = \"8LnqJuz3TsBHffnDJ3isPLHYHtRbWrC0M667Nb5haEbnXpWqGbFRyfDApymr\"\n",
    "VERIFY_SSL = False  # only for self‚Äêsigned dev\n",
    "\n",
    "HEADERS_JSON = {\n",
    "    \"Accept\":        \"application/json\",\n",
    "    \"Content-Type\":  \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "}\n",
    "\n",
    "HEADERS_OCTET = {\n",
    "    \"Content-Type\":  \"application/octet-stream\",\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "}\n",
    "\n",
    "# The folders you want to walk & upload:\n",
    "TO_UPLOAD = [\"Trained_models\", \"plots\", \"MODEL_PROVENANCE\"]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Create draft with ALL required metadata\n",
    "# -----------------------------------------------------------------------------\n",
    "def create_draft():\n",
    "    payload = {\n",
    "  \"metadata\": {\n",
    "    \"title\":            \"RandomForest Iris Model Artifacts\",\n",
    "    \"creators\": [ {\n",
    "      \"person_or_org\": {\n",
    "        \"type\":        \"personal\",\n",
    "        \"given_name\":  \"Reema\",\n",
    "        \"family_name\": \"Dass\"\n",
    "      }\n",
    "    } ],\n",
    "    \"publication_date\": \"2025-04-24\",\n",
    "    \"resource_type\":    { \"id\": \"software\" },\n",
    "    \"access\": {\n",
    "      \"record\": \"public\",\n",
    "      \"files\":  \"public\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "    r = requests.post(f\"{API_BASE}/api/records\",\n",
    "                      headers=HEADERS_JSON,\n",
    "                      json=payload,\n",
    "                      verify=VERIFY_SSL)\n",
    "    r.raise_for_status()\n",
    "    draft = r.json()\n",
    "    print(\"‚úÖ Draft created:\", draft[\"id\"])\n",
    "    return draft[\"id\"], draft[\"links\"]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Register, upload and commit a single file\n",
    "# -----------------------------------------------------------------------------\n",
    "def upload_and_commit(links, key, path):\n",
    "    # 2a) register the filename in the draft\n",
    "    r1 = requests.post(links[\"files\"],\n",
    "                       headers=HEADERS_JSON,\n",
    "                       json=[{\"key\": key}],\n",
    "                       verify=VERIFY_SSL)\n",
    "    r1.raise_for_status()\n",
    "    entry = next(e for e in r1.json()[\"entries\"] if e[\"key\"] == key)\n",
    "    file_links = entry[\"links\"]\n",
    "\n",
    "    # 2b) upload the bytes\n",
    "    with open(path, \"rb\") as fp:\n",
    "        r2 = requests.put(file_links[\"content\"],\n",
    "                          headers=HEADERS_OCTET,\n",
    "                          data=fp,\n",
    "                          verify=VERIFY_SSL)\n",
    "    r2.raise_for_status()\n",
    "\n",
    "    # 2c) commit the upload\n",
    "    r3 = requests.post(file_links[\"commit\"],\n",
    "                       headers=HEADERS_JSON,\n",
    "                       verify=VERIFY_SSL)\n",
    "    r3.raise_for_status()\n",
    "    print(f\"  ‚Ä¢ Uploaded {key}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Walk each folder and upload every file\n",
    "# -----------------------------------------------------------------------------\n",
    "def upload_folder(links):\n",
    "    for folder in TO_UPLOAD:\n",
    "        if not os.path.isdir(folder):\n",
    "            print(f\"‚ö†Ô∏è Skipping missing folder {folder}\")\n",
    "            continue\n",
    "        base = os.path.dirname(folder) or folder\n",
    "        for root, _, files in os.walk(folder):\n",
    "            for fn in files:\n",
    "                local = os.path.join(root, fn)\n",
    "                # create a POSIX‚Äêstyle key preserving subfolders\n",
    "                key = os.path.relpath(local, start=base).replace(os.sep, \"/\")\n",
    "                upload_and_commit(links, key, local)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Publish the draft\n",
    "# -----------------------------------------------------------------------------\n",
    "def publish(links):\n",
    "    r = requests.post(links[\"publish\"],\n",
    "                      headers=HEADERS_JSON,\n",
    "                      verify=VERIFY_SSL)\n",
    "    if not r.ok:\n",
    "        print(\"‚ùå Publish failed:\", r.status_code, r.text)\n",
    "        try: print(r.json())\n",
    "        except: pass\n",
    "        r.raise_for_status()\n",
    "    print(\"‚úÖ Published:\", r.json()[\"id\"])\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    recid, links = create_draft()\n",
    "    upload_folder(links)\n",
    "    publish(links)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b463223e-5425-465c-ae63-815cbb053301",
   "metadata": {},
   "source": [
    "########################################################################\n",
    "# FETCH metadata from INVENIO\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee538759-38b9-4ea8-bdc6-41cc65ede642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_metadata(record_id, model_name, api_base, headers, verify_ssl=True):\n",
    "    \"\"\"\n",
    "    Fetch Invenio metadata and save to a file named after the model inside 'Invenio_metadata' folder.\n",
    "    \"\"\"\n",
    "    response = requests.get(f\"{API_BASE}/api/records/{record_id}\",\n",
    "                            headers=headers,\n",
    "                            verify=VERIFY_SSL)\n",
    "    response.raise_for_status()\n",
    "    metadata = response.json()\n",
    "\n",
    "    print(\"‚úÖ Metadata fetched successfully\")\n",
    "\n",
    "    # Create the folder if it doesn't exist\n",
    "    os.makedirs(\"Invenio_metadata\", exist_ok=True)\n",
    "\n",
    "    # Construct path and save\n",
    "    file_path = os.path.join(\"Invenio_metadata\", f\"{model_name}_invenio.json\")\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ Metadata saved at {file_path}\")\n",
    "    return file_path\n",
    "path = fetch_metadata(recid, model_name, api_base=API_BASE, headers=HEADERS_JSON)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7423f2-0ff3-4104-913e-50eeb32d9d0f",
   "metadata": {},
   "source": [
    "METADATA EXTRACTION FROM INVENIO and ADD it to main Provenence FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d5968b-997e-4458-bf6b-a14dcc883698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Function: Extract metadata\n",
    "# ----------------------------\n",
    "def extract_metadata(metadata):\n",
    "    print(\"‚úÖ Metadata loaded successfully\")\n",
    "    print(\"‚ÑπÔ∏è ID:\", metadata.get(\"id\", \"N/A\"))\n",
    "    print(\"üîç Extracting required fields...\")\n",
    "\n",
    "    extracted_data = {\n",
    "        \"invenio_metadata\": {\n",
    "            \"id\": metadata.get(\"id\", \"\"),\n",
    "            \"title\": metadata.get(\"metadata\", {}).get(\"title\", \"\"),\n",
    "            \"creator\": \", \".join([\n",
    "                creator[\"person_or_org\"].get(\"name\", \"\")\n",
    "                for creator in metadata.get(\"metadata\", {}).get(\"creators\", [])\n",
    "            ]),\n",
    "            \"publication_date\": metadata.get(\"metadata\", {}).get(\"publication_date\", \"\"),\n",
    "            \"files\": [],\n",
    "            \"pids\": metadata.get(\"pids\", {}),\n",
    "            \"version_info\": metadata.get(\"versions\", {}),\n",
    "            \"status\": metadata.get(\"status\", \"\"),\n",
    "            \"views\": metadata.get(\"stats\", {}).get(\"this_version\", {}).get(\"views\", 0),\n",
    "            \"downloads\": metadata.get(\"stats\", {}).get(\"this_version\", {}).get(\"downloads\", 0),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for key, file_info in metadata.get(\"files\", {}).get(\"entries\", {}).items():\n",
    "        file_detail = {\n",
    "            \"key\": key,\n",
    "            \"url\": file_info[\"links\"].get(\"content\", \"\"),\n",
    "            \"size\": file_info.get(\"size\", 0),\n",
    "            \"mimetype\": file_info.get(\"mimetype\", \"\"),\n",
    "            \"checksum\": file_info.get(\"checksum\", \"\"),\n",
    "            \"metadata\": file_info.get(\"metadata\", {}),\n",
    "        }\n",
    "        extracted_data[\"invenio_metadata\"][\"files\"].append(file_detail)\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "invenio_path = f\"Invenio_metadata/{model_name}_invenio.json\"\n",
    "run_summary_path = f\"MODEL_PROVENANCE/{model_name}/{model_name}_run_summary.json\"\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Load Invenio metadata\n",
    "# ----------------------------\n",
    "with open(invenio_path, \"r\") as f:\n",
    "    original_metadata = json.load(f)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Extract metadata\n",
    "# ----------------------------\n",
    "extracted_metadata = extract_metadata(original_metadata)\n",
    "print(\"üì§ Extracted Metadata Preview:\")\n",
    "print(json.dumps(extracted_metadata, indent=4)[:1000])  # Preview\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Load run summary\n",
    "# ----------------------------\n",
    "with open(run_summary_path, \"r\") as f:\n",
    "    existing_metadata = json.load(f)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 4: Merge metadata\n",
    "# ----------------------------\n",
    "existing_metadata.update(extracted_metadata)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 5: Save updated summary\n",
    "# ----------------------------\n",
    "with open(run_summary_path, \"w\") as f:\n",
    "    json.dump(existing_metadata, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Invenio metadata embedded successfully into: {run_summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa460d3-f443-46b1-ba5e-4f1339ba4eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_git_version_info():\n",
    "    try:\n",
    "        commit = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).decode().strip()\n",
    "        tag = subprocess.check_output([\"git\", \"describe\", \"--tags\", \"--exact-match\"], stderr=subprocess.DEVNULL).decode().strip()\n",
    "    except subprocess.CalledProcessError:\n",
    "        tag = \"untagged\"\n",
    "    return commit, tag\n",
    "\n",
    "commit_hash, version_tag = get_git_version_info()\n",
    "print(\"Commit:\", commit_hash)\n",
    "print(\"Version Tag:\", version_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e6109-de5e-4ba4-baae-7de79c1fb131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
