{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ff24f2a-931c-4543-b179-dc8bf1bfd74d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "########################################################\n",
    "# EXPERIMENT CODE\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb7ee73-aa97-48ef-b1c8-f00cc2af0b0f",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Collects metadata about a user session for logging or reproducibility purposes.\n",
    "This includes system details, user information, and optional project metadata.\n",
    "It supports:\n",
    "- Automatic population via environment variables.\n",
    "- Interactive prompting if fields are missing (when `prompt_fields=True`).\n",
    "- Fixed overrides for role and project ID if provided.\n",
    "Useful in collaborative research, ML experiment tracking, or Jupyter environments.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f97a375-387a-4368-a629-556f56f51dcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import platform\n",
    "import sys\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "def prompt_if_none(env_key, prompt_text, default_value=\"unknown\"):\n",
    "    val = os.getenv(env_key)\n",
    "    if not val:\n",
    "        try:\n",
    "            val = input(f\"{prompt_text} (default: {default_value}): \").strip() or default_value\n",
    "        except Exception:\n",
    "            val = default_value\n",
    "    return val\n",
    "\n",
    "def collect_session_metadata(\n",
    "    prompt_fields=True,\n",
    "    fixed_role=None,\n",
    "    fixed_project_id=None\n",
    "):\n",
    "    session_id = str(uuid.uuid4())\n",
    "    \n",
    "    session_metadata = {\n",
    "        \"session_id\": session_id,\n",
    "        \"username\": os.getenv(\"JUPYTERHUB_USER\", getpass.getuser()),\n",
    "        \"timestamp_utc\": datetime.utcnow().isoformat(),\n",
    "        \"hostname\": platform.node(),\n",
    "        \"platform\": platform.system(),\n",
    "        \"os_version\": platform.version(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "    }\n",
    "\n",
    "    # Prompt or use defaults\n",
    "    session_metadata[\"role\"] = fixed_role or (\n",
    "        prompt_if_none(\"RESEARCHER_ROLE\", \"Enter your role\", \"collaborator\") if prompt_fields \n",
    "        else os.getenv(\"RESEARCHER_ROLE\", \"researcher\")\n",
    "    )\n",
    "    session_metadata[\"project_id\"] = fixed_project_id or (\n",
    "        prompt_if_none(\"PROJECT_ID\", \"Enter project ID\", \"default_project\") if prompt_fields \n",
    "        else os.getenv(\"PROJECT_ID\", \"default_project\")\n",
    "    )\n",
    "\n",
    "    print(\"\\nğŸ“Œ Session Metadata:\")\n",
    "    for k, v in session_metadata.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    return session_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca4f0ae-39ee-4b22-b013-dfb1fa1b5694",
   "metadata": {},
   "source": [
    "LIBRARY IMPORTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca332e5-6501-4310-920b-2b769477b46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# ğŸ“¦ Standard Library Imports\n",
    "# ============================\n",
    "import os\n",
    "import glob\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import ast\n",
    "import pickle\n",
    "import platform\n",
    "import subprocess\n",
    "from datetime import datetime, timezone\n",
    "from pprint import pprint\n",
    "from typing import List, Dict, Any\n",
    "import xml.etree.ElementTree as ET\n",
    "import urllib.parse\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ğŸ“Š Data and Visualization\n",
    "# ============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================\n",
    "# ğŸ¤– Machine Learning\n",
    "# ============================\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    RocCurveDisplay,\n",
    "    PrecisionRecallDisplay\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "# ============================\n",
    "# ğŸ”¬ Experiment Tracking\n",
    "# ============================\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# ============================\n",
    "# ğŸŒ Web / API / Networking\n",
    "# ============================\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ============================\n",
    "# ğŸ§ª Git & Version Control\n",
    "# ============================\n",
    "import git\n",
    "from git import Repo, GitCommandError\n",
    "import hashlib\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ğŸ§  SHAP for Explainability\n",
    "# ============================\n",
    "import shap\n",
    "\n",
    "# ============================\n",
    "# ğŸ§¬ RDF & Provenance (rdflib)\n",
    "# ============================\n",
    "from rdflib import Graph, URIRef, Literal\n",
    "from rdflib.namespace import PROV, XSD\n",
    "\n",
    "# ============================\n",
    "# âš™ï¸ System Monitoring\n",
    "# ============================\n",
    "import psutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfde18b5-ae9c-442c-a5b3-7dfb06957646",
   "metadata": {},
   "source": [
    "#Dataset metadata!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a394398-cd25-45b5-89ac-6d909b65d417",
   "metadata": {},
   "source": [
    "#Metadata from ZONEDO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de603b35-9534-477d-a851-8b6e5efb910c",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Fetches and structures dataset metadata from a given DOI using the DataCite API.\n",
    "\n",
    "This function:\n",
    "- Sends a GET request to the DataCite API to retrieve metadata about a dataset.\n",
    "- Extracts important fields (e.g., title, creator, publisher, version, license).\n",
    "- Fills in placeholders like \"info not available\" when data is missing.\n",
    "- Adds additional fields based on the PROV-O provenance model for traceability.\n",
    "\n",
    "Returns a dictionary that can be used for logging, tracking, or enriching metadata\n",
    "in data catalogs, ML pipelines, or reproducibility systems.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3e945-508a-4c1c-8bb1-c1b5d9121615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "\n",
    "def extract_dataset_metadata_from_doi(doi: str) -> dict:\n",
    "    base_url = f\"https://api.datacite.org/dois/{doi.lower()}\"\n",
    "    r = requests.get(base_url)\n",
    "    r.raise_for_status()\n",
    "    meta = r.json().get(\"data\", {}).get(\"attributes\", {})\n",
    "\n",
    "    # Print all available raw metadata (for debugging/exploration)\n",
    "    pprint.pprint(meta)\n",
    "\n",
    "    # --- Standard metadata fields ---\n",
    "    title = meta.get(\"titles\", [{}])[0].get(\"title\", \"info not available\")\n",
    "    creators = [c.get(\"name\", \"\") for c in meta.get(\"creators\", [])]\n",
    "    publisher = meta.get(\"publisher\", \"info not available\")\n",
    "    pub_year = meta.get(\"publicationYear\", \"info not available\")\n",
    "    url = meta.get(\"url\", f\"https://doi.org/{doi}\")\n",
    "    created = meta.get(\"created\")\n",
    "    updated = meta.get(\"updated\")\n",
    "    description = meta.get(\"descriptions\", [])\n",
    "    license_info = meta.get(\"rightsList\", [])\n",
    "    language = meta.get(\"language\", \"info not available\")\n",
    "    subjects = meta.get(\"subjects\", [])\n",
    "    related_ids = meta.get(\"relatedIdentifiers\", [])\n",
    "    citation_count = meta.get(\"citationCount\", 0)\n",
    "    registered = meta.get(\"registered\", \"\")\n",
    "    schema_version = meta.get(\"schemaVersion\", \"\")\n",
    "    citation_trend = meta.get(\"citationsOverTime\", [])\n",
    "\n",
    "    # --- Convert useful extras into strings ---\n",
    "    dataset_description = description[0][\"description\"] if description else \"info not available\"\n",
    "    dataset_license = license_info[0][\"rights\"] if license_info else \"info not available\"\n",
    "    dataset_subjects = \", \".join(s.get(\"subject\", \"\") for s in subjects) if subjects else \"info not available\"\n",
    "    related_resources = [r.get(\"relatedIdentifier\", \"\") for r in related_ids]\n",
    "\n",
    "    # --- Structured output ---\n",
    "    dataset_metadata = {\n",
    "        \"dataset_id\": doi,\n",
    "        \"dataset_title\": title,\n",
    "        \"dataset_creator\": \", \".join(creators) if creators else \"info not available\",\n",
    "        \"dataset_publisher\": publisher,\n",
    "        \"dataset_publication_date\": pub_year,\n",
    "        \"dataset_description\": dataset_description,\n",
    "        \"dataset_version\": meta.get(\"version\", \"info not available\"),\n",
    "        \"dataset_license\": dataset_license,\n",
    "        \"dataset_subjects\": dataset_subjects,\n",
    "        \"dataset_language\": language,\n",
    "        \"dataset_access_url\": url,\n",
    "        \"dataset_documentation\": url,\n",
    "        \"metadata_standard\": meta.get(\"types\", {}).get(\"resourceTypeGeneral\", \"info not available\"),\n",
    "        \"related_resources\": related_resources,\n",
    "        \"citation_count\": citation_count,\n",
    "        \"citations_over_time\": citation_trend,\n",
    "        \"schema_version\": schema_version,\n",
    "        \"registered_date\": registered,\n",
    "        \"created\": created,\n",
    "        \"updated\": updated,\n",
    "\n",
    "        # PROV-O traceability fields\n",
    "        \"prov_entity\": title,\n",
    "        \"prov_activity\": \"Ingestion and Publication\",\n",
    "        \"prov_agent_dataset_creator\": \", \".join(creators) if creators else \"info not available\",\n",
    "        \"prov_used\": url,\n",
    "        \"prov_wasDerivedFrom\": doi,\n",
    "        \"prov_wasAttributedTo\": \", \".join(creators) if creators else \"info not available\",\n",
    "        \"prov_startedAtTime\": pub_year,\n",
    "        \"prov_role_dataset_creator\": \"Original Data Author\",\n",
    "        \"prov_role_database_creator\": \"Database Ingestor and Maintainer\"\n",
    "    }\n",
    "\n",
    "    return dataset_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5384df92-5841-472c-872d-370452f6f902",
   "metadata": {},
   "outputs": [],
   "source": [
    " # doi_metadata = extract_dataset_metadata_from_doi(\"10.24432/C56C76\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef6b25a-51ae-4afa-87ee-64a855c0aaa2",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Fetches and compiles structured metadata about a database table from a local API.\n",
    "\n",
    "This function:\n",
    "- Retrieves database-level metadata (e.g., name, owner, description).\n",
    "- Retrieves versioned history for the specific table.\n",
    "- Constructs a flat metadata dictionary with standardized and FAIR4ML/PROV-Oâ€“style fields.\n",
    "- Can be used in ML pipelines, reproducibility tracking, or data cataloging systems.\n",
    "\n",
    "Arguments:\n",
    "- db_id (str): The database identifier.\n",
    "- table_id (str): The table identifier within the database.\n",
    "- selected_version (str): Version string for the dataset.\n",
    "- target_variable (str): Name of the ML target column.\n",
    "- num_samples (int): Number of data samples.\n",
    "\n",
    "Returns:\n",
    "- dict: A metadata dictionary or an empty dict if request fails.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee5cf89-7c47-4601-b57c-04415d5966c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "DB_API = \"http://localhost/api/database/{db_id}\"\n",
    "HISTORY_API = \"http://localhost/api/database/{db_id}/table/{table_id}/history\"\n",
    "\n",
    "def fetch_db_dataset_metadata(\n",
    "    db_id: str,\n",
    "    table_id: str,\n",
    "    selected_version: str,\n",
    "    target_variable: str,\n",
    "    num_samples: int\n",
    ") -> dict:\n",
    "    try:\n",
    "        # Fetch main DB metadata\n",
    "        db_url = DB_API.format(db_id=db_id)\n",
    "        db_response = requests.get(db_url)\n",
    "        db_response.raise_for_status()\n",
    "        db_data = db_response.json()\n",
    "        print(db_data)\n",
    "\n",
    "        # Fetch table history metadata\n",
    "        history_url = HISTORY_API.format(db_id=db_id, table_id=table_id)\n",
    "        history_response = requests.get(history_url)\n",
    "        timestamp = \"info not available\"\n",
    "        if history_response.status_code == 200:\n",
    "            history_data = history_response.json()\n",
    "            print(history_data)\n",
    "            if isinstance(history_data, list) and len(history_data) > 0:\n",
    "                timestamp = history_data[0].get(\"timestamp\", timestamp)\n",
    "\n",
    "        # Build flat metadata structure for DB storage\n",
    "        dataset_metadata = {\n",
    "            # Basic identity\n",
    "            \"dataset_id\": table_id,\n",
    "            \"dataset_name\": next(\n",
    "                (t.get(\"name\") for t in db_data.get(\"tables\", []) if t.get(\"id\") == table_id),\n",
    "                \"table name not available\"\n",
    "            ),\n",
    "            \"dataset_version\": selected_version,\n",
    "            \"dataset_title\": db_data.get(\"name\", \"info not available\"),\n",
    "            \"dataset_description\": db_data.get(\"description\", \"info not available\"),\n",
    "\n",
    "            # Ownership and access\n",
    "            \"dataset_creator\": \"info not available\",\n",
    "            \"dataset_publisher\": db_data.get(\"owner\", {}).get(\"name\", \"info not available\"),\n",
    "            \"dataset_access_url\": db_url,\n",
    "            \"dataset_publication_date\": timestamp,\n",
    "            \"dataset_license\": \"info not available\",\n",
    "\n",
    "            # Structure\n",
    "            \"columns\": db_data.get(\"columns\", \"info not available\"),\n",
    "            \"dataset_dataset_type\": \"tabular\",\n",
    "            \"target_variable\": target_variable,\n",
    "            \"ml_task\": \"classification\",\n",
    "            \"num_samples\": num_samples,\n",
    "\n",
    "            # FAIR4ML placeholders\n",
    "            \"data_distribution\": \"info not available\",\n",
    "            \"known_issues\": \"info not available\",\n",
    "            \"trainedOn\": \"info not available\",\n",
    "            \"testedOn\": \"info not available\",\n",
    "            \"validatedOn\": \"info not available\",\n",
    "            \"modelRisks\": \"info not available\",\n",
    "            \"usageInstructions\": \"info not available\",\n",
    "            \"ethicalLegalSocial\": \"info not available\",\n",
    "\n",
    "            # PROV-style fields\n",
    "            \"prov_entity\": db_data.get(\"name\", \"info not available\"),\n",
    "            \"prov_activity\": \"Ingestion and Publication\",\n",
    "            \"prov_agent_dataset_creator\": \"info not available\",\n",
    "            \"prov_agent_database_creator\": db_data.get('owner', {}).get('name', 'info not available'),\n",
    "            \"prov_wasGeneratedBy\": db_data.get('owner', {}).get('name', 'info not available'),\n",
    "            \"prov_used\": db_url,\n",
    "            \"prov_wasDerivedFrom\": \"info not available\",\n",
    "            \"prov_wasAttributedTo\": \"info not available\",\n",
    "            \"prov_wasAssociatedWith\": db_data.get('owner', {}).get('name', 'info not available'),\n",
    "            \"prov_startedAtTime\": \"info not available\",\n",
    "            \"prov_endedAtTime\": timestamp,\n",
    "            \"prov_location\": db_url,\n",
    "            \"prov_role_dataset_creator\": \"\",\n",
    "            \"prov_role_database_creator\": \"Database Ingestor and Maintainer\"\n",
    "        }\n",
    "\n",
    "        return dataset_metadata\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[âš ï¸ Error] Failed to fetch DB metadata for {db_id}: {e}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b61178-0a4a-48d5-8b6f-737104605005",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Interactive version selector for datasets stored in a version-controlled database.\n",
    "\n",
    "This script:\n",
    "- Maintains a mapping between dataset version tags (e.g., \"v0\", \"v1\") and actual table UUIDs.\n",
    "- Prompts the user to select a version from a list of labeled options.\n",
    "- Validates the input and returns the selected version along with the corresponding table UUID.\n",
    "- Useful for ML or data analysis workflows where multiple versions of the same dataset exist.\n",
    "\n",
    "Returns:\n",
    "- selected_version (str): Version tag (e.g., \"v2\")\n",
    "- selected_table_id (str): UUID of the selected table version\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0611d-d579-485a-ac24-094c1890bc2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mapping of version tags to table UUIDs\n",
    "version_to_table_id = {\n",
    "    \"v0\": \"e9914484-a8b1-4bbc-b053-5f0ecba5a78b\",  # Original\n",
    "    \"v1\": \"76b3d980-e160-4150-9195-a61b2157352f\",  # Duplicated\n",
    "    \"v2\": \"240e4c71-b688-4644-8969-526d6cc5ef09\",  # First 100\n",
    "    \"v3\": \"ab667e6c-12a6-4052-9448-6f2093de0655\",  # Shuffled\n",
    "    \"v4\": \"95cf2caf-3f29-44bb-901d-cb2c1ea4bbe6\" ,  # Normalized\n",
    "    \"v5\": \"18b87733-a87f-4059-930b-6854fdba81b0\"   # Random\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "db_id = \"e95258f1-22c5-413f-9bf7-b0f2f6e947e8\"  # Static DB ID\n",
    "\n",
    "def select_dataset_version():\n",
    "    print(\"Select dataset version:\")\n",
    "    print(\"  v0 - Original\")\n",
    "    print(\"  v1 - Duplicated\")\n",
    "    print(\"  v2 - First 100\")\n",
    "    print(\"  v3 - Shuffled\")\n",
    "    print(\"  v4 - Normalized\")\n",
    "    print(\"  v5 - Random\")\n",
    "    \n",
    "    selected_version = input(\"Enter version (v0â€“v5): \").strip().lower()\n",
    "    \n",
    "    if selected_version not in version_to_table_id:\n",
    "        raise ValueError(f\"âŒ Invalid version selected: {selected_version}\")\n",
    "    \n",
    "    selected_table_id = version_to_table_id[selected_version]\n",
    "    \n",
    "    print(f\"\\nâœ… You selected version '{selected_version}' â†’ Table ID: {selected_table_id}\\n\")\n",
    "    \n",
    "    return selected_version, selected_table_id\n",
    "\n",
    "# Usage: #TODO CALL\n",
    "selected_version, selected_table_id = select_dataset_version()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78280d38-b041-4c22-8adf-5fd9c5934960",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Logs structured metadata to an active MLflow run.\n",
    "\n",
    "This utility function:\n",
    "- Logs each key-value pair in a flat metadata dictionary as MLflow tags.\n",
    "- Optionally prefixes each key (e.g., \"session_\", \"prov_\").\n",
    "- Skips empty or None values.\n",
    "- Truncates long values and handles dicts/lists by serializing to JSON.\n",
    "- Also saves the entire metadata dictionary as a JSON file and logs it as an MLflow artifact.\n",
    "\n",
    "This function is useful for reproducibility, audit trails, experiment documentation, \n",
    "and standard-compliant ML metadata logging.\n",
    "\n",
    "Args:\n",
    "    metadata (dict): Flat dictionary of metadata fields.\n",
    "    prefix (str): Optional prefix to prepend to each tag key.\n",
    "    snapshot_name (str): Filename for saved full JSON snapshot artifact.\n",
    "\n",
    "Raises:\n",
    "    RuntimeError: If no MLflow run is active.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab28b04-4db7-43bf-8320-f6382120984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import mlflow\n",
    "# \n",
    "def log_metadata_dict_to_mlflow(metadata: dict, prefix: str = \"\", snapshot_name: str = \"metadata_snapshot.json\"):\n",
    "    \"\"\"\n",
    "    Logs a flat metadata dictionary to MLflow:\n",
    "    - Adds prefix to each key if provided (e.g., \"session_\")\n",
    "    - Skips empty values\n",
    "    - Logs a full JSON artifact for traceability\n",
    "    \"\"\"\n",
    "    \n",
    "    def safe_tag(key, value):\n",
    "        if not mlflow.active_run():\n",
    "            raise RuntimeError(\"âŒ No active MLflow run.\")\n",
    "        \n",
    "        key_clean = key.replace(\":\", \"_\").replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "        try:\n",
    "            val_str = json.dumps(value) if isinstance(value, (dict, list)) else str(value)\n",
    "            if len(val_str) > 5000:\n",
    "                val_str = val_str[:5000] + \"...[TRUNCATED]\"\n",
    "            if len(key_clean) > 255:\n",
    "                print(f\"âš ï¸ Skipped tag (key too long): {key_clean}\")\n",
    "                return\n",
    "            mlflow.set_tag(key_clean, val_str)\n",
    "            print(f\"âœ… Logged tag: {key_clean}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[âš ï¸ Error logging tag] {key_clean}: {e}\")\n",
    "\n",
    "    for key, value in metadata.items():\n",
    "        if value not in [None, \"\"]:\n",
    "            full_key = f\"{prefix}{key}\" if prefix else key\n",
    "            safe_tag(full_key, value)\n",
    "\n",
    "    # Save full metadata snapshot as JSON artifact\n",
    "    os.makedirs(\"metadata\", exist_ok=True)\n",
    "    full_path = os.path.join(\"metadata\", snapshot_name)\n",
    "    with open(full_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    mlflow.log_artifact(full_path, artifact_path=\"metadata\")\n",
    "    print(f\"ğŸ“ Full metadata snapshot logged as: {snapshot_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4d6b8-34a9-47b5-974d-5927c0ee2256",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Fetches dataset content from a specific versioned table using the local API.\n",
    "\n",
    "This script:\n",
    "- Constructs a GET request to retrieve data from a specified database table.\n",
    "- Sends the request with appropriate headers.\n",
    "- Parses and prints the response JSON if successful.\n",
    "- Handles and reports errors gracefully.\n",
    "\n",
    "Useful for loading versioned datasets (e.g., for analysis or ML training)\n",
    "from a metadata-aware data management backend.\n",
    "\n",
    "Assumes:\n",
    "- `db_id` and `selected_table_id` are already defined.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3570e2-9a60-45b4-8653-28060071e728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# API endpoint URL\n",
    "API_URL = f\"http://localhost/api/database/{db_id}/table/{selected_table_id}/data?size=100000&page=0\"\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\"  # Specify the expected response format\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the API with the Accept header\n",
    "    response = requests.get(API_URL, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        dataset = response.json()\n",
    "        \n",
    "        \n",
    "        print( dataset)\n",
    "    else:\n",
    "        print(f\"Error: Received status code {response.status_code}\")\n",
    "        print(\"Response content:\", response.text)\n",
    "       \n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09557f94-325c-4bd6-882a-069a9e3c5ecd",
   "metadata": {},
   "source": [
    "replacing dynamic fetching of data When and if DBREPO isnt running (BACKUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6e020d-cb80-49ec-8bcc-687b1e08885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Read the JSON file id the API isnt available this data is saved locally but the data is from the API endpoint\n",
    "# with open(\"iris_data.json\", \"r\") as f:\n",
    "#     dataset = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcf2244-14dd-4e3d-b8cf-f7f3ba34f80f",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# ğŸ“‚ Setup MLflow\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae32d719-31fc-4ac4-856d-4430a51cd957",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Initializes MLflow tracking with a local SQLite backend.\n",
    "\n",
    "This script:\n",
    "- Ensures a local directory (`mlrunlogs/`) exists for MLflow logs and database.\n",
    "- Sets MLflow's tracking URI to a local SQLite database (`mlflow.db`).\n",
    "- Prompts the user to enter an experiment name; falls back to 'default_experiment' if none is given.\n",
    "- Registers or activates the specified experiment.\n",
    "\n",
    "Useful for local ML experimentation, reproducibility, and metadata tracking\n",
    "without needing a remote MLflow server.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe91ec0-6447-4586-b7cc-2c1f74d4218f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "# Ensure tracking directory exists\n",
    "project_dir = os.getcwd()\n",
    "mlrunlogs_dir = os.path.join(project_dir, \"mlrunlogs\")\n",
    "os.makedirs(mlrunlogs_dir, exist_ok=True)\n",
    "\n",
    "# Set MLflow tracking URI (local SQLite backend)\n",
    "mlflow_tracking_path = os.path.join(mlrunlogs_dir, \"mlflow.db\")\n",
    "mlflow.set_tracking_uri(\"mlrunlogs/mlflow.db\")\n",
    "\n",
    "# Prompt for experiment name\n",
    "experiment_name = input(\"Enter experiment name for MLflow: \").strip()\n",
    "if not experiment_name:\n",
    "    experiment_name = \"default_experiment\"\n",
    "    print(\"âš ï¸ No name entered. Using fallback:\", experiment_name)\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2c2c5f-cc36-41a3-9643-83ef95b9f55e",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# ğŸ”„ Git Commit Hash for previous commit for metadata\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e125f-3104-476a-8157-af335c8d7d84",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Extracts metadata from the latest Git commit in a specified local repository.\n",
    "\n",
    "This function:\n",
    "- Connects to a Git repository at the provided path.\n",
    "- Retrieves metadata from the latest commit, including commit hash, author, time, message, and branch.\n",
    "- Handles detached HEAD states (e.g., in CI/CD or temp checkouts).\n",
    "- Returns placeholder values if the repository is inaccessible or invalid.\n",
    "\n",
    "Useful for:\n",
    "- Logging code version info into ML experiment tracking (e.g., MLflow).\n",
    "- Reproducibility audits.\n",
    "- Attaching Git context to research or development metadata.\n",
    "\n",
    "Returns:\n",
    "    dict: Git metadata fields, or 'not available' if any error occurs.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838dd233-25dc-4725-974d-4da89c257782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import os\n",
    "\n",
    "def get_latest_git_commit(repo_path: str = \"C:/Users/reema/REPO\") -> dict:\n",
    "    \"\"\"\n",
    "    Returns the latest Git commit metadata from the given repo path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        repo = git.Repo(repo_path)\n",
    "        commit = repo.head.commit\n",
    "        commit_metadata = {\n",
    "            \"git_commit\": commit.hexsha,\n",
    "            \"git_author\": commit.author.name,\n",
    "            \"git_email\": commit.author.email,\n",
    "            \"git_commit_time\": str(commit.committed_datetime),\n",
    "            \"git_message\": commit.message.strip(),\n",
    "            \"git_branch\": repo.active_branch.name if not repo.head.is_detached else \"detached\"\n",
    "        }\n",
    "        return commit_metadata\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[âš ï¸ Git Error] Could not read Git repo at {repo_path}: {e}\")\n",
    "        return {\n",
    "            \"git_commit\": \"not available\",\n",
    "            \"git_author\": \"not available\",\n",
    "            \"git_email\": \"not available\",\n",
    "            \"git_commit_time\": \"not available\",\n",
    "            \"git_message\": \"not available\",\n",
    "            \"git_branch\": \"not available\"\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "repo_dir = \"C:/Users/reema/REPO\"\n",
    "git_metadata = get_latest_git_commit(repo_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430d15ef-3432-4e45-88fb-b7048a5b10a9",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# Make threadpoolctl safe so MLflowâ€™s autologger wonâ€™t crash â”€â”€â”€\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd08ac85-e5a2-4f8b-b4de-cf893d6c2a3e",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Patches `threadpoolctl` to avoid crashes with MLflow autologging,\n",
    "then enables MLflow's automatic logging for supported ML libraries \n",
    "(e.g., scikit-learn, XGBoost, LightGBM, TensorFlow, etc.).\n",
    "\n",
    "This is particularly useful in environments where `threadpoolctl.threadpool_info()` \n",
    "may raise runtime errors (e.g., inside Docker or limited-thread environments).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9668451f-4352-4bdc-8b6b-bbe49074212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Patch threadpoolctl if needed to avoid autolog crashes â”€â”€â”€\n",
    "try:\n",
    "    import threadpoolctl\n",
    "    _original_threadpool_info = threadpoolctl.threadpool_info\n",
    "\n",
    "    def _safe_threadpool_info(*args, **kwargs):\n",
    "        try:\n",
    "            return _original_threadpool_info(*args, **kwargs)\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    threadpoolctl.threadpool_info = _safe_threadpool_info\n",
    "except ImportError:\n",
    "    pass  # If threadpoolctl isn't installed, we just skip this patch\n",
    "\n",
    "# â”€â”€â”€ Enable MLflow autologging (generic, works with sklearn and more) â”€â”€â”€\n",
    "import mlflow\n",
    "\n",
    "mlflow.autolog(\n",
    "    log_input_examples=True,\n",
    "    log_model_signatures=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e532d5f6-003c-4897-be80-d668e85ff4e6",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Logs a comprehensive set of metadata to MLflow for an ML experiment run.\n",
    "\n",
    "This function:\n",
    "- Captures model metadata, evaluation metrics, hyperparameters, label encodings.\n",
    "- Logs preprocessing configuration and creates a unique hash for reproducibility.\n",
    "- Tracks the compute environment and Git commit for traceability.\n",
    "- Includes support for FAIR and MLSEA-aligned metadata standards.\n",
    "- Optionally adds previously logged justification tags (e.g., from interactive inputs).\n",
    "\n",
    "Useful for:\n",
    "- ML provenance tracking\n",
    "- Reproducibility and auditability of experiments\n",
    "- Visualizing metadata-rich runs in MLflow dashboards\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b608670-96a5-42b0-b69b-263ac1e452eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import platform\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from subprocess import check_output, CalledProcessError\n",
    "\n",
    "def log_standard_metadata(\n",
    "    model_name: str,\n",
    "    model,\n",
    "    hyperparams: dict,\n",
    "    acc: float,\n",
    "    prec: float,\n",
    "    rec: float,\n",
    "    f1: float,\n",
    "    auc: float,\n",
    "    label_map: dict,\n",
    "    run_id: str,\n",
    "    test_size: float,\n",
    "    random_state: int,\n",
    "    id_cols: list,\n",
    "    target_col: str,\n",
    "    X,\n",
    "    y,\n",
    "    run_data=None\n",
    "):\n",
    "    # === Experiment Metadata ===\n",
    "    mlflow.set_tag(\"run_id\", run_id)  # [MLflow / DB anchor]\n",
    "    mlflow.set_tag(\"model_name\", model_name)  # [ML Metadata, FAIR]\n",
    "    mlflow.set_tag(\"model_architecture\", model.__class__.__name__)  # [MLSEA]\n",
    "    mlflow.set_tag(\"test_size\", test_size)  # [MLSEA, Reproducibility]\n",
    "    mlflow.set_tag(\"random_state\", random_state)  # [MLSEA, Reproducibility]\n",
    "\n",
    "    # === Evaluation Metrics ===\n",
    "    mlflow.set_tag(\"accuracy\", acc)\n",
    "    mlflow.set_tag(\"precision_macro\", prec)\n",
    "    mlflow.set_tag(\"recall_macro\", rec)\n",
    "    mlflow.set_tag(\"f1_macro\", f1)\n",
    "    mlflow.set_tag(\"roc_auc\", auc)\n",
    "\n",
    "    # === Hyperparameters and Label Encoding ===\n",
    "    mlflow.set_tag(\"hyperparameters\", json.dumps(hyperparams))  # [FAIR, MLSEA]\n",
    "    mlflow.set_tag(\"label_map\", json.dumps(label_map))  # [ML Preprocessing]\n",
    "\n",
    "    # === Preprocessing Snapshot ===\n",
    "    preprocessing_info = {\n",
    "        \"dropped_columns\": id_cols,\n",
    "        \"numeric_columns\": list(X.columns),\n",
    "        \"target_column\": target_col,\n",
    "        \"stratified\": False,\n",
    "        \"coercion_strategy\": \"Numeric cast (auto)\",\n",
    "        \"feature_engineering\": \"None\",\n",
    "        \"missing_value_strategy\": \"None\",\n",
    "        \"outlier_detection\": \"None\",\n",
    "        \"encoding_strategy\": \"LabelEncoder (target only)\",\n",
    "        \"scaling\": \"None\",\n",
    "        \"sampling\": \"None\",\n",
    "        \"feature_selection\": \"None\",\n",
    "        \"train_test_split\": {\"test_size\": test_size, \"random_state\": random_state},\n",
    "        \"imbalance_ratio\": str(dict(zip(*np.unique(y, return_counts=True)))),\n",
    "        \"preprocessing_timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    preprocessing_hash = hashlib.sha256(json.dumps(preprocessing_info).encode()).hexdigest()\n",
    "    mlflow.set_tag(\"preprocessing_info\", json.dumps(preprocessing_info))  # [MLSEA]\n",
    "    mlflow.set_tag(\"preprocessing_hash\", preprocessing_hash)\n",
    "\n",
    "    # === Reproducibility ===\n",
    "    mlflow.set_tag(\"model_serialization\", \"pickle\")  # [FAIR, MLSEA]\n",
    "    mlflow.set_tag(\"model_path\", f\"{model_name}.pkl\")\n",
    "\n",
    "    try:\n",
    "        sha = check_output([\"git\", \"rev-parse\", \"HEAD\"], text=True).strip()\n",
    "    except CalledProcessError:\n",
    "        sha = \"unknown\"\n",
    "    mlflow.set_tag(\"git_commit\", sha)\n",
    "\n",
    "    # === Compute Environment ===\n",
    "    compute_env = {\n",
    "        \"os\": f\"{platform.system()} {platform.release()}\",\n",
    "        \"cpu\": platform.processor(),\n",
    "        \"ram_gb\": round(psutil.virtual_memory().total / (1024 ** 3), 2),\n",
    "        \"python_version\": platform.python_version(),\n",
    "        \"sklearn_version\": sklearn.__version__,\n",
    "        \"pandas_version\": pd.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "    }\n",
    "    mlflow.set_tag(\"compute_environment\", json.dumps(compute_env))  # [Reproducibility]\n",
    "\n",
    "    # === Optional: Tag MLflow Justifications (previously logged manually) ===\n",
    "    if run_data:\n",
    "        for key, val in run_data.tags.items():\n",
    "            if key.startswith(\"justification_\"):\n",
    "                mlflow.set_tag(key, val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcddf313-6f86-443a-be51-f8512b943c9e",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Generates a comprehensive reproducibility text log for a machine learning model run.\n",
    "\n",
    "This function:\n",
    "- Compiles metadata including model/dataset identifiers, hyperparameters, evaluation metrics, \n",
    "  Git commit hash, and a structured reproduction guide.\n",
    "- Optionally appends a system architecture description if the file is available.\n",
    "- Saves the log in a YAML-formatted `.txt` file under the `MODEL_PROVENANCE/` directory,\n",
    "  nested by model name.\n",
    "\n",
    "Intended use:\n",
    "- Auditable experiment tracking\n",
    "- Research paper supplementary material\n",
    "- Automated logging in ML pipelines\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f000f9-d0b6-41f7-92d3-4b605e4ecaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_reproducibility_txt_log(\n",
    "    model_name: str,\n",
    "    dataset_name: str,\n",
    "    dataset_version: str,\n",
    "    hyperparams: dict,\n",
    "    metrics: dict,\n",
    "    git_commit: str,\n",
    "    run_id: str,\n",
    "    architecture_file_path: str = \"provenance_architecture_description.txt\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a reproducibility log (YAML + architecture) and return the saved path.\n",
    "    This log combines:\n",
    "    - Model and dataset details\n",
    "    - Hyperparameters and evaluation metrics\n",
    "    - Git provenance info\n",
    "    - Reproduction steps\n",
    "    - Provenance architecture description\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_values(d):\n",
    "        \"\"\"Convert numpy floats to native floats.\"\"\"\n",
    "        return {k: float(v) if isinstance(v, (np.float32, np.float64)) else v for k, v in d.items()}\n",
    "\n",
    "    timestamp = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M UTC\")\n",
    "\n",
    "    repro_data = {\n",
    "        \"ğŸ“Œ Model Details\": {\n",
    "            \"Model Name\": model_name,\n",
    "            \"Dataset Name\": dataset_name,\n",
    "            \"Dataset Version\": dataset_version,\n",
    "            \"Run ID\": run_id,\n",
    "            \"Timestamp\": timestamp\n",
    "        },\n",
    "        \"ğŸ› ï¸ Hyperparameters\": clean_values(hyperparams),\n",
    "        \"ğŸ“ˆ Metrics\": clean_values(metrics),\n",
    "        \"ğŸ”— Git Info\": {\n",
    "            \"Commit Hash\": git_commit,\n",
    "            \"Reproduce With\": f\"git checkout {git_commit}\"\n",
    "        },\n",
    "        \"ğŸš€ Reproduction Guide\": [\n",
    "            \"1. Clone the repo and checkout the commit:\",\n",
    "            f\"   git checkout {git_commit}\",\n",
    "            \"2. Load and preprocess the dataset exactly as during training.\",\n",
    "            \"3. Load the model using MLflow:\",\n",
    "            f\"   mlflow.sklearn.load_model('runs:/{run_id}/model')\",\n",
    "            \"4. Run inference or evaluation using the same pipeline/script.\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # ğŸ” Create and write to output file\n",
    "    save_dir = os.path.join(\"MODEL_PROVENANCE\", model_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    txt_path = os.path.join(save_dir, f\"{model_name}_reproducibility.txt\")\n",
    "\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as repro_file:\n",
    "        yaml.dump(repro_data, repro_file, allow_unicode=True, sort_keys=False, width=100)\n",
    "        repro_file.write(\"\\n\\n\")\n",
    "\n",
    "        if os.path.exists(architecture_file_path):\n",
    "            with open(architecture_file_path, \"r\", encoding=\"utf-8\") as arch_file:\n",
    "                architecture_description = arch_file.read()\n",
    "                repro_file.write(architecture_description)\n",
    "        else:\n",
    "            repro_file.write(\"[âš ï¸ Missing architecture description file]\\n\")\n",
    "\n",
    "    return txt_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2667134f-e84d-425d-97cf-936344f8a115",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Utility functions for collecting human-in-the-loop justifications during ML experimentation.\n",
    "\n",
    "These functions:\n",
    "- Allow logging of parameter values or decisions via MLflow.\n",
    "- Prompt the user to provide justifications for those values interactively.\n",
    "- Store justifications as MLflow tags prefixed with `justification_`.\n",
    "\n",
    "This supports transparency, documentation, and accountability in model development,\n",
    "especially in research or regulated environments.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08a1f1-c3a6-45bd-97b1-92e2fade9ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_with_justification(log_func, key: str, value, context: str = \"\"):\n",
    "    \"\"\"\n",
    "    Log a value using the specified MLflow log function (e.g., mlflow.log_param),\n",
    "    then prompt the user for a justification and log it as a tag.\n",
    "    \"\"\"\n",
    "    log_func(key, value)\n",
    "    print(f\"\\nğŸ“ Justification for `{key}` ({context})\")\n",
    "    user_reason = input(\"â†’ Why did you choose this value? \")\n",
    "    mlflow.set_tag(f\"justification_{key}\", user_reason or \"No justification provided\")\n",
    "\n",
    "def log_justification(key: str, question: str):\n",
    "    \"\"\"\n",
    "    Prompt for a justification only (without logging a value), and log it as a tag.\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“ Justification for `{key}`\")\n",
    "    user_reason = input(f\"â†’ {question} \")\n",
    "    mlflow.set_tag(f\"justification_{key}\", user_reason or \"No justification provided\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00237086-0d9c-41b2-a780-b2322ecd69fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9058319a-adba-4a6b-93e9-d17080c0594d",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# ğŸš€ Start MLflow Run \n",
    "# ============================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef8cdd-ca7f-4c97-8ee7-8cdfb3dd67c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c62f08-a116-4060-9689-f69968e9f240",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    client = MlflowClient()\n",
    "    run_data = client.get_run(run.info.run_id).data\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Session Metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    session_metadata = collect_session_metadata(prompt_fields=True)\n",
    "    mlflow.log_params(session_metadata)  # [PROV, Internal] Session and environment context\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Dataset Metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    doi_metadata = extract_dataset_metadata_from_doi(\"10.24432/C56C76\")  # [FAIR, PROV, FAIR4ML]\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Experiment Start Time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    start_time = datetime.now().isoformat()\n",
    "    mlflow.set_tag(\"startedAtTime\", start_time)  # [PROV] Activity start time\n",
    "\n",
    "    #######################################################################\n",
    "    ### Preprocessing #####################################################\n",
    "\n",
    "    # â”€â”€ Load into a DataFrame â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    df = pd.DataFrame(dataset)\n",
    "    original_row_count = df.shape[0]\n",
    "    mlflow.log_param(\"input_row_count\", original_row_count)  # [MLSEA] Input data size\n",
    "\n",
    "    # Log column names before transformation\n",
    "    mlflow.set_tag(\"raw_columns\", ','.join(df.columns))  # [FAIR4ML, Internal]\n",
    "\n",
    "    # â”€â”€ Generate row hashes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    before_hashes = set(df.astype(str).apply(lambda row: hash(tuple(row)), axis=1))\n",
    "    mlflow.set_tag(\"row_hash_tracking\", \"enabled\")  # [Internal] Used for provenance/repeatability\n",
    "\n",
    "    # â”€â”€ Extract target variable â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    target_col = df.columns[-1]\n",
    "    mlflow.set_tag(\"target_variable\", target_col)  # [FAIR4ML, MLSEA]\n",
    "\n",
    "    # â”€â”€ Separate features and labels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    y = df[target_col]\n",
    "    X = df.drop(columns=[target_col])\n",
    "    mlflow.set_tag(\"feature_columns\", ','.join(X.columns))  # [FAIR4ML, MLSEA]\n",
    "\n",
    "    # â”€â”€ Drop ID columns (case-insensitive) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    id_cols = [c for c in X.columns if c.lower() == \"id\"]\n",
    "    if id_cols:\n",
    "        X = X.drop(columns=id_cols)\n",
    "        mlflow.set_tag(\"dropped_id_columns\", ','.join(id_cols))  # [Internal]\n",
    "\n",
    "    # â”€â”€ Convert columns to numeric where possible â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    numeric_conversion_count = 0\n",
    "    for c in X.columns:\n",
    "        try:\n",
    "            X[c] = pd.to_numeric(X[c])\n",
    "            numeric_conversion_count += 1\n",
    "        except Exception:\n",
    "            continue\n",
    "    mlflow.log_param(\"numeric_columns_converted\", numeric_conversion_count)  # [Internal, FAIR4ML]\n",
    "\n",
    "    # â”€â”€ Print diagnostic info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"ML_EXP_Shapes:\", X.shape, y.shape)\n",
    "    mlflow.log_param(\"feature_matrix_shape\", str(X.shape))  # [MLSEA]\n",
    "    mlflow.log_param(\"label_vector_shape\", str(y.shape))    # [MLSEA]\n",
    "#######################################################################################################\n",
    "### 8) Label Encoding and Metadata Logging ############################################################\n",
    "\n",
    "# â”€â”€ Encode class labels numerically â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    print(\"ML_EXP_Classes:\", le.classes_)\n",
    "    \n",
    "    mlflow.set_tag(\"class_names\", ','.join(le.classes_))  # [FAIR4ML, MLSEA]\n",
    "    \n",
    "    # â”€â”€ Count rows and hash comparison before vs after preprocessing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    count_end = df.shape[0]\n",
    "    after_hashes = set(df.astype(str).apply(lambda row: hash(tuple(row)), axis=1))\n",
    "    \n",
    "    n_insert = len(after_hashes - before_hashes)\n",
    "    n_delete = len(before_hashes - after_hashes)\n",
    "    \n",
    "    #######################################################################################################\n",
    "    ### Metadata Logging (Standardized Format) ############################################################\n",
    "    \n",
    "    # â”€â”€ Extended DB Metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    db_meta = fetch_db_dataset_metadata(db_id, selected_table_id, selected_version, target_col, df.shape[0])  # [Internal]\n",
    "    \n",
    "    mlflow.set_tag(\"Internal_DBRepo_table_last_modified\", db_meta.get(\"dataset_publication_date\", \"unknown\"))\n",
    "  # [PROV]\n",
    "    \n",
    "    # â”€â”€ Row Count Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    mlflow.log_metric(\"row_count_start\", original_row_count)              # [MLSEA, FAIR4ML]\n",
    "    mlflow.log_metric(\"row_count_end\", count_end)                  # [MLSEA, FAIR4ML]\n",
    "    mlflow.log_metric(\"num_inserted_rows\", n_insert)               # [PROV]\n",
    "    mlflow.log_metric(\"num_deleted_rows\", n_delete)                # [PROV]\n",
    "    \n",
    "    # â”€â”€ Raw Data Source Metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    mlflow.set_tag(\"data_source\", API_URL)                         # [FAIR]\n",
    "    mlflow.log_param(\"retrieval_time_utc\", datetime.utcnow().isoformat())  # [PROV]\n",
    "    mlflow.log_param(\"raw_row_count\", len(df))                     # [MLSEA]\n",
    "    mlflow.log_param(\"raw_columns\", df.columns.tolist())           # [FAIR4ML]\n",
    "    mlflow.log_param(\"dropped_columns\", id_cols)                   # [Internal]\n",
    "    \n",
    "    # â”€â”€ Post-Processing Metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    mlflow.log_param(\"final_num_features\", X.shape[1])             # [MLSEA]\n",
    "    mlflow.log_param(\"final_feature_names\", X.columns.tolist())    # [FAIR4ML]\n",
    "    mlflow.set_tag(\"target_variable_encoded\", target_col)          # [FAIR4ML]\n",
    "    \n",
    "    # â”€â”€ Label Mapping as Artifact â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    label_map = {int(idx): cls for idx, cls in enumerate(le.classes_)}\n",
    "    buffer = io.StringIO()\n",
    "    json.dump(label_map, buffer, indent=2)\n",
    "    buffer.seek(0)\n",
    "    mlflow.log_text(buffer.getvalue(), artifact_file=\"label_mapping.json\")  # [FAIR4ML]\n",
    "    \n",
    "    # â”€â”€ Training Metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_name = f\"RandomForest_Iris_v{ts}\"\n",
    "    mlflow.set_tag(\"model_name\", model_name)                       # [MLSEA]\n",
    "    \n",
    "    train_start_ts = datetime.now().isoformat()\n",
    "    mlflow.set_tag(\"training_start_time\", train_start_ts)          # [PROV]\n",
    "########################################################################################################\n",
    "### Model Parameters & Split Metadata ##################################################################\n",
    "\n",
    "# â”€â”€ Prompt test size and seed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    try:\n",
    "        test_size = float(input(\"Enter test size (e.g., 0.2 for 20% test set): \"))\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Defaulting to 0.2\")\n",
    "        test_size = 0.2\n",
    "    \n",
    "    try:\n",
    "        random_state = int(input(\"Enter random seed (e.g., 42): \"))\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Defaulting to 42\")\n",
    "        random_state = 42\n",
    "    \n",
    "    # â”€â”€ Train/test split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # â”€â”€ Log split config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    mlflow.log_param(\"test_size\", test_size)                     # [MLSEA]\n",
    "    mlflow.log_param(\"random_seed\", random_state)               # [PROV]\n",
    "    mlflow.log_param(\"n_train_samples\", X_train.shape[0])       # [FAIR4ML]\n",
    "    mlflow.log_param(\"n_test_samples\",  X_test.shape[0])        # [FAIR4ML]\n",
    "    mlflow.log_param(\"n_features\",      X_train.shape[1])       # [MLSEA]\n",
    "    \n",
    "    ########################################################################################################\n",
    "    ### Model Selection & Hyperparameters ##################################################################\n",
    "    \n",
    "    # â”€â”€ Define hyperparameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    ML_EXP_hyperparams = {\n",
    "        \"n_estimators\":       100,\n",
    "        \"criterion\":          \"entropy\",\n",
    "        \"max_depth\":          10,\n",
    "        \"min_samples_split\":  3,\n",
    "        \"min_samples_leaf\":   1,\n",
    "        \"max_features\":       \"sqrt\",\n",
    "        \"bootstrap\":          True,\n",
    "        \"oob_score\":          True,\n",
    "        \"class_weight\":       None,\n",
    "        \"verbose\":            1,\n",
    "        \"n_jobs\":             -1\n",
    "    }\n",
    "    \n",
    "    # â”€â”€ Model selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    available_models = {\n",
    "        \"random_forest\": RandomForestClassifier,\n",
    "        \"decision_tree\": DecisionTreeClassifier,\n",
    "        \"logistic_regression\": LogisticRegression,\n",
    "        \"knn\": KNeighborsClassifier,\n",
    "        \"svm\": SVC,\n",
    "        \"gradient_boosting\": GradientBoostingClassifier\n",
    "    }\n",
    "    \n",
    "    # User prompt\n",
    "    print(\"Choose a model to train:\")\n",
    "    for i, name in enumerate(available_models.keys()):\n",
    "        print(f\"{i + 1}. {name}\")\n",
    "    \n",
    "    choice = input(\"Enter model number (default 1 for random_forest): \").strip()\n",
    "    choice = int(choice) if choice else 1\n",
    "    selected_key = list(available_models.keys())[choice - 1]\n",
    "    selected_model_class = available_models[selected_key]\n",
    "    mlflow.set_tag(\"selected_model\", selected_key)  # [FAIR4ML, MLSEA]\n",
    "    \n",
    "    # â”€â”€ Initialize model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    model = selected_model_class(**ML_EXP_hyperparams)\n",
    "    \n",
    "    # â”€â”€ Log hyperparameters with justification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    for key, val in ML_EXP_hyperparams.items():\n",
    "        log_with_justification(mlflow.log_param, key, val, context=\"Hyperparameter configuration\")  # [FAIR4ML, MLSEA]\n",
    "    \n",
    "    ########################################################################################################\n",
    "    ### Model Training & Evaluation ########################################################################\n",
    "    \n",
    "    # â”€â”€ Fit the model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    model.fit(X_train, y_train)\n",
    "    train_end_ts = datetime.now().isoformat()\n",
    "    mlflow.set_tag(\"training_end_time\", train_end_ts)  # [PROV]\n",
    "    \n",
    "    # â”€â”€ Predictions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # â”€â”€ Compute and log metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    acc  = accuracy_score(y_test, y_pred)\n",
    "    auc  = roc_auc_score(y_test, y_proba, multi_class=\"ovr\")\n",
    "    prec = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    rec  = recall_score(y_test,  y_pred, average=\"macro\")\n",
    "    f1   = f1_score(y_test,      y_pred, average=\"macro\")\n",
    "    \n",
    "    mlflow.log_metric(\"accuracy\", acc)              # [MLSEA]\n",
    "    mlflow.log_metric(\"roc_auc\", auc)               # [MLSEA]\n",
    "    mlflow.log_metric(\"precision_macro\", prec)      # [MLSEA]\n",
    "    mlflow.log_metric(\"recall_macro\", rec)          # [MLSEA]\n",
    "    mlflow.log_metric(\"f1_macro\", f1)               # [MLSEA]\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "### Final Logging: Justifications, Metrics, Environment, Dataset Metadata #############################\n",
    "\n",
    "# â”€â”€ Prompt for and log justifications â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    log_justification(\"model_choice\", \"Why did you choose this model (e.g., RandomForestClassifier) for this task?\")\n",
    "    log_justification(\"target_variable\", \"Why did you choose this column as the prediction target?\")\n",
    "    log_justification(\"test_split\", \"Why this train/test ratio (e.g., 80/20)?\")\n",
    "    log_justification(\"metric_choice\", \"Why did you use accuracy/f1/ROC-AUC as your evaluation metric?\")\n",
    "    log_justification(\"threshold_accuracy\", \"Was there a threshold for accuracy? Why?\")\n",
    "    log_justification(\"dataset_version\", \"Why did you use this specific dataset version?\")\n",
    "    log_justification(\"drop_column_X\", \"Why did you drop any specific columns from the dataset?\")\n",
    "    log_justification(\"experiment_name\", \"Any context behind this experiment name or setup?\")\n",
    "    log_justification(\"model_limitations\", \"Any known model limitations?\")\n",
    "    log_justification(\"ethical_considerations\", \"Any known model ethical considerations?\")\n",
    "    log_justification(\"intended_use\", \"Known model intended use?\")\n",
    "    log_justification(\"not_intended_for\", \"Model not_intended_for?\")\n",
    "\n",
    "\n",
    "    # â”€â”€ Log model evaluation metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    mlflow.log_metric(\"precision_macro\", prec)    # [MLSEA]\n",
    "    mlflow.log_metric(\"recall_macro\", rec)        # [MLSEA]\n",
    "    mlflow.log_metric(\"f1_macro\", f1)             # [MLSEA]\n",
    "    mlflow.log_metric(\"accuracy\", acc)            # [MLSEA]\n",
    "    mlflow.log_metric(\"roc_auc\", auc)             # [MLSEA]\n",
    "    \n",
    "    # â”€â”€ Log environment info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    mlflow.log_params({\n",
    "        \"python_version\":       platform.python_version(),\n",
    "        \"os_platform\":          f\"{platform.system()} {platform.release()}\",\n",
    "        \"sklearn_version\":      sklearn.__version__,\n",
    "        \"pandas_version\":       pd.__version__,\n",
    "        \"numpy_version\":        np.__version__,\n",
    "        \"matplotlib_version\":   matplotlib.__version__,\n",
    "        \"seaborn_version\":      sns.__version__,\n",
    "        \"shap_version\":         shap.__version__,\n",
    "    })  # [PROV, Internal]\n",
    "    \n",
    "    # â”€â”€ Tag notebook name â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    mlflow.set_tag(\"notebook_name\", \"RQ1_2.ipynb\")  # [Internal]\n",
    "    \n",
    "    # â”€â”€ Dataset metadata tags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    mlflow.set_tag(\"dataset_name\",    db_meta.get(\"dataset_name\", \"unknown\") )    # [FAIR4ML, PROV]\n",
    "    mlflow.set_tag(\"dataset_version\", selected_version)                                           # [FAIR4ML, Internal]\n",
    "    mlflow.set_tag(\"dataset_id\",      selected_table_id)  # [FAIR4ML, Internal]\n",
    "\n",
    "########################################################################################################\n",
    "### Plots: Feature Importance, ROC, PR, Confusion Matrix, SHAP #########################################\n",
    "\n",
    "# â”€â”€ Create plot output directory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # plot_dir = os.path.join(\"ML_EXP_plots\", run.info.run_id) ##TODO test this path change\n",
    "\n",
    "    summary_dir = os.path.join(os.getcwd(), \"MODEL_PROVENANCE\", model_name)\n",
    "    os.makedirs(summary_dir, exist_ok=True)\n",
    "    \n",
    "    # os.makedirs(plot_dir, exist_ok=True)\n",
    "    plot_dir = summary_dir  # ğŸ‘ˆ Use the same directory as the summary\n",
    "\n",
    "    # â”€â”€ 1) Feature Importance Bar Chart â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = model.feature_importances_\n",
    "        feature_names = getattr(X_train, \"columns\", [f\"f{i}\" for i in range(X_train.shape[1])])\n",
    "        \n",
    "        fi_path = os.path.join(plot_dir, \"feature_importances.png\")\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.barplot(x=importances, y=feature_names)\n",
    "        plt.title(\"Feature Importances\")\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fi_path)\n",
    "        mlflow.log_artifact(fi_path)  # [MLSEA]\n",
    "        plt.close()\n",
    "    \n",
    "    # â”€â”€ 2) Multi-class ROC Curves â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    classes = np.unique(y_test)\n",
    "    y_test_bin = label_binarize(y_test, classes=classes)\n",
    "    \n",
    "    for idx, cls in enumerate(classes):\n",
    "        disp = RocCurveDisplay.from_predictions(y_test_bin[:, idx], y_proba[:, idx], name=f\"ROC for class {cls}\")\n",
    "        roc_path = os.path.join(plot_dir, f\"roc_curve_cls_{cls}.png\")\n",
    "        disp.figure_.savefig(roc_path)\n",
    "        mlflow.log_artifact(roc_path)  # [MLSEA]\n",
    "        plt.close(disp.figure_)\n",
    "    \n",
    "    # â”€â”€ 3) Multi-class Precision-Recall Curves â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    for idx, cls in enumerate(classes):\n",
    "        disp = PrecisionRecallDisplay.from_predictions(y_test_bin[:, idx], y_proba[:, idx], name=f\"PR curve for class {cls}\")\n",
    "        pr_path = os.path.join(plot_dir, f\"pr_curve_cls_{cls}.png\")\n",
    "        disp.figure_.savefig(pr_path)\n",
    "        mlflow.log_artifact(pr_path)  # [MLSEA]\n",
    "        plt.close(disp.figure_)\n",
    "    \n",
    "    # â”€â”€ 4) Confusion Matrix Plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    cm_path = os.path.join(plot_dir, \"confusion_matrix.png\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(cm_path)\n",
    "    mlflow.log_artifact(cm_path)  # [MLSEA]\n",
    "    plt.close()\n",
    "    \n",
    "    # â”€â”€ 5) SHAP Summary Plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    shap_path = os.path.join(plot_dir, \"shap_summary.png\")\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    \n",
    "    shap.summary_plot(shap_values, X_test, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(shap_path)\n",
    "    mlflow.log_artifact(shap_path)  # [FAIR4ML, MLSEA]\n",
    "    plt.close()\n",
    "    \n",
    "    ########################################################################################################\n",
    "    ### Final: Metadata Summary Logging ####################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    # log_standard_metadata(\n",
    "    #     model_name=model_name,\n",
    "    #     model=model,\n",
    "    #     hyperparams=ML_EXP_hyperparams,\n",
    "    #     acc=acc,\n",
    "    #     prec=prec,\n",
    "    #     rec=rec,\n",
    "    #     f1=f1,\n",
    "    #     auc=auc,\n",
    "    #     label_map=label_map,\n",
    "    #     run_id=run.info.run_id,\n",
    "    #     test_size=test_size,\n",
    "    #     random_state=random_state,\n",
    "    #     run_data=run_data\n",
    "    # )\n",
    "    log_standard_metadata(\n",
    "    model_name=model_name,\n",
    "    model=model,\n",
    "    hyperparams=ML_EXP_hyperparams,\n",
    "    acc=acc,\n",
    "    prec=prec,\n",
    "    rec=rec,\n",
    "    f1=f1,\n",
    "    auc=auc,\n",
    "    label_map=label_map,\n",
    "    run_id=run.info.run_id,\n",
    "    test_size=test_size,\n",
    "    random_state=random_state,\n",
    "    id_cols=id_cols,         # âœ… list of dropped ID columns\n",
    "    target_col=target_col,   # âœ… your target column, likely defined as df.columns[-1]\n",
    "    X=X,                     # âœ… your features DataFrame\n",
    "    y=y,                     # âœ… your labels array or Series\n",
    "    run_data=run_data        # optional but useful\n",
    "    )\n",
    "\n",
    "########################################################################################################\n",
    "### Export Model (.pkl) and Log as Artifact ############################################################\n",
    "\n",
    "# â”€â”€ Define output path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    from pathlib import Path\n",
    "\n",
    "    summary_dir = Path(summary_dir)  # Make sure it's a Path object\n",
    "\n",
    "    pkl_path = summary_dir / f\"{model_name}.pkl\"\n",
    "    # os.makedirs(\"Trained_models\", exist_ok=True)  # Ensure the folder exists\n",
    "    \n",
    "    # â”€â”€ Serialize the trained model to disk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    with open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    # â”€â”€ Log the serialized model to MLflow as an artifact â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    mlflow.log_artifact(pkl_path, artifact_path=model_name)  # [FAIR4ML, MLSEA]\n",
    "\n",
    "########################################################################################################\n",
    "### COMMIT: Git Integration + Provenance Logging #######################################################\n",
    "\n",
    "    def get_latest_commit_hash(repo_path=\".\"):\n",
    "        res = subprocess.run(\n",
    "            [\"git\", \"-C\", repo_path, \"rev-parse\", \"HEAD\"],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        return res.stdout.strip()\n",
    "    \n",
    "    def get_remote_url(repo_path=\".\", remote=\"origin\"):\n",
    "        res = subprocess.run(\n",
    "            [\"git\", \"-C\", repo_path, \"config\", \"--get\", f\"remote.{remote}.url\"],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        return res.stdout.strip()\n",
    "    \n",
    "    def make_commit_link(remote_url, commit_hash):\n",
    "        base = remote_url.rstrip(\".git\")\n",
    "        if base.startswith(\"git@\"):\n",
    "            base = base.replace(\":\", \"/\").replace(\"git@\", \"https://\")\n",
    "        return f\"{base}/commit/{commit_hash}\"\n",
    "    \n",
    "    def simple_commit_and_push_and_log(repo_path=\".\", message=\"Auto commit\", remote=\"origin\", branch=\"main\"):\n",
    "        status = subprocess.run([\"git\", \"-C\", repo_path, \"status\", \"--porcelain\"], capture_output=True, text=True)\n",
    "        if not status.stdout.strip():\n",
    "            print(\"ğŸŸ¡ No changes to commit.\")\n",
    "            return None, None\n",
    "    \n",
    "        subprocess.run([\"git\", \"-C\", repo_path, \"add\", \"--all\"], capture_output=True, text=True)\n",
    "        commit = subprocess.run([\"git\", \"-C\", repo_path, \"commit\", \"-m\", message], capture_output=True, text=True)\n",
    "        if commit.returncode:\n",
    "            print(\"âŒ git commit failed:\\n\", commit.stderr)\n",
    "            return None, None\n",
    "        print(\"âœ… Commit successful.\")\n",
    "    \n",
    "        push = subprocess.run([\"git\", \"-C\", repo_path, \"push\", \"-u\", remote, branch], capture_output=True, text=True)\n",
    "        if push.returncode:\n",
    "            print(\"âŒ git push failed:\\n\", push.stderr)\n",
    "        else:\n",
    "            print(\"ğŸš€ Push successful.\")\n",
    "    \n",
    "        sha = get_latest_commit_hash(repo_path)\n",
    "        url = get_remote_url(repo_path, remote)\n",
    "        link = make_commit_link(url, sha)\n",
    "        return sha, link\n",
    "    \n",
    "    # â”€â”€ Perform commit and get commit SHA and link â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    sha, link = simple_commit_and_push_and_log(\n",
    "        repo_path=\".\",\n",
    "        message=\"Auto commit after successful training\"\n",
    "    )\n",
    "    \n",
    "    # â”€â”€ Ask for version tag and log it â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    def get_version_tag_for_commit(commit_hash, known_tags=None):\n",
    "        known_tags = known_tags or {}\n",
    "        version_tag = known_tags.get(commit_hash, \"untagged\")\n",
    "        if version_tag == \"untagged\":\n",
    "            print(f\"âš ï¸ Commit {commit_hash[:8]} is not tagged with a version.\")\n",
    "            user_input = input(\"ğŸ”– Enter version tag for this commit (or press Enter to skip): \").strip()\n",
    "            version_tag = user_input if user_input else \"untagged\"\n",
    "        return commit_hash, version_tag\n",
    "    \n",
    "    commit, version_tag = get_version_tag_for_commit(sha)\n",
    "    mlflow.set_tag(\"GIT_code_version\", version_tag)  # [PROV]\n",
    "    mlflow.set_tag(\"model_version\", version_tag)  # [PROV]\n",
    "\n",
    "    \n",
    "    \n",
    "    # â”€â”€ Log author info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    def get_git_author():\n",
    "        name = subprocess.check_output([\"git\", \"config\", \"user.name\"]).decode().strip()\n",
    "        email = subprocess.check_output([\"git\", \"config\", \"user.email\"]).decode().strip()\n",
    "        return name, email\n",
    "    \n",
    "    name, email = get_git_author()\n",
    "    mlflow.set_tag(\"GIT_user\", name)               # [PROV]\n",
    "    mlflow.set_tag(\"GIT_user_email\", email)        # [PROV]\n",
    "    \n",
    "    # â”€â”€ Log Git diff between this and previous commit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if sha and link:\n",
    "        previous_commit_hash = db_meta.get(\"code_commit_hash\", \"\")  # Fallback for comparison\n",
    "        if previous_commit_hash:\n",
    "            diff_text = subprocess.check_output(\n",
    "                [\"git\", \"-C\", \".\", \"diff\", previous_commit_hash, sha],\n",
    "                encoding=\"utf-8\", errors=\"ignore\"\n",
    "            )\n",
    "    \n",
    "            remote_url = get_remote_url(\".\")\n",
    "            remote_url = remote_url.rstrip(\".git\")\n",
    "            if remote_url.startswith(\"git@\"):\n",
    "                remote_url = remote_url.replace(\":\", \"/\").replace(\"git@\", \"https://\")\n",
    "    \n",
    "            previous_commit_url = f\"{remote_url}/commit/{previous_commit_hash}\"\n",
    "            current_commit_url  = f\"{remote_url}/commit/{sha}\"\n",
    "    \n",
    "            diff_data = {\n",
    "                \"GIT_previous_commit\":        previous_commit_hash,\n",
    "                \"GIT_previous_commit_url\":    previous_commit_url,\n",
    "                \"GIT_current_commit\":         sha,\n",
    "                \"GIT_current_commit_url\":     current_commit_url,\n",
    "                \"GIT_diff\":                   diff_text\n",
    "            }\n",
    "    \n",
    "            mlflow.log_dict(diff_data, artifact_file=\"GIT_commit_diff.json\")  # [PROV]\n",
    "            mlflow.set_tag(\"GIT_previous_commit_hash\", previous_commit_hash)\n",
    "            mlflow.set_tag(\"GIT_current_commit_hash\", sha)\n",
    "            mlflow.set_tag(\"GIT_current_commit_url\", link)\n",
    "########################################################################################################\n",
    "### Reproducibility Metadata Extraction + Text Log #####################################################\n",
    "\n",
    "# â”€â”€ Log all categorized metadata (FAIR, PROV\n",
    "    , DBRepo, etc.) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # log_metadata_dict_to_mlflow(categorized_fields)  # [FAIR4ML, PROV, Internal]\n",
    "\n",
    "    log_metadata_dict_to_mlflow(\n",
    "        metadata=doi_metadata,\n",
    "        prefix=\"DOI_\",\n",
    "        snapshot_name=\"doi_metadata_snapshot.json\"\n",
    "    )\n",
    "    # â”€â”€ Retrieve full run metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    run_id    = run.info.run_id\n",
    "    run_info  = client.get_run(run_id).info\n",
    "    run_data  = client.get_run(run_id).data\n",
    "    \n",
    "    params  = dict(run_data.params)\n",
    "    metrics = dict(run_data.metrics)\n",
    "    tags    = dict(run_data.tags)\n",
    "    \n",
    "    # â”€â”€ List all artifacts in the run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    artifact_uri  = run_info.artifact_uri\n",
    "    artifact_meta = []\n",
    "    \n",
    "    def _gather(path=\"\"):\n",
    "        for af in client.list_artifacts(run_id, path):\n",
    "            if af.is_dir:\n",
    "                _gather(af.path)\n",
    "            else:\n",
    "                rel_path = af.path.lower()\n",
    "                if rel_path.endswith((\".json\", \".txt\", \".patch\")):\n",
    "                    artifact_meta.append({\"path\": af.path, \"type\": \"text\"})\n",
    "                elif rel_path.endswith((\".png\", \".jpg\", \".jpeg\", \".svg\")):\n",
    "                    artifact_meta.append({\"path\": af.path, \"type\": \"image\"})\n",
    "                else:\n",
    "                    artifact_meta.append({\"path\": af.path, \"type\": \"other\"})\n",
    "    \n",
    "    _gather()\n",
    "    \n",
    "    # â”€â”€ (Optional) Store artifact meta if needed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    mlflow.log_dict({\"artifacts\": artifact_meta}, artifact_file=\"artifact_summary.json\")  # [Internal]\n",
    "    \n",
    "    # â”€â”€ Notebook directory (for trace/log location reference) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    notebook_dir = os.getcwd()\n",
    "    \n",
    "    ########################################################################################################\n",
    "    ### Generate Reproducibility Instructions ##############################################################\n",
    "    \n",
    "    # â”€â”€ Generate reproducibility .txt log with key details â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    repro_txt_path = generate_reproducibility_txt_log(\n",
    "        model_name=model_name,\n",
    "        dataset_name=db_meta.get(\"dataset_name\", \"unknown\"),\n",
    "        dataset_version=selected_version,\n",
    "        hyperparams=ML_EXP_hyperparams,\n",
    "        metrics={\n",
    "            \"accuracy\": acc,\n",
    "            \"f1_macro\": f1,\n",
    "            \"precision_macro\": prec,\n",
    "            \"recall_macro\": rec,\n",
    "            \"roc_auc\": auc\n",
    "        },\n",
    "        git_commit=sha,\n",
    "        run_id=run_id\n",
    "    )\n",
    "    \n",
    "    # â”€â”€ Log the .txt path to MLflow for traceability â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    mlflow.log_param(\"reproducibility_log_path\", repro_txt_path)  # [Internal, FAIR4ML]\n",
    "########################################################################################################\n",
    "### COMBINE: Export Full Run Summary as JSON ###########################################################\n",
    "\n",
    "# â”€â”€ Create output directory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # summary_dir = os.path.join(os.getcwd(), \"MODEL_PROVENANCE\", model_name)\n",
    "    # os.makedirs(summary_dir, exist_ok=True)\n",
    "    \n",
    "    # â”€â”€ Prepare run summary dict â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    summary = {\n",
    "        \"run_id\":         run_id,\n",
    "        \"run_name\":       run_info.run_name,\n",
    "        \"experiment_id\":  run_info.experiment_id,\n",
    "        \"start_time\":     run_info.start_time,\n",
    "        \"end_time\":       run_info.end_time,\n",
    "        \"params\":         params,\n",
    "        \"metrics\":        metrics,\n",
    "        \"tags\":           tags,\n",
    "        \"artifacts\":      artifact_meta\n",
    "    }\n",
    "    \n",
    "    # â”€â”€ Write summary to JSON file â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    summary_filename    = f\"{model_name}_run_summary.json\"\n",
    "    summary_local_path  = os.path.join(summary_dir, summary_filename)\n",
    "    \n",
    "    with open(summary_local_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    # â”€â”€ Log summary JSON to MLflow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    mlflow.log_artifact(summary_local_path, artifact_path=\"run_summaries\")  # [FAIR4ML, Internal]\n",
    "    print(\"ğŸ“ Run summary JSON logged at:\", summary_local_path)\n",
    "    \n",
    "    # â”€â”€ End MLflow run with PROV-O end timestamp â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    end_time = datetime.now().isoformat()\n",
    "    mlflow.set_tag(\"endedAtTime\", end_time)  # [PROV]\n",
    "    mlflow.end_run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4d9a4-032f-4b76-b74e-6a3a54ebaf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_name = summary_dir.name\n",
    "print(upload_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b51a1a-b4ee-42d6-9724-bba5419553b4",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "This script generates a minimal `requirements.txt` file by:\n",
    "- Running `pip freeze` to list all installed Python packages.\n",
    "- Filtering the packages to include only those that match a predefined set of required keywords.\n",
    "- Writing the filtered package list (with versions) to `requirements.txt`.\n",
    "\n",
    "This ensures that only relevant dependencies are included for reproducibility and lightweight environment setup.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee2f35d-1cb5-481a-aa26-3212cabca3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "required_keywords = [\n",
    "    \"mlflow\", \"scikit-learn\", \"pandas\", \"numpy\", \"pyyaml\", \"seaborn\",\n",
    "    \"matplotlib\", \"shap\", \"rdflib\", \"requests\", \"python-dotenv\", \"gitpython\", \"psutil\", \"pyld\"\n",
    "]\n",
    "\n",
    "# Run pip freeze\n",
    "result = subprocess.run([\"pip\", \"freeze\"], stdout=subprocess.PIPE, text=True)\n",
    "all_packages = result.stdout.splitlines()\n",
    "\n",
    "# Filter based on matching names\n",
    "filtered = [pkg for pkg in all_packages if any(kw.lower() in pkg.lower() for kw in required_keywords)]\n",
    "\n",
    "# Ensure summary_dir is a Path object\n",
    "summary_dir = Path(summary_dir)\n",
    "\n",
    "# Define your custom file name with .txt extension\n",
    "custom_name = \"requirements.txt\"\n",
    "\n",
    "# Create full output path\n",
    "custom_output_path = summary_dir / custom_name\n",
    "\n",
    "# Save the renamed metadata in text format\n",
    "with open(custom_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Renamed metadata saved to: {custom_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950e5171-41c0-48e6-be9b-898ae82401e6",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "This script reads a saved MLflow run summary JSON file and programmatically inserts \n",
    "the extracted metadata into various tables of a remote database (via REST API). \n",
    "It covers:\n",
    "\n",
    "1. Session Metadata: Captures runtime details (username, OS, script name, etc.).\n",
    "2. Experiment Metadata: Stores run/session/model linkage and timestamps.\n",
    "3. Git Metadata: Tracks versioning, author, and repository details.\n",
    "4. Dataset Metadata: Logs dataset version, structure, and FAIR-related fields.\n",
    "5. Model Metadata: Includes algorithm, features used, training split, and provenance info.\n",
    "6. Metrics + Justifications: Extracts evaluation metrics and manually logged justifications.\n",
    "\n",
    "Each metadata section is mapped to a dedicated table via unique table IDs and inserted \n",
    "using `POST` requests to the configured database API.\n",
    "\n",
    "âš ï¸ Assumes the summary file path is stored in `summary_local_path`.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe4cc5-396c-48ae-8561-df4e0a44fbe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "auth = (\"reema2609\", \"Toothless!26\")\n",
    "BASE = \"http://localhost/api/database/ef3c32c8-a489-418b-b353-14392f1ab82a/table\"\n",
    "TABLES = {\n",
    "    \"session_metadata\": \"f533de62-2f66-4675-9876-3cb8cadaf70d\",\n",
    "    \"experiment_metadata\": \"bc51a23a-026a-45bb-b08f-1dcadb6787f1\",\n",
    "    \"git_metadata\": \"4043653d-1c74-4817-9054-c08462b37990\",\n",
    "    \"dataset_metadata\": \"9ba8470a-e16f-4228-87a1-b17b34a761ff\",\n",
    "    \"model_metadata\": \"e871bc74-b656-4bef-81cf-c601b5a57ddf\",\n",
    "    \"justification_metadata\": \"f3b0582a-ac88-444f-8214-577fbad0cbcf\"\n",
    "}\n",
    "import uuid\n",
    "\n",
    "# Generate a unique dataset_id using UUID\n",
    "dataset_guid = str(uuid.uuid4())\n",
    "# --- Load metadata file ---\n",
    "# with open(\"MODEL_PROVENANCE/b788db5d12174c28bc175589898f7f95/RandomForest_Iris_v20250516_193049_run_summary.json\", \"r\") as f:\n",
    "with open(summary_local_path, \"r\") as f:\n",
    "\n",
    "    meta = json.load(f)\n",
    "\n",
    "# --- Helper ---\n",
    "def to_mysql_datetime(ts):\n",
    "    return datetime.strptime(ts.split(\".\")[0], \"%Y-%m-%dT%H:%M:%S\").isoformat() + \"+00:00\"\n",
    "\n",
    "# --- Extract shared values ---\n",
    "run_id = meta[\"run_id\"]\n",
    "session_id = meta[\"params\"][\"session_id\"]\n",
    "dataset_id = meta[\"tags\"][\"dataset_id\"]\n",
    "model_id = \"model_\" + meta[\"tags\"][\"model_name\"].lower() + f\"_{ts}\"\n",
    "git_commit = meta[\"tags\"][\"git_commit\"]\n",
    "git_version = meta[\"tags\"][\"GIT_code_version\"]\n",
    "timestamp_utc = meta[\"params\"][\"timestamp_utc\"]\n",
    "username = meta[\"params\"][\"username\"]\n",
    "platform = meta[\"params\"][\"platform\"]\n",
    "hostname = meta[\"params\"][\"hostname\"]\n",
    "target_var = meta[\"tags\"][\"target_variable\"]\n",
    "label_map = meta[\"tags\"][\"label_map\"]\n",
    "feature_list = meta[\"params\"][\"final_feature_names\"]\n",
    "dataset_name = meta[\"tags\"][\"dataset_name\"]\n",
    "dataset_version = meta[\"tags\"][\"dataset_version\"]\n",
    "estimator = meta[\"tags\"][\"estimator_name\"]\n",
    "feature_select = meta[\"tags\"][\"feature_columns\"]\n",
    "label_snap = meta[\"tags\"][\"target_variable_encoded\"]\n",
    "model_name = meta[\"tags\"][\"model_name\"]\n",
    "imbalance_ratio = 1.0 if \"imbalance_ratio\" not in meta[\"tags\"] else meta[\"tags\"][\"imbalance_ratio\"]\n",
    "\n",
    "# --- Extract shared values (exact keys from JSON) ---\n",
    "python_version = meta[\"params\"].get(\"python_version\")\n",
    "os_platform = meta[\"params\"].get(\"os_platform\")\n",
    "role = meta[\"params\"].get(\"role\")\n",
    "project_id = meta[\"params\"].get(\"project_id\")\n",
    "script_name = meta[\"params\"].get(\"source_file_name\", \"None_specified\")\n",
    "\n",
    "# --- Build full session payload (VARCHAR timestamp, full mapping) ---\n",
    "session_payload = {\n",
    "    \"session_id\": session_id,\n",
    "    \"username\": username,\n",
    "    \"timestamp\": timestamp_utc,\n",
    "    \"hostname\": hostname,\n",
    "    \"platform\": platform,\n",
    "    \"python_version\": python_version,\n",
    "    \"os_platform\": os_platform,\n",
    "    \"role\": role,\n",
    "    \"project_id\": project_id,\n",
    "    \"script_name\": script_name\n",
    "}\n",
    "\n",
    "session_url = f\"{BASE}/{TABLES['session_metadata']}/data\"\n",
    "\n",
    "response = requests.post(\n",
    "    session_url,\n",
    "    headers=headers,\n",
    "    auth=auth,\n",
    "    json={\"data\": session_payload}\n",
    ")\n",
    "\n",
    "# --- Logging ---\n",
    "print(\"\\nğŸ” Session Metadata POST\")\n",
    "print(\"â¡ï¸ URL:\", session_url)\n",
    "print(\"ğŸ“¦ Payload:\")\n",
    "print(json.dumps(session_payload, indent=2))\n",
    "print(\"ğŸ“¤ Status Code:\", response.status_code)\n",
    "print(\"ğŸ“ Response Text:\", response.text)\n",
    "#######################################################\n",
    "\n",
    "# # --- 2. Experiment Metadata ---\n",
    "exp_payload = {\n",
    "    \"runid\": run_id,\n",
    "    \"sessionid\": session_id,\n",
    "    \"modelid\": model_id,\n",
    "    \"datasetid\": dataset_guid,\n",
    "    \"git_commit\": meta[\"tags\"].get(\"git_commit\",\"None_specified\"),\n",
    "    \"invenioid\": meta[\"tags\"].get(\"DOI_dataset_id\",\"None_specified\"),\n",
    "    \"timestamp\": timestamp_utc,  # keep as VARCHAR\n",
    "\n",
    "    # NEW fields\n",
    "    \"experiment_id\": meta[\"tags\"].get(\"experiment_id\",\"None_specified\"),\n",
    "    \"run_name\": meta.get(\"run_name\",\"None_specified\"),\n",
    "    \"training_start_time\": meta[\"tags\"].get(\"training_start_time\",\"None_specified\"),\n",
    "    \"training_end_time\": meta[\"tags\"].get(\"training_end_time\",\"None_specified\"),\n",
    "    \"source_file\": meta[\"tags\"].get(\"mlflow.source.name\",\"None_specified\"),\n",
    "    \"source_notebook\": meta[\"tags\"].get(\"notebook_name\",\"None_specified\")\n",
    "}\n",
    "\n",
    "exp_url = f\"{BASE}/{TABLES['experiment_metadata']}/data\"\n",
    "\n",
    "response = requests.post(\n",
    "    exp_url,\n",
    "    headers=headers,\n",
    "    auth=auth,\n",
    "    json={\"data\": exp_payload}\n",
    ")\n",
    "\n",
    "# --- Logging ---\n",
    "print(\"\\nğŸ” Experiment Metadata POST\")\n",
    "print(\"â¡ï¸ URL:\", exp_url)\n",
    "print(\"ğŸ“¦ Payload:\")\n",
    "print(json.dumps(exp_payload, indent=2))\n",
    "print(\"ğŸ“¤ Status Code:\", response.status_code)\n",
    "print(\"ğŸ“ Response Text:\", response.text)\n",
    "\n",
    "\n",
    "# --- 3. Git Metadata ---\n",
    "git_payload = {\n",
    "    \"commit_hash\": meta[\"tags\"][\"git_commit\"],\n",
    "    \"repo_url\": meta[\"tags\"].get(\"DOI_prov_used\",\"None_specified\"),\n",
    "    \"branch\": meta[\"tags\"].get(\"GIT_branch\", \"main\"),\n",
    "    \"author\": meta[\"tags\"].get(\"GIT_user\",\"None_specified\"),\n",
    "    \"author_email\": meta[\"tags\"].get(\"GIT_user_email\",\"None_specified\"),\n",
    "    \"version\": meta[\"tags\"].get(\"GIT_code_version\",\"None_specified\"),\n",
    "    \"origin_url\": meta[\"tags\"].get(\"GIT_origin_url\",\"None_specified\"),\n",
    "    \"timestamp\": meta[\"params\"].get(\"timestamp_utc\",\"None_specified\")\n",
    "}\n",
    "\n",
    "\n",
    "git_url = f\"{BASE}/{TABLES['git_metadata']}/data\"\n",
    "\n",
    "response = requests.post(\n",
    "    git_url,\n",
    "    headers=headers,\n",
    "    auth=auth,\n",
    "    json={\"data\": git_payload}\n",
    ")\n",
    "\n",
    "# --- Logging ---\n",
    "print(\"\\nğŸ” Git Metadata POST\")\n",
    "print(\"â¡ï¸ URL:\", git_url)\n",
    "print(\"ğŸ“¦ Payload:\")\n",
    "print(json.dumps(git_payload, indent=2))\n",
    "print(\"ğŸ“¤ Status Code:\", response.status_code)\n",
    "print(\"ğŸ“ Response Text:\", response.text)\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Utility cleaning functions ---\n",
    "def clean(value, fallback=\"unknown\"):\n",
    "    if value in [None, \"\", \"None_specified\", \"[]\", \"â€”\", \"not specified\", \"null\", \"None\"]:\n",
    "        return fallback\n",
    "    return value\n",
    "\n",
    "def clean_date_for_timestamp(value):\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            value = value.replace(\"T\", \" \").replace(\"Z\", \"\")\n",
    "            dt = datetime.fromisoformat(value.split(\".\")[0])\n",
    "            return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# --- Extract and clean timestamps ---\n",
    "created_ts = clean_date_for_timestamp(meta[\"tags\"].get(\"DOI_dataset_created\"))\n",
    "updated_ts = clean_date_for_timestamp(meta[\"tags\"].get(\"DOI_dataset_updated\"))\n",
    "\n",
    "# --- Build payload dynamically ---\n",
    "dataset_payload = {\n",
    "    \"dataset_guid\": dataset_guid,\n",
    "    \"dataset_id\": dataset_id,\n",
    "    \"table_name\": clean(meta[\"tags\"].get(\"dataset_name\")),\n",
    "    \"detailed_type\": \"CSV\",\n",
    "    \"classes\": str(meta[\"tags\"].get(\"dataset_num_classes\", 3)),\n",
    "    \"features\": str(meta[\"params\"].get(\"final_num_features\", 4)),\n",
    "    \"output_type\": clean(meta[\"tags\"].get(\"output_type\", \"categorical\")),\n",
    "    \"version\": clean(meta[\"tags\"].get(\"dataset_version\")),\n",
    "    \"source_url\": clean(meta[\"tags\"].get(\"DOI_dataset_url\")),\n",
    "    \"title\": clean(meta[\"tags\"].get(\"DOI_dataset_title\")),\n",
    "    \"description\": clean(meta[\"tags\"].get(\"DOI_dataset_description\")),\n",
    "    \"license\": clean(meta[\"tags\"].get(\"DOI_dataset_license\")),\n",
    "    \"creator\": clean(meta[\"tags\"].get(\"DOI_dataset_creator\")),\n",
    "    \"publisher\": clean(meta[\"tags\"].get(\"DOI_dataset_publisher\")),\n",
    "    \"publication_year\": str(meta[\"tags\"].get(\"DOI_dataset_publication_year\", \"unknown\")),\n",
    "    \"language\": clean(meta[\"tags\"].get(\"DOI_dataset_language\", \"unknown\")),\n",
    "    \"citation_count\": str(meta[\"tags\"].get(\"DOI_dataset_citation_count\", 0)),\n",
    "    \"subjects\": clean(meta[\"tags\"].get(\"DOI_dataset_subjects\", \"info not available\")),\n",
    "    \"related_resources\": json.dumps(meta[\"tags\"].get(\"DOI_dataset_related_resources\", [])),\n",
    "    \"schema_version\": clean(meta[\"tags\"].get(\"DOI_dataset_schema_version\", \"unknown\")),\n",
    "    \"registered\": clean_date_for_timestamp(meta[\"tags\"].get(\"DOI_dataset_registered\")),\n",
    "    \"citations_over_time\": json.dumps(meta[\"tags\"].get(\"DOI_dataset_citations_over_time\", [])),\n",
    "    \"created\": str(created_ts) if created_ts else \"\",\n",
    "    \"updated\": str(updated_ts) if updated_ts else \"\"\n",
    "}\n",
    "\n",
    "# --- Build URL ---\n",
    "dataset_url = f\"{BASE}/{TABLES['dataset_metadata']}/data\"\n",
    "\n",
    "# --- Send POST request ---\n",
    "response = requests.post(\n",
    "    dataset_url,\n",
    "    headers=headers,\n",
    "    auth=auth,\n",
    "    json={\"data\": dataset_payload}\n",
    ")\n",
    "\n",
    "# --- Log request/response ---\n",
    "print(\"\\nğŸ” Dataset Metadata POST\")\n",
    "print(\"â¡ï¸ URL:\", dataset_url)\n",
    "print(\"ğŸ“¦ Payload:\")\n",
    "print(json.dumps(dataset_payload, indent=2))\n",
    "print(\"ğŸ“¤ Status Code:\", response.status_code)\n",
    "print(\"ğŸ“ Response Text:\", response.text)\n",
    "print(\"ğŸ“¬ Headers:\", dict(response.headers))\n",
    "print(\"ğŸ“¦ Payload Sent:\")\n",
    "print(json.dumps(dataset_payload, indent=2))\n",
    "\n",
    "# -- Log POST request for model metadata --\n",
    "model_payload = {\n",
    "    \"model_id\": model_id,\n",
    "    \"name\": model_name,\n",
    "    \"algo\": estimator,\n",
    "    \"architecture\": meta[\"tags\"].get(\"model_architecture\"),\n",
    "    \"features\": meta[\"params\"].get(\"final_feature_names\"),\n",
    "    \"label_snap\": meta[\"tags\"].get(\"target_variable_encoded\"),\n",
    "    \"train_split\": float(meta[\"params\"][\"n_train_samples\"]) / int(meta[\"params\"][\"input_row_count\"]),\n",
    "    \"test_split\": float(meta[\"params\"][\"n_test_samples\"]) / int(meta[\"params\"][\"input_row_count\"]),\n",
    "    \"target_var\": meta[\"tags\"].get(\"target_variable\"),\n",
    "    \"label_map\": meta[\"tags\"].get(\"label_map\"),\n",
    "    \"feature_select\": meta[\"tags\"].get(\"feature_columns\"),\n",
    "    \"imbalance_ratio\": meta[\"tags\"].get(\"imbalance_ratio\", 1.0),\n",
    "    \"version\": meta[\"tags\"].get(\"model_version\"),\n",
    "    \"serialization_format\": meta[\"tags\"].get(\"model_serialization\"),\n",
    "    \"model_path\": meta[\"tags\"].get(\"model_path\"),\n",
    "    \"model_short_name\": meta[\"tags\"].get(\"selected_model\"),\n",
    "    \"hyperparameters\": meta[\"tags\"].get(\"hyperparameters\"),\n",
    "    \"preprocessing_info\": meta[\"tags\"].get(\"preprocessing_info\")\n",
    "}\n",
    "\n",
    "\n",
    "model_url = f\"{BASE}/{TABLES['model_metadata']}/data\"\n",
    "response = requests.post(\n",
    "    model_url,\n",
    "    headers=headers,\n",
    "    auth=auth,\n",
    "    json={\"data\": model_payload}\n",
    ")\n",
    "\n",
    "# --- Logging ---\n",
    "print(\"\\nğŸ” MODEL METADATA POST\")\n",
    "print(\"â¡ï¸ URL:\", model_url)\n",
    "print(\"ğŸ“¦ Payload:\")\n",
    "print(json.dumps(model_payload, indent=2))\n",
    "print(\"ğŸ“¤ Status Code:\", response.status_code)\n",
    "print(\"ğŸ“ Response Text:\", response.text)\n",
    "\n",
    "# --- Extract Metrics + Justification (final section) ------------------------------------\n",
    "# --- Allowed lowercase fields from your DB schema ---\n",
    "allowed_fields = {\n",
    "    \"run_id\", \"accuracy\", \"f1_macro\", \"num_deleted_rows\", \"num_inserted_rows\",\n",
    "    \"precision_macro\", \"recall_macro\", \"roc_auc\", \"row_count_end\", \"row_count_start\",\n",
    "    \"training_accuracy_score\", \"training_f1_score\", \"training_log_loss\",\n",
    "    \"training_precision_score\", \"training_recall_score\", \"training_roc_auc\", \"training_score\",\n",
    "    \"justification_bootstrap\", \"justification_class_weight\", \"justification_criterion\",\n",
    "    \"justification_dataset_version\", \"justification_drop_column_x\", \"justification_ethical_considerations\",\n",
    "    \"justification_experiment_name\", \"justification_intended_use\", \"justification_max_depth\",\n",
    "    \"justification_max_features\", \"justification_metric_choice\", \"justification_min_samples_leaf\",\n",
    "    \"justification_min_samples_split\", \"justification_model_choice\", \"justification_model_limitations\",\n",
    "    \"justification_not_intended_for\", \"justification_n_estimators\", \"justification_n_jobs\",\n",
    "    \"justification_oob_score\", \"justification_target_variable\", \"justification_test_split\",\n",
    "    \"justification_threshold_accuracy\", \"justification_verbose\"\n",
    "}\n",
    "\n",
    "# --- Build raw payload from JSON ---\n",
    "raw_payload = {\n",
    "    \"run_id\": meta.get(\"run_id\", \"unknown_run\"),\n",
    "    **meta.get(\"metrics\", {}),\n",
    "    **{\n",
    "        k: v for k, v in meta.get(\"tags\", {}).items()\n",
    "        if k.startswith(\"justification_\") or k.startswith(\"justification_MLSEA_\")\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Normalize keys and filter only valid ones ---\n",
    "clean_payload = {\n",
    "    k.lower(): v for k, v in raw_payload.items()\n",
    "    if k.lower() in allowed_fields\n",
    "}\n",
    "\n",
    "# --- POST request ---\n",
    "metrics_url = f\"{BASE}/{TABLES['justification_metadata']}/data\"\n",
    "response = requests.post(\n",
    "    metrics_url,\n",
    "    headers=headers,\n",
    "    auth=auth,\n",
    "    json={\"data\": clean_payload}\n",
    ")\n",
    "\n",
    "# --- Log Result ---\n",
    "print(\"\\nğŸ” Cleaned Metrics + Justification POST\")\n",
    "print(\"â¡ï¸ URL:\", metrics_url)\n",
    "print(\"ğŸ“¦ Payload:\")\n",
    "print(json.dumps(clean_payload, indent=2))\n",
    "print(\"ğŸ“¤ Status Code:\", response.status_code)\n",
    "print(\"ğŸ“ Response Text:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb53336-135e-4486-ad08-577edb19998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta[\"tags\"].get(\"DOI_dataset_created\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8838b3ec-4c63-447f-87a9-dbb433138503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def append_to_dataset_csv(payload, filepath=\"dataset_metadata_log.csv\"):\n",
    "    file = Path(filepath)\n",
    "    headers = list(payload.keys())\n",
    "    \n",
    "    # Write header if file doesn't exist\n",
    "    write_header = not file.exists()\n",
    "    \n",
    "    with open(filepath, mode='a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=headers)\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(payload)\n",
    "append_to_dataset_csv(dataset_payload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f3d37-d830-40b8-a147-dcdc1c491fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ” Cross-Reference MLflow Run Metadata from All Tables in Local DB\n",
    "\n",
    "This script fetches and cross-references metadata records for a specific `run_id`\n",
    "across multiple database tables in a local VRE system via REST API.\n",
    "\n",
    "It attempts to:\n",
    "- Retrieve all records from the six defined metadata tables (session, experiment, git, dataset, model, metrics).\n",
    "- Match each table's relevant entry based on foreign key references from `experiment_metadata`.\n",
    "- Collect matched records in a dictionary (`table_objects`) and print them in a structured JSON format.\n",
    "\n",
    "Useful for:\n",
    "- Debugging provenance trace for a single run\n",
    "- Verifying if all expected metadata entries were successfully stored\n",
    "- Inspecting broken or incomplete relationships between tables\n",
    "\n",
    "Note:\n",
    "- Assumes `run_id` is defined and valid\n",
    "- Queries the API at `http://localhost`, expecting an accessible backend\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee29480c-3428-4ffe-b326-e0d4b2744a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Constants\n",
    "DB_ID = \"ef3c32c8-a489-418b-b353-14392f1ab82a\"\n",
    "HEADERS = {\"Accept\": \"application/json\"}\n",
    "TABLES = {\n",
    "    \"session_metadata\": \"f533de62-2f66-4675-9876-3cb8cadaf70d\",\n",
    "    \"experiment_metadata\": \"bc51a23a-026a-45bb-b08f-1dcadb6787f1\",\n",
    "    \"git_metadata\": \"4043653d-1c74-4817-9054-c08462b37990\",\n",
    "    \"dataset_metadata\": \"46d10ebb-2155-4198-9c52-c3bc266b321f\",\n",
    "    \"model_metadata\": \"2b3857ef-b521-4791-8ff6-3c358b581128\",\n",
    "    \"justification_metadata\": \"f3b0582a-ac88-444f-8214-577fbad0cbcf\"\n",
    "}\n",
    "# Replace this with your actual run ID\n",
    "run_id = run_id\n",
    "\n",
    "# Dictionary to store results\n",
    "table_objects = {}\n",
    "\n",
    "# Step 1: Fetch experiment_metadata first\n",
    "try:\n",
    "    url = f\"http://localhost/api/database/{DB_ID}/table/{TABLES['experiment_metadata']}/data?size=100000&page=0\"\n",
    "    r = requests.get(url, headers=HEADERS)\n",
    "    r.raise_for_status()\n",
    "    records = r.json()\n",
    "    experiment = next((x for x in records if x.get(\"runid\") == run_id), None)\n",
    "    table_objects[\"experiment_metadata\"] = experiment or \"âŒ No match for experiment_metadata\"\n",
    "except Exception as e:\n",
    "    experiment = None\n",
    "    table_objects[\"experiment_metadata\"] = f\"âš ï¸ Request failed: {e}\"\n",
    "\n",
    "# Step 2: Extract related identifiers\n",
    "sid = experiment.get(\"sessionid\") if experiment else None\n",
    "mid = experiment.get(\"modelid\") if experiment else None\n",
    "did = experiment.get(\"datasetid\") if experiment else None\n",
    "ghash = experiment.get(\"git_commit\") if experiment else None\n",
    "\n",
    "# Step 3: Fetch the rest of the metadata tables\n",
    "for table, table_id in TABLES.items():\n",
    "    if table == \"experiment_metadata\":\n",
    "        continue  # already handled\n",
    "\n",
    "    url = f\"http://localhost/api/database/{DB_ID}/table/{table_id}/data?size=100000&page=0\"\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS)\n",
    "        r.raise_for_status()\n",
    "        records = r.json()\n",
    "\n",
    "        if table == \"justification_metadata\":\n",
    "            match = next((x for x in records if x.get(\"run_id\") == run_id), None)\n",
    "        elif table == \"model_metadata\":\n",
    "            match = next((x for x in records if x.get(\"model_id\") == mid), None)\n",
    "        elif table == \"dataset_metadata\":\n",
    "            match = next((x for x in records if x.get(\"dataset_guid\") == did), None)\n",
    "        elif table == \"session_metadata\":\n",
    "            if not sid:\n",
    "                match = None\n",
    "            else:\n",
    "               sid = str(sid).strip()\n",
    "            match = next((x for x in records if str(x.get(\"session_id\", \"\")).strip() == sid), None)\n",
    "\n",
    "\n",
    "        elif table == \"git_metadata\":\n",
    "            match = next((x for x in records if x.get(\"commit_hash\") == ghash), None)\n",
    "        else:\n",
    "            match = None\n",
    "\n",
    "        table_objects[table] = match or f\"âŒ No match for {table}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        table_objects[table] = f\"âš ï¸ Request failed: {e}\"\n",
    "\n",
    "# # Step 4: Save the results to a file\n",
    "# output_path = f\"run_metadata_trace_{run_id}.json\"\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(table_objects, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# print(f\"âœ… Metadata trace saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af02016-3e12-4745-a612-de0ac1bc4411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f4057-4b08-4dd3-91ce-7b5b51a729f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc084d1d-a570-4f53-bd27-d95dcc068d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_structured_metadata(data: dict) -> dict:\n",
    "    # Step 1: Initialize output structure\n",
    "    structured = {\n",
    "        \"FAIR\": {},\n",
    "        \"FAIR4ML\": {},\n",
    "        \"MLSEA\": {},\n",
    "        \"Croissant\": {},\n",
    "        \"Uncategorized\": {}\n",
    "    }\n",
    "\n",
    "    # Step 2: Populate each metadata standard\n",
    "    if \"dataset_metadata\" in data:\n",
    "        ds = data[\"dataset_metadata\"]\n",
    "        structured[\"FAIR\"].update({\n",
    "            \"identifier\": ds.get(\"dataset_id\"),\n",
    "            \"title\": ds.get(\"title\"),\n",
    "            \"description\": ds.get(\"description\"),\n",
    "            \"creator\": ds.get(\"creator\"),\n",
    "            \"license\": ds.get(\"license\"),\n",
    "            \"version\": ds.get(\"version\"),\n",
    "            \"created\": ds.get(\"created\"),\n",
    "            \"updated\": ds.get(\"updated\"),\n",
    "            \"url\": ds.get(\"source_url\")\n",
    "        })\n",
    "\n",
    "    if \"experiment_metadata\" in data:\n",
    "        exp = data[\"experiment_metadata\"]\n",
    "        structured[\"FAIR4ML\"].update({\n",
    "            \"experiment_id\": exp.get(\"experiment_id\"),\n",
    "            \"run_id\": exp.get(\"runid\"),\n",
    "            \"session_id\": exp.get(\"sessionid\"),\n",
    "            \"model_id\": exp.get(\"modelid\"),\n",
    "            \"dataset_id\": exp.get(\"datasetid\"),\n",
    "            \"timestamp\": exp.get(\"timestamp\"),\n",
    "            \"training_start_time\": exp.get(\"training_start_time\"),\n",
    "            \"training_end_time\": exp.get(\"training_end_time\"),\n",
    "            \"source_file\": exp.get(\"source_file\"),\n",
    "            \"source_notebook\": exp.get(\"source_notebook\"),\n",
    "            \"invenio_id\": exp.get(\"invenioid\")\n",
    "        })\n",
    "\n",
    "    if \"justification_metadata\" in data:\n",
    "        metrics = data[\"justification_metadata\"]\n",
    "        structured[\"MLSEA\"].update({\n",
    "            k: v for k, v in metrics.items()\n",
    "            if any(key in k for key in [\"score\", \"accuracy\", \"f1\", \"roc\", \"precision\", \"recall\", \"log_loss\", \"justification_\"])\n",
    "        })\n",
    "\n",
    "    if \"model_metadata\" in data:\n",
    "        model = data[\"model_metadata\"]\n",
    "        structured[\"Croissant\"].update({\n",
    "            \"model_id\": model.get(\"model_id\"),\n",
    "            \"name\": model.get(\"name\"),\n",
    "            \"algo\": model.get(\"algo\"),\n",
    "            \"architecture\": model.get(\"architecture\"),\n",
    "            \"features\": model.get(\"features\"),\n",
    "            \"target_variable\": model.get(\"target_var\"),\n",
    "            \"label_encoding\": model.get(\"label_snap\"),\n",
    "            \"model_path\": model.get(\"model_path\"),\n",
    "            \"serialization_format\": model.get(\"serialization_format\"),\n",
    "            \"model_version\": model.get(\"version\"),\n",
    "            \"hyperparameters\": model.get(\"hyperparameters\"),\n",
    "            \"preprocessing_info\": model.get(\"preprocessing_info\")\n",
    "        })\n",
    "\n",
    "    structured[\"Uncategorized\"][\"session_metadata\"] = data.get(\"session_metadata\", {})\n",
    "    structured[\"Uncategorized\"][\"git_metadata\"] = data.get(\"git_metadata\", {})\n",
    "\n",
    "    return structured\n",
    "\n",
    "\n",
    "def rename_structured_metadata(structured: dict) -> dict:\n",
    "    # Renaming maps\n",
    "    rename_map = {\n",
    "        \"FAIR\": {\n",
    "            \"title\": \"dc:title\",\n",
    "            \"description\": \"dc:description\",\n",
    "            \"creator\": \"dc:creator\",\n",
    "            \"license\": \"dc:license\",\n",
    "            \"version\": \"dcterms:hasVersion\",\n",
    "            \"created\": \"dcterms:created\",\n",
    "            \"updated\": \"dcterms:modified\",\n",
    "            \"identifier\": \"dcterms:identifier\",\n",
    "            \"url\": \"dcat:landingPage\"\n",
    "        },\n",
    "        \"FAIR4ML\": {\n",
    "            \"experiment_id\": \"fair4ml:experimentID\",\n",
    "            \"run_id\": \"fair4ml:runID\",\n",
    "            \"session_id\": \"fair4ml:sessionID\",\n",
    "            \"model_id\": \"fair4ml:modelID\",\n",
    "            \"dataset_id\": \"fair4ml:datasetID\",\n",
    "            \"git_commit\": \"prov:wasDerivedFrom\",\n",
    "            \"timestamp\": \"prov:startedAtTime\",\n",
    "            \"training_start_time\": \"fair4ml:trainingStartTime\",\n",
    "            \"training_end_time\": \"fair4ml:trainingEndTime\",\n",
    "            \"source_file\": \"prov:usedFile\",\n",
    "            \"source_notebook\": \"fair4ml:usedNotebook\"\n",
    "        },\n",
    "        \"MLSEA\": {\n",
    "            \"run_id\": \"mlsea:run_id\",\n",
    "            \"accuracy\": \"mlsea:accuracy\",\n",
    "            \"f1_macro\": \"mlsea:f1_macro\",\n",
    "            \"num_deleted_rows\": \"mlsea:num_deleted_rows\",\n",
    "            \"num_inserted_rows\": \"mlsea:num_inserted_rows\",\n",
    "            \"precision_macro\": \"mlsea:precision_macro\",\n",
    "            \"recall_macro\": \"mlsea:recall_macro\",\n",
    "            \"roc_auc\": \"mlsea:roc_auc\",\n",
    "            \"row_count_end\": \"mlsea:row_count_end\",\n",
    "            \"row_count_start\": \"mlsea:row_count_start\",\n",
    "            \"training_accuracy_score\": \"mlsea:training_accuracy_score\",\n",
    "            \"training_f1_score\": \"mlsea:training_f1_score\",\n",
    "            \"training_log_loss\": \"mlsea:training_log_loss\",\n",
    "            \"training_precision_score\": \"mlsea:training_precision_score\",\n",
    "            \"training_recall_score\": \"mlsea:training_recall_score\",\n",
    "            \"training_roc_auc\": \"mlsea:training_roc_auc\",\n",
    "            \"training_score\": \"mlsea:training_score\"\n",
    "        },\n",
    "        \"Croissant\": {\n",
    "            \"model_id\": \"mls:model_id\",\n",
    "            \"name\": \"mls:modelName\",\n",
    "            \"algo\": \"mls:learningAlgorithm\",\n",
    "            \"architecture\": \"mls:modelArchitecture\",\n",
    "            \"features\": \"mls:featureList\",\n",
    "            \"target_variable\": \"mls:targetVariable\",\n",
    "            \"label_encoding\": \"mls:labelEncoding\",\n",
    "            \"model_path\": \"mls:modelPath\",\n",
    "            \"serialization_format\": \"mls:serializationFormat\",\n",
    "            \"model_version\": \"mls:modelVersion\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    renamed = {}\n",
    "    for category, content in structured.items():\n",
    "        renamed_fields = {}\n",
    "        for k, v in content.items():\n",
    "            new_key = rename_map.get(category, {}).get(k, k)\n",
    "            renamed_fields[new_key] = v\n",
    "        renamed[category] = renamed_fields\n",
    "\n",
    "    return renamed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa7801-68c0-4cbb-a897-4d69ae6c409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_metadata = generate_structured_metadata(table_objects)\n",
    "renamed_metadata = rename_structured_metadata(structured_metadata)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Ensure summary_dir is a Path object\n",
    "summary_dir = Path(summary_dir)\n",
    "\n",
    "# Define your custom file name\n",
    "custom_name = \"structured_metadata.json\"\n",
    "\n",
    "# Create full output path\n",
    "custom_output_path = summary_dir / custom_name\n",
    "\n",
    "# Save the renamed metadata\n",
    "with open(custom_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(renamed_metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Renamed metadata saved to: {custom_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba895421-a512-45ac-b078-3f9eb1dc84f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d2a4c-34cd-43c1-9efb-5e95c8bf0bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "from rdflib.namespace import XSD, FOAF, PROV\n",
    "from collections import defaultdict\n",
    "\n",
    "# === Paths ===\n",
    "summary_dir = Path(summary_dir)\n",
    "structured_file = summary_dir / \"structured_metadata.json\"\n",
    "\n",
    "# === Fallback load if table_objects not defined ===\n",
    "if \"table_objects\" not in globals():\n",
    "    with open(structured_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        table_objects = json.load(f)\n",
    "\n",
    "\n",
    "# === Helpers ===\n",
    "def safe_literal(value):\n",
    "    return value and str(value).strip().lower() not in [\"none\", \"no justification provided\", \"â€”\"]\n",
    "\n",
    "def safe_uri(base, value, suffix=\"\"):\n",
    "    if not safe_literal(value):\n",
    "        return None\n",
    "    cleaned = str(value).strip().replace(\" \", \"_\").replace(\"/\", \"_\").replace(\":\", \"_\")\n",
    "    return URIRef(base + cleaned + suffix)\n",
    "\n",
    "# === Entity extraction ===  (MUST COME FIRST!)\n",
    "experiment = table_objects.get(\"experiment_metadata\", {})\n",
    "model = table_objects.get(\"model_metadata\", {})\n",
    "dataset = table_objects.get(\"dataset_metadata\", {})\n",
    "git = table_objects.get(\"git_metadata\", {})\n",
    "session = table_objects.get(\"session_metadata\", {})\n",
    "metrics = table_objects.get(\"justification_metadata\", {})\n",
    "\n",
    "# === Namespaces ===\n",
    "base_uri = f\"https://github.com/reema-dass26/ml-provenance/provenance/run_{experiment.get('runid', 'default')}/\"\n",
    "EX = Namespace(base_uri)\n",
    "g = Graph()\n",
    "g.bind(\"prov\", PROV)\n",
    "g.bind(\"ex\", EX)\n",
    "g.bind(\"foaf\", FOAF)\n",
    "\n",
    "# === Create URIs with fallback defaults ===\n",
    "model_uri = safe_uri(EX, f\"model/{model.get('model_id')}\") or URIRef(EX[\"model/default\"])\n",
    "dataset_uri = safe_uri(EX, f\"dataset/{experiment.get('datasetid')}\") or URIRef(EX[\"dataset/default\"])\n",
    "code_uri = safe_uri(EX, f\"code/{git.get('commit_hash')}\") or URIRef(EX[\"code/default\"])\n",
    "activity_uri = safe_uri(EX, f\"run/{experiment.get('runid')}\") or URIRef(EX[\"run/default\"])\n",
    "agent_uri = safe_uri(EX, f\"agent/{session.get('username')}\") or URIRef(EX[\"agent/default\"])\n",
    "\n",
    "# === RDF types ===\n",
    "g.add((model_uri, RDF.type, PROV.Entity))\n",
    "g.add((dataset_uri, RDF.type, PROV.Entity))\n",
    "g.add((code_uri, RDF.type, PROV.Entity))\n",
    "g.add((activity_uri, RDF.type, PROV.Activity))\n",
    "g.add((agent_uri, RDF.type, PROV.Agent))\n",
    "g.add((agent_uri, FOAF.name, Literal(session.get(\"username\"))))\n",
    "\n",
    "# === PROV Relationships ===\n",
    "if safe_literal(experiment.get(\"training_start_time\")):\n",
    "    g.add((activity_uri, PROV.startedAtTime, Literal(experiment[\"training_start_time\"], datatype=XSD.dateTime)))\n",
    "if safe_literal(experiment.get(\"training_end_time\")):\n",
    "    g.add((activity_uri, PROV.endedAtTime, Literal(experiment[\"training_end_time\"], datatype=XSD.dateTime)))\n",
    "\n",
    "g.add((activity_uri, PROV.used, dataset_uri))\n",
    "g.add((activity_uri, PROV.used, code_uri))\n",
    "g.add((model_uri, PROV.wasGeneratedBy, activity_uri))\n",
    "g.add((activity_uri, PROV.wasAssociatedWith, agent_uri))\n",
    "\n",
    "# === Add all fields as literals ===\n",
    "def add_literals_from_dict(data: dict, subject):\n",
    "    for k, v in data.items():\n",
    "        if safe_literal(v):\n",
    "            g.add((subject, EX[k], Literal(v)))\n",
    "\n",
    "for data, uri in [\n",
    "    (model, model_uri),\n",
    "    (dataset, dataset_uri),\n",
    "    (git, code_uri),\n",
    "    (experiment, activity_uri),\n",
    "    (session, agent_uri),\n",
    "    (metrics, activity_uri)\n",
    "]:\n",
    "    add_literals_from_dict(data, uri)\n",
    "\n",
    "# === Save RDF/JSON-LD ===\n",
    "jsonld_path = summary_dir / \"prov_JSONLD_export.jsonld\"\n",
    "rdfxml_path = summary_dir / \"prov_RDFXML_export.rdf\"\n",
    "\n",
    "with open(jsonld_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(g.serialize(format=\"json-ld\", indent=2))\n",
    "\n",
    "with open(rdfxml_path, \"wb\") as f:\n",
    "    f.write(g.serialize(format=\"xml\", encoding=\"utf-8\"))\n",
    "\n",
    "print(f\"âœ… Exported to: {jsonld_path.name}, {rdfxml_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ee550-014e-4d72-9b2d-892787d09f10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pyshacl\n",
    "!pip uninstall rdflib\n",
    "!pip install rdflib==6.3.2 pyshacl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551eeda-8536-42a4-815e-38870b343c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create example SHACL shape file for FAIR metadata fields\n",
    "fair_shacl_content = \"\"\"\n",
    "@prefix sh: <http://www.w3.org/ns/shacl#> .\n",
    "@prefix dc: <http://purl.org/dc/elements/1.1/> .\n",
    "@prefix dcterms: <http://purl.org/dc/terms/> .\n",
    "@prefix dcat: <http://www.w3.org/ns/dcat#> .\n",
    "@prefix ex: <http://example.org/> .\n",
    "\n",
    "ex:FAIRShape a sh:NodeShape ;\n",
    "    sh:targetSubjectsOf dc:title ;\n",
    "    sh:property [\n",
    "        sh:path dc:title ;\n",
    "        sh:datatype xsd:string ;\n",
    "        sh:minCount 1 ;\n",
    "    ] ;\n",
    "    sh:property [\n",
    "        sh:path dc:creator ;\n",
    "        sh:datatype xsd:string ;\n",
    "        sh:minCount 1 ;\n",
    "    ] ;\n",
    "    sh:property [\n",
    "        sh:path dcterms:hasVersion ;\n",
    "        sh:datatype xsd:string ;\n",
    "        sh:minCount 1 ;\n",
    "    ] ;\n",
    "    sh:property [\n",
    "        sh:path dcat:landingPage ;\n",
    "        sh:datatype xsd:string ;\n",
    "        sh:minCount 1 ;\n",
    "    ] .\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "shapes_dir = Path(\"shapes\")\n",
    "shapes_dir.mkdir(exist_ok=True)\n",
    "fair_shape_path = shapes_dir / \"fair_shapes.ttl\"\n",
    "fair_shape_path.write_text(fair_shacl_content.strip(), encoding=\"utf-8\")\n",
    "\n",
    "fair_shape_path.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7555efb-64b8-4aca-92e1-6fdf0bc5c81a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a5e3bbb-0288-47d0-9dc4-2855d7e4801a",
   "metadata": {},
   "source": [
    "########################################################################\n",
    "# Post metadata to INVENIO\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745dc1c9-ed88-45dc-bd8c-1065c9c17aa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "API_BASE   = \"https://127.0.0.1:5000\"\n",
    "TOKEN      = \"MoWvZMgAFsFIVfaEx1klbtB3pnf71wWHcTdS37hygmLGus9fqEGqXgmW6cRl\"\n",
    "VERIFY_SSL = False  # only for selfâ€signed dev\n",
    "\n",
    "HEADERS_JSON = {\n",
    "    \"Accept\":        \"application/json\",\n",
    "    \"Content-Type\":  \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "}\n",
    "\n",
    "HEADERS_OCTET = {\n",
    "    \"Content-Type\":  \"application/octet-stream\",\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "}\n",
    "\n",
    "# The folders you want to walk & upload:\n",
    "TO_UPLOAD = [\"Trained_models\", \"MODEL_PROVENANCE\"]\n",
    "\n",
    "upload_name = summary_dir.name\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Create draft with ALL required metadata\n",
    "# -----------------------------------------------------------------------------\n",
    "def create_draft():\n",
    "    payload = {\n",
    "  \"metadata\": {\n",
    "    \"title\": upload_name,\n",
    "    \"creators\": [ {\n",
    "      \"person_or_org\": {\n",
    "        \"type\":        \"personal\",\n",
    "        \"given_name\":  \"Reema\",\n",
    "        \"family_name\": \"Dass\"\n",
    "      }\n",
    "    } ],\n",
    "    \"publication_date\": \"2025-04-24\",\n",
    "    \"resource_type\":    { \"id\": \"software\" },\n",
    "    \"access\": {\n",
    "      \"record\": \"public\",\n",
    "      \"files\":  \"public\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "    r = requests.post(f\"{API_BASE}/api/records\",\n",
    "                      headers=HEADERS_JSON,\n",
    "                      json=payload,\n",
    "                      verify=VERIFY_SSL)\n",
    "    r.raise_for_status()\n",
    "    draft = r.json()\n",
    "    print(\"âœ… Draft created:\", draft[\"id\"])\n",
    "    return draft[\"id\"], draft[\"links\"]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Register, upload and commit a single file\n",
    "# -----------------------------------------------------------------------------\n",
    "def upload_and_commit(links, key, path):\n",
    "    # 2a) register the filename in the draft\n",
    "    r1 = requests.post(links[\"files\"],\n",
    "                       headers=HEADERS_JSON,\n",
    "                       json=[{\"key\": key}],\n",
    "                       verify=VERIFY_SSL)\n",
    "    r1.raise_for_status()\n",
    "    entry = next(e for e in r1.json()[\"entries\"] if e[\"key\"] == key)\n",
    "    file_links = entry[\"links\"]\n",
    "\n",
    "    # 2b) upload the bytes\n",
    "    with open(path, \"rb\") as fp:\n",
    "        r2 = requests.put(file_links[\"content\"],\n",
    "                          headers=HEADERS_OCTET,\n",
    "                          data=fp,\n",
    "                          verify=VERIFY_SSL)\n",
    "    r2.raise_for_status()\n",
    "\n",
    "    # 2c) commit the upload\n",
    "    r3 = requests.post(file_links[\"commit\"],\n",
    "                       headers=HEADERS_JSON,\n",
    "                       verify=VERIFY_SSL)\n",
    "    r3.raise_for_status()\n",
    "    print(f\"  â€¢ Uploaded {key}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Walk each folder and upload every file\n",
    "# -----------------------------------------------------------------------------\n",
    "def upload_folder(links):\n",
    "    for folder in TO_UPLOAD:\n",
    "        if not os.path.isdir(folder):\n",
    "            print(f\"âš ï¸ Skipping missing folder {folder}\")\n",
    "            continue\n",
    "        base = os.path.dirname(folder) or folder\n",
    "        for root, _, files in os.walk(folder):\n",
    "            for fn in files:\n",
    "                local = os.path.join(root, fn)\n",
    "                # create a POSIXâ€style key preserving subfolders\n",
    "                key = os.path.relpath(local, start=base).replace(os.sep, \"/\")\n",
    "                upload_and_commit(links, key, local)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Publish the draft\n",
    "# -----------------------------------------------------------------------------\n",
    "def publish(links):\n",
    "    r = requests.post(links[\"publish\"],\n",
    "                      headers=HEADERS_JSON,\n",
    "                      verify=VERIFY_SSL)\n",
    "    if not r.ok:\n",
    "        print(\"âŒ Publish failed:\", r.status_code, r.text)\n",
    "        try: print(r.json())\n",
    "        except: pass\n",
    "        r.raise_for_status()\n",
    "    print(\"âœ… Published:\", r.json()[\"id\"])\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    recid, links = create_draft()\n",
    "    upload_folder(links)\n",
    "    publish(links)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b463223e-5425-465c-ae63-815cbb053301",
   "metadata": {},
   "source": [
    "########################################################################\n",
    "# FETCH metadata from INVENIO\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee538759-38b9-4ea8-bdc6-41cc65ede642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_metadata(record_id, model_name, api_base, headers, verify_ssl=True):\n",
    "    \"\"\"\n",
    "    Fetch Invenio metadata and save to a file named after the model inside 'Invenio_metadata' folder.\n",
    "    \"\"\"\n",
    "    response = requests.get(f\"{API_BASE}/api/records/{record_id}\",\n",
    "                            headers=headers,\n",
    "                            verify=VERIFY_SSL)\n",
    "    response.raise_for_status()\n",
    "    metadata = response.json()\n",
    "\n",
    "    print(\"âœ… Metadata fetched successfully\")\n",
    "\n",
    "    # Create the folder if it doesn't exist\n",
    "    os.makedirs(\"Invenio_metadata\", exist_ok=True)\n",
    "\n",
    "    # Construct path and save\n",
    "    file_path = os.path.join(\"Invenio_metadata\", f\"{model_name}_invenio.json\")\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "\n",
    "    print(f\"âœ… Metadata saved at {file_path}\")\n",
    "    return file_path\n",
    "path = fetch_metadata(recid, model_name, api_base=API_BASE, headers=HEADERS_JSON)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7423f2-0ff3-4104-913e-50eeb32d9d0f",
   "metadata": {},
   "source": [
    "METADATA EXTRACTION FROM INVENIO and ADD it to main Provenence FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d5968b-997e-4458-bf6b-a14dcc883698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Function: Extract metadata\n",
    "# ----------------------------\n",
    "def extract_metadata(metadata):\n",
    "    print(\"âœ… Metadata loaded successfully\")\n",
    "    print(\"â„¹ï¸ ID:\", metadata.get(\"id\", \"N/A\"))\n",
    "    print(\"ğŸ” Extracting required fields...\")\n",
    "\n",
    "    extracted_data = {\n",
    "        \"invenio_metadata\": {\n",
    "            \"id\": metadata.get(\"id\", \"\"),\n",
    "            \"title\": metadata.get(\"metadata\", {}).get(\"title\", \"\"),\n",
    "            \"creator\": \", \".join([\n",
    "                creator[\"person_or_org\"].get(\"name\", \"\")\n",
    "                for creator in metadata.get(\"metadata\", {}).get(\"creators\", [])\n",
    "            ]),\n",
    "            \"publication_date\": metadata.get(\"metadata\", {}).get(\"publication_date\", \"\"),\n",
    "            \"files\": [],\n",
    "            \"pids\": metadata.get(\"pids\", {}),\n",
    "            \"version_info\": metadata.get(\"versions\", {}),\n",
    "            \"status\": metadata.get(\"status\", \"\"),\n",
    "            \"views\": metadata.get(\"stats\", {}).get(\"this_version\", {}).get(\"views\", 0),\n",
    "            \"downloads\": metadata.get(\"stats\", {}).get(\"this_version\", {}).get(\"downloads\", 0),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for key, file_info in metadata.get(\"files\", {}).get(\"entries\", {}).items():\n",
    "        file_detail = {\n",
    "            \"key\": key,\n",
    "            \"url\": file_info[\"links\"].get(\"content\", \"\"),\n",
    "            \"size\": file_info.get(\"size\", 0),\n",
    "            \"mimetype\": file_info.get(\"mimetype\", \"\"),\n",
    "            \"checksum\": file_info.get(\"checksum\", \"\"),\n",
    "            \"metadata\": file_info.get(\"metadata\", {}),\n",
    "        }\n",
    "        extracted_data[\"invenio_metadata\"][\"files\"].append(file_detail)\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "invenio_path = f\"Invenio_metadata/{model_name}_invenio.json\"\n",
    "run_summary_path = f\"MODEL_PROVENANCE/{model_name}/{model_name}_run_summary.json\"\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Load Invenio metadata\n",
    "# ----------------------------\n",
    "with open(invenio_path, \"r\") as f:\n",
    "    original_metadata = json.load(f)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Extract metadata\n",
    "# ----------------------------\n",
    "extracted_metadata = extract_metadata(original_metadata)\n",
    "print(\"ğŸ“¤ Extracted Metadata Preview:\")\n",
    "print(json.dumps(extracted_metadata, indent=4)[:1000])  # Preview\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Load run summary\n",
    "# ----------------------------\n",
    "with open(run_summary_path, \"r\") as f:\n",
    "    existing_metadata = json.load(f)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 4: Merge metadata\n",
    "# ----------------------------\n",
    "existing_metadata.update(extracted_metadata)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 5: Save updated summary\n",
    "# ----------------------------\n",
    "with open(run_summary_path, \"w\") as f:\n",
    "    json.dump(existing_metadata, f, indent=4)\n",
    "\n",
    "print(f\"âœ… Invenio metadata embedded successfully into: {run_summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e6109-de5e-4ba4-baae-7de79c1fb131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
