âœ… RQ1 â€“ Provenance of ML Models and Metadata Standards
ðŸ”¹ RQ1.1 â€“ Data Provenance
Relevant fields:

Internal_DBRepo_*: full trace of data schema, dropped columns, feature names, n_records, data source, version, access URL.

FAIR_dataset_*: complete dataset metadata like title, creator, license, DOI, and URL.

MLSEA_dataPreprocessing: detailed preprocessing steps, feature selection, encoding, splitting, coercion strategy.

ðŸ“Œ How it contributes:
Demonstrates automatic capture of dataset structure, transformations, and identifiers aligned with FAIR and PROV-O standards.

ðŸ”¹ RQ1.2 â€“ Model Provenance
Relevant fields:

ML_EXP_params, MLSEA_hyperparameters: logs all model config (e.g., depth, criterion, split).

ML_EXP_metrics, MLSEA_evaluationMetrics: logs model evaluation (accuracy, F1, AUC).

FAIR4ML_*, PROV-O_*: partial alignment with FAIR4ML, PROV-O terms for training activities, agents, timestamps.

mlflow.log-model.history: links to MLflow model artifacts with versioning details.

MLSEA_modelArchitecture, MLSEA_trainingCodeSnapshot, ML_EXP_notebook_name

ðŸ“Œ How it contributes:
Youâ€™re already logging model training metadata in FAIR4ML, MLSEA, and PROV-O format â€” this directly answers RQ1.2 on capturing model provenance and exporting it in standard formats.

âœ… RQ2 â€“ Role of Metadata in Auditing & Debugging
Relevant metadata:

GIT_code_version, GIT_current_commit_hash, GIT_user, GIT_user_email: enable code traceability.

MLSEA_trainingProcedure: documents the train/test strategy (test size, random state).

Internal_DBRepo_target_name: explicit target values for each instance.

training_confusion_matrix.png, roc_curve_cls_*.png, feature_importances.png: visual outputs useful for model audit trails.

ðŸ“Œ Use-case alignment:

Versioning & Error Impact: You have commit diffs (GIT_commit_diff.json) and linked commits.

Reproducibility: You logged hyperparameters, train/test splits, feature set, preprocessing steps.

Modelâ€“Data Relationship: ML_EXP_dataset_id + MLSEA_modelArchitecture bridges models to dataset.

âœ… RQ3 â€“ Visualization of Provenance
Artifacts supporting visualization:

confusion_matrix.png, pr_curve_cls_*.png, roc_curve_cls_*.png: interpretable performance plots.

shap_summary.png: shows feature impact; useful for explainability.

metadata/categorized_fields_snapshot.json: foundational for structured, visual dashboard elements.

model/MLmodel, model/conda.yaml: essential for visualizing the environment config.

ðŸ“Œ How it contributes:
Supports building interactive dashboards. Metadata is already in a JSON structure, enabling traceability flows and reproducibility widgets.

âœ… RQ4 â€“ Schema Mapping and Interoperability
Relevant mappings:

PROV-O_*: partially mapped (e.g., prov:wasGeneratedBy, prov:used, prov:Entity).

FAIR4ML_*: field mappings for target variable, intended use, emissions, etc.

MLSEA_*: includes preprocessing and training pipeline metadata.

ðŸ“Œ How it contributes:
You can use this run as a test case for converting structured metadata into RDF/XML or JSON-LD (directly tied to RQ4). Some FAIR4ML and PROV-O fields are still info not available, which you can fill for completeness.