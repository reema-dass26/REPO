{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "746b7627-1b71-42a5-a112-284fbbd81090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define standard fields with requirement levels and justifications\n",
    "validated_fields = [\n",
    "    (\"FAIR\", \"dc:title\", \"‚úÖ Core\", \"Essential for discovery (DCMI)\"),\n",
    "    (\"FAIR\", \"dc:description\", \"‚úÖ Core\", \"Helps users interpret dataset meaning\"),\n",
    "    (\"FAIR\", \"dc:creator\", \"‚úÖ Core\", \"Attribution required by FAIR\"),\n",
    "    (\"FAIR\", \"dc:license\", \"‚úÖ Core\", \"Required for reuse clarity\"),\n",
    "    (\"FAIR\", \"dc:publisher\", \"üü° Recommended\", \"Common in DCAT and citation systems\"),\n",
    "    (\"FAIR\", \"dc:subject\", \"üü° Recommended\", \"Improves search and categorization\"),\n",
    "    (\"FAIR\", \"dc:issued\", \"üü° Recommended\", \"Standard publishing metadata\"),\n",
    "    (\"FAIR\", \"dc:language\", \"üü§ Optional\", \"Relevant for multilingual datasets\"),\n",
    "    (\"FAIR\", \"dcat:landingPage\", \"‚úÖ Core\", \"Required by DCAT for linking dataset access\"),\n",
    "\n",
    "    (\"PROV-O\", \"prov:Entity\", \"‚úÖ Core\", \"Describes the dataset or model as an object\"),\n",
    "    (\"PROV-O\", \"prov:Activity\", \"‚úÖ Core\", \"Connects actions to data objects\"),\n",
    "    (\"PROV-O\", \"prov:Agent\", \"‚úÖ Core\", \"Identifies responsible party\"),\n",
    "    (\"PROV-O\", \"prov:wasGeneratedBy\", \"‚úÖ Core\", \"Links output to activity\"),\n",
    "    (\"PROV-O\", \"prov:used\", \"‚úÖ Core\", \"Links input to activity\"),\n",
    "    (\"PROV-O\", \"prov:wasAssociatedWith\", \"üü° Recommended\", \"Useful for complex systems\"),\n",
    "    (\"PROV-O\", \"prov:startedAtTime\", \"‚úÖ Core\", \"Supports reproducibility\"),\n",
    "    (\"PROV-O\", \"prov:endedAtTime\", \"‚úÖ Core\", \"Supports reproducibility\"),\n",
    "\n",
    "    (\"FAIR4ML\", \"fair4ml:trainedOn\", \"‚úÖ Core\", \"Links model to dataset\"),\n",
    "    (\"FAIR4ML\", \"fair4ml:trainingStartTime\", \"‚úÖ Core\", \"Temporal traceability\"),\n",
    "    (\"FAIR4ML\", \"fair4ml:trainingEndTime\", \"‚úÖ Core\", \"Temporal traceability\"),\n",
    "    (\"FAIR4ML\", \"fair4ml:modelType\", \"‚úÖ Core\", \"Essential for reuse and understanding\"),\n",
    "    (\"FAIR4ML\", \"fair4ml:targetVariable\", \"‚úÖ Core\", \"Key for supervised learning\"),\n",
    "    (\"FAIR4ML\", \"fair4ml:trainingScriptVersion\", \"‚úÖ Core\", \"Links model to source code\"),\n",
    "    (\"FAIR4ML\", \"fair4ml:runEnvironment\", \"üü° Recommended\", \"Helpful for reproduction\"),\n",
    "\n",
    "    (\"MLSEA\", \"mlsea:accuracy\", \"‚úÖ Core\", \"Primary performance metric\"),\n",
    "    (\"MLSEA\", \"mlsea:f1_score\", \"‚úÖ Core\", \"Widely used for imbalance\"),\n",
    "    (\"MLSEA\", \"mlsea:roc_auc\", \"üü° Recommended\", \"For probabilistic classifiers\"),\n",
    "    (\"MLSEA\", \"mlsea:precision\", \"üü° Recommended\", \"Used in evaluation\"),\n",
    "    (\"MLSEA\", \"mlsea:recall\", \"üü° Recommended\", \"Used in evaluation\"),\n",
    "\n",
    "    (\"Croissant\", \"mls:modelName\", \"‚úÖ Core\", \"Essential identifier\"),\n",
    "    (\"Croissant\", \"mls:learningAlgorithm\", \"‚úÖ Core\", \"Defines the method used\"),\n",
    "    (\"Croissant\", \"mls:hyperparameters\", \"üü° Recommended\", \"Enhances reproducibility\"),\n",
    "    (\"Croissant\", \"mls:hasInput\", \"‚úÖ Core\", \"Links to input dataset\"),\n",
    "    (\"Croissant\", \"mls:hasOutput\", \"üü° Recommended\", \"Expected result structure\"),\n",
    "\n",
    "    (\"Internal\", \"session_metadata.username\", \"‚úÖ Core\", \"Links action to a person\"),\n",
    "    (\"Internal\", \"session_metadata.role\", \"üü° Recommended\", \"Improves team attribution\"),\n",
    "    (\"Internal\", \"git_metadata.commit_hash\", \"‚úÖ Core\", \"Traceability to exact code version\"),\n",
    "    (\"Internal\", \"git_metadata.branch\", \"üü° Recommended\", \"Clarifies versioning\"),\n",
    "    (\"Internal\", \"justification.why_model\", \"üü° Recommended\", \"Supports explainability\"),\n",
    "    (\"Internal\", \"justification.why_dataset\", \"üü° Recommended\", \"Supports explainability\"),\n",
    "]\n",
    "\n",
    "# Assign weights\n",
    "weight_lookup = {\"‚úÖ Core\": 1.0, \"üü° Recommended\": 0.5, \"üü§ Optional\": 0.25}\n",
    "\n",
    "# Create DataFrame\n",
    "validated_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Standard\": std,\n",
    "        \"Field\": field,\n",
    "        \"Requirement\": level,\n",
    "        \"Weight\": weight_lookup.get(level, 0),\n",
    "        \"Justification\": just\n",
    "    } for std, field, level, just in validated_fields\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290144ce-06f2-461e-97a8-29f4503deac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load structured metadata from your run\n",
    "with open(\"../MODEL_PROVENANCE/RandomForest_Iris_v20250616_154241/structured_metadata.json\", \"r\") as f:\n",
    "    structured_metadata = json.load(f)\n",
    "\n",
    "# Flatten all fields\n",
    "flat_fields = set()\n",
    "for section, fields in structured_metadata.items():\n",
    "    if isinstance(fields, dict):\n",
    "        for key in fields:\n",
    "            if isinstance(fields[key], dict):\n",
    "                for subkey in fields[key]:\n",
    "                    flat_fields.add(f\"{key}.{subkey}\")\n",
    "            else:\n",
    "                flat_fields.add(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062c2f62-efdb-4dc6-87bd-2c4923a7cc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Metadata Coverage Summary\n",
      "Score: 15.0 / 32.25\n",
      "Coverage: 46.51%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Standard</th>\n",
       "      <th>Field</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>Captured?</th>\n",
       "      <th>Score</th>\n",
       "      <th>Max Score</th>\n",
       "      <th>Justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FAIR</td>\n",
       "      <td>dc:title</td>\n",
       "      <td>‚úÖ Core</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Essential for discovery (DCMI)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FAIR</td>\n",
       "      <td>dc:description</td>\n",
       "      <td>‚úÖ Core</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Helps users interpret dataset meaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FAIR</td>\n",
       "      <td>dc:creator</td>\n",
       "      <td>‚úÖ Core</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Attribution required by FAIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FAIR</td>\n",
       "      <td>dc:license</td>\n",
       "      <td>‚úÖ Core</td>\n",
       "      <td>‚úÖ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Required for reuse clarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FAIR</td>\n",
       "      <td>dc:publisher</td>\n",
       "      <td>üü° Recommended</td>\n",
       "      <td>‚ùå</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Common in DCAT and citation systems</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Standard           Field    Requirement Captured?  Score  Max Score  \\\n",
       "0     FAIR        dc:title         ‚úÖ Core         ‚úÖ    1.0        1.0   \n",
       "1     FAIR  dc:description         ‚úÖ Core         ‚úÖ    1.0        1.0   \n",
       "2     FAIR      dc:creator         ‚úÖ Core         ‚úÖ    1.0        1.0   \n",
       "3     FAIR      dc:license         ‚úÖ Core         ‚úÖ    1.0        1.0   \n",
       "4     FAIR    dc:publisher  üü° Recommended         ‚ùå    0.0        0.5   \n",
       "\n",
       "                           Justification  \n",
       "0         Essential for discovery (DCMI)  \n",
       "1  Helps users interpret dataset meaning  \n",
       "2           Attribution required by FAIR  \n",
       "3             Required for reuse clarity  \n",
       "4    Common in DCAT and citation systems  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare and score\n",
    "total_possible = 0\n",
    "total_achieved = 0\n",
    "comparison = []\n",
    "\n",
    "for _, row in validated_df.iterrows():\n",
    "    field = row[\"Field\"]\n",
    "    found = \"‚úÖ\" if field in flat_fields else \"‚ùå\"\n",
    "    score = row[\"Weight\"] if found == \"‚úÖ\" else 0\n",
    "    total_possible += row[\"Weight\"]\n",
    "    total_achieved += score\n",
    "    comparison.append({\n",
    "        \"Standard\": row[\"Standard\"],\n",
    "        \"Field\": field,\n",
    "        \"Requirement\": row[\"Requirement\"],\n",
    "        \"Captured?\": found,\n",
    "        \"Score\": score,\n",
    "        \"Max Score\": row[\"Weight\"],\n",
    "        \"Justification\": row[\"Justification\"]\n",
    "    })\n",
    "\n",
    "# Create results DataFrame and summary\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "\n",
    "print(\"üîé Metadata Coverage Summary\")\n",
    "print(f\"Score: {total_achieved} / {total_possible}\")\n",
    "print(f\"Coverage: {round((total_achieved / total_possible) * 100, 2)}%\")\n",
    "comparison_df.head()  # Display first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a29b8-1346-4b67-8d13-fe3f1c4872aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
