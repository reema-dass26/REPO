{
  "@context": {
    "params.criterion": {
      "@id": "params.criterion"
    },
    "tags.justification_dataset_version": {
      "@id": "tags.justification_dataset_version"
    },
    "params.database.id": {
      "@id": "params.database.id"
    },
    "metrics.precision_score_X_test": {
      "@id": "metrics.precision_score_X_test"
    },
    "params.oob_score": {
      "@id": "params.oob_score"
    },
    "run_name": {
      "@id": "run_name"
    },
    "invenio_metadata.files.url": {
      "@id": "invenio_metadata.files.url"
    },
    "tags.notebook_name": {
      "@id": "tags.notebook_name"
    },
    "params.ccp_alpha": {
      "@id": "params.ccp_alpha"
    },
    "metrics.training_precision_score": {
      "@id": "metrics.training_precision_score"
    },
    "tags.dbrepo.repository_name": {
      "@id": "tags.dbrepo.repository_name"
    },
    "artifacts.content": {
      "@id": "artifacts.content"
    },
    "params.max_depth": {
      "@id": "params.max_depth"
    },
    "invenio_metadata.publication_date": {
      "@id": "invenio_metadata.publication_date"
    },
    "params.n_train_samples": {
      "@id": "params.n_train_samples"
    },
    "invenio_metadata.version_info.is_latest_draft": {
      "@id": "invenio_metadata.version_info.is_latest_draft"
    },
    "metrics.training_log_loss": {
      "@id": "metrics.training_log_loss"
    },
    "tags.justification_max_depth": {
      "@id": "tags.justification_max_depth"
    },
    "artifacts": {
      "@id": "artifacts"
    },
    "params.class_weight": {
      "@id": "params.class_weight"
    },
    "params.columns_raw": {
      "@id": "params.columns_raw"
    },
    "invenio_metadata": {
      "@id": "invenio_metadata"
    },
    "params.min_impurity_decrease": {
      "@id": "params.min_impurity_decrease"
    },
    "metrics.dbrepo.row_count_start": {
      "@id": "metrics.dbrepo.row_count_start"
    },
    "params.retrieval_time": {
      "@id": "params.retrieval_time"
    },
    "params.warm_start": {
      "@id": "params.warm_start"
    },
    "params.python_version": {
      "@id": "params.python_version"
    },
    "tags": {
      "@id": "tags"
    },
    "params.dataset.doi": {
      "@id": "params.dataset.doi"
    },
    "tags.justification_test_split": {
      "@id": "tags.justification_test_split"
    },
    "invenio_metadata.pids.oai.provider": {
      "@id": "invenio_metadata.pids.oai.provider"
    },
    "tags.mlflow.runName": {
      "@id": "tags.mlflow.runName"
    },
    "metrics.f1_score_X_test": {
      "@id": "metrics.f1_score_X_test"
    },
    "params.max_features": {
      "@id": "params.max_features"
    },
    "tags.estimator_name": {
      "@id": "tags.estimator_name"
    },
    "params.shap_version": {
      "@id": "params.shap_version"
    },
    "tags.justification_metric_choice": {
      "@id": "tags.justification_metric_choice"
    },
    "tags.dataset_id": {
      "@id": "tags.dataset_id"
    },
    "tags.mlflow.user": {
      "@id": "tags.mlflow.user"
    },
    "tags.mlflow.source.name": {
      "@id": "tags.mlflow.source.name"
    },
    "metrics.accuracy": {
      "@id": "metrics.accuracy"
    },
    "invenio_metadata.version_info": {
      "@id": "invenio_metadata.version_info"
    },
    "tags.justification_experiment_name": {
      "@id": "tags.justification_experiment_name"
    },
    "params.numpy_version": {
      "@id": "params.numpy_version"
    },
    "metrics.roc_auc": {
      "@id": "metrics.roc_auc"
    },
    "tags.dbrepo.table_last_modified": {
      "@id": "tags.dbrepo.table_last_modified"
    },
    "tags.training_start_time": {
      "@id": "tags.training_start_time"
    },
    "metrics.roc_auc_score_X_test": {
      "@id": "metrics.roc_auc_score_X_test"
    },
    "params.max_leaf_nodes": {
      "@id": "params.max_leaf_nodes"
    },
    "tags.justification_criterion": {
      "@id": "tags.justification_criterion"
    },
    "invenio_metadata.status": {
      "@id": "invenio_metadata.status"
    },
    "tags.data_source": {
      "@id": "tags.data_source"
    },
    "invenio_metadata.files.mimetype": {
      "@id": "invenio_metadata.files.mimetype"
    },
    "params.feature_names": {
      "@id": "params.feature_names"
    },
    "invenio_metadata.files.size": {
      "@id": "invenio_metadata.files.size"
    },
    "invenio_metadata.creator": {
      "@id": "invenio_metadata.creator"
    },
    "params.dataset.publisher": {
      "@id": "params.dataset.publisher"
    },
    "params.verbose": {
      "@id": "params.verbose"
    },
    "start_time": {
      "@id": "prov:startedAtTime",
      "@type": "xsd:dateTime"
    },
    "invenio_metadata.files.checksum": {
      "@id": "invenio_metadata.files.checksum"
    },
    "tags.git_previous_commit_hash": {
      "@id": "tags.git_previous_commit_hash"
    },
    "tags.model_name": {
      "@id": "tags.model_name"
    },
    "tags.justification_random_state": {
      "@id": "tags.justification_random_state"
    },
    "tags.mlflow.source.type": {
      "@id": "tags.mlflow.source.type"
    },
    "invenio_metadata.files.key": {
      "@id": "invenio_metadata.files.key"
    },
    "metrics.dbrepo.row_count_end": {
      "@id": "metrics.dbrepo.row_count_end"
    },
    "invenio_metadata.files.metadata.height": {
      "@id": "invenio_metadata.files.metadata.height"
    },
    "params": {
      "@id": "params"
    },
    "invenio_metadata.pids.oai": {
      "@id": "invenio_metadata.pids.oai"
    },
    "tags.target_name": {
      "@id": "tags.target_name"
    },
    "tags.dataset_name": {
      "@id": "tags.dataset_name"
    },
    "params.matplotlib_version": {
      "@id": "params.matplotlib_version"
    },
    "params.os_platform": {
      "@id": "params.os_platform"
    },
    "end_time": {
      "@id": "prov:endedAtTime",
      "@type": "xsd:dateTime"
    },
    "tags.justification_verbose": {
      "@id": "tags.justification_verbose"
    },
    "params.random_state": {
      "@id": "params.random_state"
    },
    "params.n_records": {
      "@id": "params.n_records"
    },
    "tags.dataset_version": {
      "@id": "tags.dataset_version"
    },
    "metrics.recall_score_X_test": {
      "@id": "metrics.recall_score_X_test"
    },
    "metrics.training_score": {
      "@id": "metrics.training_score"
    },
    "invenio_metadata.views": {
      "@id": "invenio_metadata.views"
    },
    "tags.dbrepo.granularity": {
      "@id": "tags.dbrepo.granularity"
    },
    "params.seaborn_version": {
      "@id": "params.seaborn_version"
    },
    "artifacts.uri": {
      "@id": "artifacts.uri"
    },
    "tags.justification_threshold_accuracy": {
      "@id": "tags.justification_threshold_accuracy"
    },
    "tags.training_end_time": {
      "@id": "tags.training_end_time"
    },
    "tags.justification_drop_column_X": {
      "@id": "tags.justification_drop_column_X"
    },
    "metrics.dbrepo.num_deletes": {
      "@id": "metrics.dbrepo.num_deletes"
    },
    "tags.dbrepo.protocol_version": {
      "@id": "tags.dbrepo.protocol_version"
    },
    "tags.justification_min_samples_leaf": {
      "@id": "tags.justification_min_samples_leaf"
    },
    "tags.estimator_class": {
      "@id": "tags.estimator_class"
    },
    "metrics.dbrepo.num_inserts": {
      "@id": "metrics.dbrepo.num_inserts"
    },
    "invenio_metadata.version_info.index": {
      "@id": "invenio_metadata.version_info.index"
    },
    "run_id": {
      "@id": "run_id"
    },
    "params.dropped_columns": {
      "@id": "params.dropped_columns"
    },
    "tags.justification_class_weight": {
      "@id": "tags.justification_class_weight"
    },
    "params.n_test_samples": {
      "@id": "params.n_test_samples"
    },
    "params.test_size": {
      "@id": "params.test_size"
    },
    "invenio_metadata.pids": {
      "@id": "invenio_metadata.pids"
    },
    "tags.git_current_commit_hash": {
      "@id": "tags.git_current_commit_hash"
    },
    "params.database.owner": {
      "@id": "params.database.owner"
    },
    "params.bootstrap": {
      "@id": "params.bootstrap"
    },
    "metrics.precision_macro": {
      "@id": "metrics.precision_macro"
    },
    "tags.git__current_commit_url": {
      "@id": "tags.git__current_commit_url"
    },
    "artifacts.type": {
      "@id": "artifacts.type"
    },
    "invenio_metadata.files": {
      "@id": "invenio_metadata.files"
    },
    "tags.dbrepo.admin_email": {
      "@id": "tags.dbrepo.admin_email"
    },
    "invenio_metadata.title": {
      "@id": "invenio_metadata.title"
    },
    "tags.dbrepo.base_url": {
      "@id": "tags.dbrepo.base_url"
    },
    "tags.justification_model_choice": {
      "@id": "tags.justification_model_choice"
    },
    "params.n_features": {
      "@id": "params.n_features"
    },
    "invenio_metadata.id": {
      "@id": "invenio_metadata.id"
    },
    "metrics.training_roc_auc": {
      "@id": "metrics.training_roc_auc"
    },
    "tags.justification_min_samples_split": {
      "@id": "tags.justification_min_samples_split"
    },
    "params.min_samples_leaf": {
      "@id": "params.min_samples_leaf"
    },
    "invenio_metadata.files.metadata.width": {
      "@id": "invenio_metadata.files.metadata.width"
    },
    "params.n_estimators": {
      "@id": "params.n_estimators"
    },
    "params.database.name": {
      "@id": "params.database.name"
    },
    "params.dataset.authors": {
      "@id": "params.dataset.authors"
    },
    "tags.justification_target_variable": {
      "@id": "tags.justification_target_variable"
    },
    "params.sklearn_version": {
      "@id": "params.sklearn_version"
    },
    "metrics.f1_macro": {
      "@id": "metrics.f1_macro"
    },
    "experiment_id": {
      "@id": "experiment_id"
    },
    "params.dataset.published": {
      "@id": "params.dataset.published"
    },
    "metrics.training_accuracy_score": {
      "@id": "metrics.training_accuracy_score"
    },
    "tags.justification_max_features": {
      "@id": "tags.justification_max_features"
    },
    "tags.justification_bootstrap": {
      "@id": "tags.justification_bootstrap"
    },
    "tags.justification_oob_score": {
      "@id": "tags.justification_oob_score"
    },
    "metrics.accuracy_score_X_test": {
      "@id": "metrics.accuracy_score_X_test"
    },
    "params.min_weight_fraction_leaf": {
      "@id": "params.min_weight_fraction_leaf"
    },
    "params.n_jobs": {
      "@id": "params.n_jobs"
    },
    "invenio_metadata.pids.oai.identifier": {
      "@id": "invenio_metadata.pids.oai.identifier"
    },
    "params.dataset.title": {
      "@id": "params.dataset.title"
    },
    "params.database.description": {
      "@id": "params.database.description"
    },
    "metrics": {
      "@id": "metrics"
    },
    "params.min_samples_split": {
      "@id": "params.min_samples_split"
    },
    "metrics.recall_macro": {
      "@id": "metrics.recall_macro"
    },
    "tags.justification_n_jobs": {
      "@id": "tags.justification_n_jobs"
    },
    "artifacts.path": {
      "@id": "artifacts.path"
    },
    "metrics.training_recall_score": {
      "@id": "metrics.training_recall_score"
    },
    "params.max_samples": {
      "@id": "params.max_samples"
    },
    "tags.mlflow.log-model.history": {
      "@id": "tags.mlflow.log-model.history"
    },
    "invenio_metadata.files.metadata": {
      "@id": "invenio_metadata.files.metadata"
    },
    "metrics.training_f1_score": {
      "@id": "metrics.training_f1_score"
    },
    "params.pandas_version": {
      "@id": "params.pandas_version"
    },
    "params.n_features_final": {
      "@id": "params.n_features_final"
    },
    "invenio_metadata.version_info.is_latest": {
      "@id": "invenio_metadata.version_info.is_latest"
    },
    "invenio_metadata.downloads": {
      "@id": "invenio_metadata.downloads"
    },
    "tags.justification_n_estimators": {
      "@id": "tags.justification_n_estimators"
    }
  },
  "run_id": "8f7521eaa562415d9a450f4167a127ab",
  "run_name": "defiant-bat-29",
  "experiment_id": "615223710259862608",
  "params": {
    "bootstrap": "True",
    "ccp_alpha": "0.0",
    "class_weight": "None",
    "columns_raw": "['id', 'sepallengthcm', 'sepalwidthcm', 'petallengthcm', 'petalwidthcm', 'species']",
    "criterion": "entropy",
    "database.description": "None",
    "database.id": "c3a42d17-42b7-43c9-a504-2363fb4c9c8d",
    "database.name": "Iris",
    "database.owner": "reema",
    "dataset.authors": "[\"Marshall Michael\"]",
    "dataset.doi": "10.5281/ZENODO.1404173",
    "dataset.published": "2018-8-27",
    "dataset.publisher": "Zenodo",
    "dataset.title": "Scikit-Learn Iris",
    "dropped_columns": "['id']",
    "feature_names": "['sepallengthcm', 'sepalwidthcm', 'petallengthcm', 'petalwidthcm']",
    "matplotlib_version": "3.7.2",
    "max_depth": "12",
    "max_features": "sqrt",
    "max_leaf_nodes": "None",
    "max_samples": "None",
    "min_impurity_decrease": "0.0",
    "min_samples_leaf": "2",
    "min_samples_split": "5",
    "min_weight_fraction_leaf": "0.0",
    "numpy_version": "1.24.4",
    "n_estimators": "200",
    "n_features": "4",
    "n_features_final": "4",
    "n_jobs": "-1",
    "n_records": "150",
    "n_test_samples": "30",
    "n_train_samples": "120",
    "oob_score": "False",
    "os_platform": "Windows 10",
    "pandas_version": "2.2.3",
    "python_version": "3.11.5",
    "random_state": "42",
    "retrieval_time": "2025-04-25T11:14:07.431218",
    "seaborn_version": "0.12.2",
    "shap_version": "0.47.1",
    "sklearn_version": "1.3.0",
    "test_size": "0.2",
    "verbose": "1",
    "warm_start": "False"
  },
  "metrics": {
    "accuracy": 1.0,
    "dbrepo.num_deletes": 0.0,
    "dbrepo.num_inserts": 1.0,
    "dbrepo.row_count_end": 150.0,
    "dbrepo.row_count_start": 150.0,
    "f1_macro": 1.0,
    "precision_macro": 1.0,
    "recall_macro": 1.0,
    "roc_auc": 1.0,
    "training_accuracy_score": 0.9666666666666667,
    "training_f1_score": 0.9666666666666667,
    "training_log_loss": 0.06535223011958341,
    "training_precision_score": 0.9674588284344383,
    "training_recall_score": 0.9666666666666667,
    "training_roc_auc": 0.9987492182614135,
    "training_score": 0.9666666666666667
  },
  "artifacts": [
    {
      "path": "RandomForest_Iris_v20250425_131407/RandomForest_Iris_v20250425_131407.pkl",
      "type": "other",
      "uri": "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/RandomForest_Iris_v20250425_131407/RandomForest_Iris_v20250425_131407.pkl"
    },
    {
      "path": "commit_diff.json",
      "type": "text",
      "content": "{\n  \"previous_commit\": \"e67d756afe694e01d42bbac8ab69de73007f473a\",\n  \"previous_commit_url\": \"https://github.com/reema-dass26/REPO/commit/e67d756afe694e01d42bbac8ab69de73007f473a\",\n  \"current_commit_url\": \"https://github.com/reema-dass26/REPO/commit/ac1670d33efd1522e7eb68bb44b935d9574f8bf0\",\n  \"current_commit\": \"ac1670d33efd1522e7eb68bb44b935d9574f8bf0\",\n  \"diff\": \"diff --git a/notebooks/RQ_notebooks/RQ1_updated-Backup.ipynb b/notebooks/RQ_notebooks/RQ1_updated-Backup.ipynb\\nnew file mode 100644\\nindex 0000000..1ff819b\\n--- /dev/null\\n+++ b/notebooks/RQ_notebooks/RQ1_updated-Backup.ipynb\\n@@ -0,0 +1,3270 @@\\n+{\\n+ \\\"cells\\\": [\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 8,\\n+   \\\"id\\\": \\\"12fa6f59-927c-4003-964f-83e53793fd36\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"# TODO: atm the mlflow autolog isnt capturing metrics n params\\\\n\\\",\\n+    \\\"# and sklearn.autolog throws error( posted the issue on github)\\\\n\\\",\\n+    \\\"# Ideally, I should be able to fetch most of the imp detail via MLFLOW AUTOLOG. will check that later in time\\\\n\\\",\\n+    \\\"#============================\\\\n\\\",\\n+    \\\"# \\ud83e\\udde0 MLflow Autologging\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# mlflow.autolog(log_input_examples=True, log_model_signatures=True)\\\\n\\\",\\n+    \\\"# mlflow.sklearn.autolog() \\\\n\\\",\\n+    \\\"# mlflow.sklearn.autolog(\\\\n\\\",\\n+    \\\"#     log_input_examples=True,\\\\n\\\",\\n+    \\\"#     log_model_signatures=True,\\\\n\\\",\\n+    \\\"#     log_post_training_metrics=True,        # calls model.score() \\u2192 accuracy\\\\n\\\",\\n+    \\\"#     disable_for_unsupported_versions=True,  # skips if versions still wonky\\\\n\\\",\\n+    \\\"#     exclusive=True                          # only patch the sklearn integration\\\\n\\\",\\n+    \\\"# )\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 9,\\n+   \\\"id\\\": \\\"1ce1a579-f08b-40bd-b4db-21b388aaea74\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\u2699\\ufe0f Install Dependencies (if needed )\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# !pip install mlflow scikit-learn pandas numpy matplotlib seaborn shap requests GitPython\\\\n\\\",\\n+    \\\"# !pip install --upgrade threadpoolctl\\\\n\\\",\\n+    \\\"# !pip install setuptools\\\\n\\\",\\n+    \\\"# !pip install ace_tools \\\\n\\\",\\n+    \\\"# !pip install rdflib\\\\n\\\",\\n+    \\\"# !pip install streamlit-option-menu\\\\n\\\",\\n+    \\\"# !pip install streamlit-agraph\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"1ca4f0ae-39ee-4b22-b013-dfb1fa1b5694\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"LIBRARY IMPORTS:\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 51,\\n+   \\\"id\\\": \\\"8ca332e5-6501-4310-920b-2b769477b46e\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83d\\udce6 Standard Library Imports\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"import os\\\\n\\\",\\n+    \\\"import glob\\\\n\\\",\\n+    \\\"import io\\\\n\\\",\\n+    \\\"import json\\\\n\\\",\\n+    \\\"import time\\\\n\\\",\\n+    \\\"import ast\\\\n\\\",\\n+    \\\"import pickle\\\\n\\\",\\n+    \\\"import platform\\\\n\\\",\\n+    \\\"import subprocess\\\\n\\\",\\n+    \\\"from datetime import datetime, timezone\\\\n\\\",\\n+    \\\"from pprint import pprint\\\\n\\\",\\n+    \\\"from typing import List, Dict, Any\\\\n\\\",\\n+    \\\"import xml.etree.ElementTree as ET\\\\n\\\",\\n+    \\\"import urllib.parse\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83d\\udcca Data and Visualization\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"import pandas as pd\\\\n\\\",\\n+    \\\"import numpy as np\\\\n\\\",\\n+    \\\"import seaborn as sns\\\\n\\\",\\n+    \\\"import matplotlib\\\\n\\\",\\n+    \\\"import matplotlib.pyplot as plt\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83e\\udd16 Machine Learning\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"import sklearn\\\\n\\\",\\n+    \\\"from sklearn.datasets import load_iris\\\\n\\\",\\n+    \\\"from sklearn.ensemble import RandomForestClassifier\\\\n\\\",\\n+    \\\"from sklearn.model_selection import train_test_split\\\\n\\\",\\n+    \\\"from sklearn.preprocessing import LabelEncoder, label_binarize\\\\n\\\",\\n+    \\\"from sklearn.metrics import (\\\\n\\\",\\n+    \\\"    accuracy_score,\\\\n\\\",\\n+    \\\"    roc_auc_score,\\\\n\\\",\\n+    \\\"    confusion_matrix,\\\\n\\\",\\n+    \\\"    precision_score,\\\\n\\\",\\n+    \\\"    recall_score,\\\\n\\\",\\n+    \\\"    f1_score,\\\\n\\\",\\n+    \\\"    RocCurveDisplay,\\\\n\\\",\\n+    \\\"    PrecisionRecallDisplay\\\\n\\\",\\n+    \\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83d\\udd2c Experiment Tracking\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"import mlflow\\\\n\\\",\\n+    \\\"import mlflow.sklearn\\\\n\\\",\\n+    \\\"from mlflow import MlflowClient\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83c\\udf10 Web / API / Networking\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"import requests\\\\n\\\",\\n+    \\\"from dotenv import load_dotenv\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83e\\uddea Git & Version Control\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"import git\\\\n\\\",\\n+    \\\"from git import Repo, GitCommandError\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83e\\udde0 SHAP for Explainability\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"import shap\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83e\\uddec RDF & Provenance (rdflib)\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"from rdflib import Graph, URIRef, Literal\\\\n\\\",\\n+    \\\"from rdflib.namespace import PROV, XSD\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\u2699\\ufe0f System Monitoring\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"import psutil\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"61d4d6b8-34a9-47b5-974d-5927c0ee2256\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"DBREPO INTEGRETION\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 32,\\n+   \\\"id\\\": \\\"8e3570e2-9a60-45b4-8653-28060071e728\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"API Response: [{'id': '1', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '2', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '3', 'sepallengthcm': '4.700000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '4', 'sepallengthcm': '4.600000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '5', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.600000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '6', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.900000000000000000', 'petallengthcm': '1.700000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '7', 'sepallengthcm': '4.600000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '8', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '9', 'sepallengthcm': '4.400000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '10', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '11', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.700000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '12', 'sepallengthcm': '4.800000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '13', 'sepallengthcm': '4.800000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '14', 'sepallengthcm': '4.300000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.100000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '15', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '4.000000000000000000', 'petallengthcm': '1.200000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '16', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '4.400000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '17', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.900000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '18', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '19', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '1.700000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '20', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '21', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.700000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '22', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.700000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '23', 'sepallengthcm': '4.600000000000000000', 'sepalwidthcm': '3.600000000000000000', 'petallengthcm': '1.000000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '24', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '1.700000000000000000', 'petalwidthcm': '0.500000000000000000', 'species': 'Iris-setosa'}, {'id': '25', 'sepallengthcm': '4.800000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.900000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '26', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '27', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '28', 'sepallengthcm': '5.200000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '29', 'sepallengthcm': '5.200000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '30', 'sepallengthcm': '4.700000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '31', 'sepallengthcm': '4.800000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '32', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '33', 'sepallengthcm': '5.200000000000000000', 'sepalwidthcm': '4.100000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '34', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '4.200000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '35', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '36', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '1.200000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '37', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '38', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '39', 'sepallengthcm': '4.400000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '40', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '41', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '42', 'sepallengthcm': '4.500000000000000000', 'sepalwidthcm': '2.300000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '43', 'sepallengthcm': '4.400000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '44', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.600000000000000000', 'species': 'Iris-setosa'}, {'id': '45', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '1.900000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '46', 'sepallengthcm': '4.800000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '47', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '48', 'sepallengthcm': '4.600000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '49', 'sepallengthcm': '5.300000000000000000', 'sepalwidthcm': '3.700000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '50', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '51', 'sepallengthcm': '7.000000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '4.700000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '52', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '53', 'sepallengthcm': '6.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '4.900000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '54', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '2.300000000000000000', 'petallengthcm': '4.000000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '55', 'sepallengthcm': '6.500000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.600000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '56', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '57', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '4.700000000000000000', 'petalwidthcm': '1.600000000000000000', 'species': 'Iris-versicolor'}, {'id': '58', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '2.400000000000000000', 'petallengthcm': '3.300000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '59', 'sepallengthcm': '6.600000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.600000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '60', 'sepallengthcm': '5.200000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '3.900000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '61', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '2.000000000000000000', 'petallengthcm': '3.500000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '62', 'sepallengthcm': '5.900000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.200000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '63', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '2.200000000000000000', 'petallengthcm': '4.000000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '64', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.700000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '65', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '3.600000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '66', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '4.400000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '67', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '68', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '4.100000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '69', 'sepallengthcm': '6.200000000000000000', 'sepalwidthcm': '2.200000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '70', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '3.900000000000000000', 'petalwidthcm': '1.100000000000000000', 'species': 'Iris-versicolor'}, {'id': '71', 'sepallengthcm': '5.900000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '4.800000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-versicolor'}, {'id': '72', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.000000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '73', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '4.900000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '74', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.700000000000000000', 'petalwidthcm': '1.200000000000000000', 'species': 'Iris-versicolor'}, {'id': '75', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.300000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '76', 'sepallengthcm': '6.600000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.400000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '77', 'sepallengthcm': '6.800000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.800000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '78', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.000000000000000000', 'petalwidthcm': '1.700000000000000000', 'species': 'Iris-versicolor'}, {'id': '79', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '80', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '2.600000000000000000', 'petallengthcm': '3.500000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '81', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '2.400000000000000000', 'petallengthcm': '3.800000000000000000', 'petalwidthcm': '1.100000000000000000', 'species': 'Iris-versicolor'}, {'id': '82', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '2.400000000000000000', 'petallengthcm': '3.700000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '83', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '3.900000000000000000', 'petalwidthcm': '1.200000000000000000', 'species': 'Iris-versicolor'}, {'id': '84', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '1.600000000000000000', 'species': 'Iris-versicolor'}, {'id': '85', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '86', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.600000000000000000', 'species': 'Iris-versicolor'}, {'id': '87', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '4.700000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '88', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.300000000000000000', 'petallengthcm': '4.400000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '89', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.100000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '90', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '4.000000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '91', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '2.600000000000000000', 'petallengthcm': '4.400000000000000000', 'petalwidthcm': '1.200000000000000000', 'species': 'Iris-versicolor'}, {'id': '92', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.600000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '93', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.600000000000000000', 'petallengthcm': '4.000000000000000000', 'petalwidthcm': '1.200000000000000000', 'species': 'Iris-versicolor'}, {'id': '94', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '2.300000000000000000', 'petallengthcm': '3.300000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '95', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '4.200000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '96', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.200000000000000000', 'petalwidthcm': '1.200000000000000000', 'species': 'Iris-versicolor'}, {'id': '97', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.200000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '98', 'sepallengthcm': '6.200000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.300000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '99', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '3.000000000000000000', 'petalwidthcm': '1.100000000000000000', 'species': 'Iris-versicolor'}, {'id': '100', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.100000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '101', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '6.000000000000000000', 'petalwidthcm': '2.500000000000000000', 'species': 'Iris-virginica'}, {'id': '102', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '1.900000000000000000', 'species': 'Iris-virginica'}, {'id': '103', 'sepallengthcm': '7.100000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.900000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '104', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '105', 'sepallengthcm': '6.500000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.800000000000000000', 'petalwidthcm': '2.200000000000000000', 'species': 'Iris-virginica'}, {'id': '106', 'sepallengthcm': '7.600000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '6.600000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '107', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.700000000000000000', 'species': 'Iris-virginica'}, {'id': '108', 'sepallengthcm': '7.300000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '6.300000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '109', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '5.800000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '110', 'sepallengthcm': '7.200000000000000000', 'sepalwidthcm': '3.600000000000000000', 'petallengthcm': '6.100000000000000000', 'petalwidthcm': '2.500000000000000000', 'species': 'Iris-virginica'}, {'id': '111', 'sepallengthcm': '6.500000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '112', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '5.300000000000000000', 'petalwidthcm': '1.900000000000000000', 'species': 'Iris-virginica'}, {'id': '113', 'sepallengthcm': '6.800000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.500000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '114', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '5.000000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '115', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '2.400000000000000000', 'species': 'Iris-virginica'}, {'id': '116', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '5.300000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '117', 'sepallengthcm': '6.500000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.500000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '118', 'sepallengthcm': '7.700000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '6.700000000000000000', 'petalwidthcm': '2.200000000000000000', 'species': 'Iris-virginica'}, {'id': '119', 'sepallengthcm': '7.700000000000000000', 'sepalwidthcm': '2.600000000000000000', 'petallengthcm': '6.900000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '120', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '2.200000000000000000', 'petallengthcm': '5.000000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-virginica'}, {'id': '121', 'sepallengthcm': '6.900000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '5.700000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '122', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.900000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '123', 'sepallengthcm': '7.700000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '6.700000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '124', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '4.900000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '125', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '5.700000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '126', 'sepallengthcm': '7.200000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '6.000000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '127', 'sepallengthcm': '6.200000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.800000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '128', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.900000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '129', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '130', 'sepallengthcm': '7.200000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.800000000000000000', 'petalwidthcm': '1.600000000000000000', 'species': 'Iris-virginica'}, {'id': '131', 'sepallengthcm': '7.400000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '6.100000000000000000', 'petalwidthcm': '1.900000000000000000', 'species': 'Iris-virginica'}, {'id': '132', 'sepallengthcm': '7.900000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '6.400000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '133', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '2.200000000000000000', 'species': 'Iris-virginica'}, {'id': '134', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-virginica'}, {'id': '135', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '2.600000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-virginica'}, {'id': '136', 'sepallengthcm': '7.700000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '6.100000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '137', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '2.400000000000000000', 'species': 'Iris-virginica'}, {'id': '138', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '5.500000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '139', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.800000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '140', 'sepallengthcm': '6.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '5.400000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '141', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '2.400000000000000000', 'species': 'Iris-virginica'}, {'id': '142', 'sepallengthcm': '6.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '143', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '1.900000000000000000', 'species': 'Iris-virginica'}, {'id': '144', 'sepallengthcm': '6.800000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '5.900000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '145', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '5.700000000000000000', 'petalwidthcm': '2.500000000000000000', 'species': 'Iris-virginica'}, {'id': '146', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.200000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '147', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '5.000000000000000000', 'petalwidthcm': '1.900000000000000000', 'species': 'Iris-virginica'}, {'id': '148', 'sepallengthcm': '6.500000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.200000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '149', 'sepallengthcm': '6.200000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '5.400000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '150', 'sepallengthcm': '5.900000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}]\\\\n\\\",\\n+      \\\"<built-in method count of list object at 0x000001B2518C1700>\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"# API endpoint URL\\\\n\\\",\\n+    \\\"API_URL = \\\\\\\"http://localhost/api/database/c3a42d17-42b7-43c9-a504-2363fb4c9c8d/table/5315e7da-64fb-4fdb-b493-95b4138c765f/data?size=100000&page=0\\\\\\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Define the headers\\\\n\\\",\\n+    \\\"headers = {\\\\n\\\",\\n+    \\\"    \\\\\\\"Accept\\\\\\\": \\\\\\\"application/json\\\\\\\"  # Specify the expected response format\\\\n\\\",\\n+    \\\"}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"try:\\\\n\\\",\\n+    \\\"    # Send a GET request to the API with the Accept header\\\\n\\\",\\n+    \\\"    response = requests.get(API_URL, headers=headers)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # Check if the request was successful\\\\n\\\",\\n+    \\\"    if response.status_code == 200:\\\\n\\\",\\n+    \\\"        # Parse the JSON response\\\\n\\\",\\n+    \\\"        dataset = response.json()\\\\n\\\",\\n+    \\\"        print(\\\\\\\"API Response:\\\\\\\", dataset)\\\\n\\\",\\n+    \\\"        print( dataset.count)\\\\n\\\",\\n+    \\\"    else:\\\\n\\\",\\n+    \\\"        print(f\\\\\\\"Error: Received status code {response.status_code}\\\\\\\")\\\\n\\\",\\n+    \\\"        print(\\\\\\\"Response content:\\\\\\\", response.text)\\\\n\\\",\\n+    \\\"       \\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"except requests.exceptions.RequestException as e:\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"Request failed: {e}\\\\\\\")\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"09557f94-325c-4bd6-882a-069a9e3c5ecd\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"replacing dynamic fetching of data When and if DBREPO isnt running \\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 11,\\n+   \\\"id\\\": \\\"ce6e020d-cb80-49ec-8bcc-687b1e08885c\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"# 1. Read the JSON file id the API isnt available this data is saved locally but the data is from the API endpoint\\\\n\\\",\\n+    \\\"with open(\\\\\\\"iris_data.json\\\\\\\", \\\\\\\"r\\\\\\\") as f:\\\\n\\\",\\n+    \\\"    dataset = json.load(f)\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"a6c6007a-2126-4b1a-90ee-3326eb39a362\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"Metadata fetching from db repo API CALLS\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"9165f478-a44e-4125-8929-a8d77fdcb4c5\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"METADATA ON DATABASE LEVEL\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 33,\\n+   \\\"id\\\": \\\"abe912e7-bf9b-4bbd-8e43-6046745ade3f\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"DB_API = \\\\\\\"http://localhost/api/database/{db_id}\\\\\\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def fetch_db_metadata(db_id: str) -> dict:\\\\n\\\",\\n+    \\\"    url = DB_API.format(db_id=db_id)\\\\n\\\",\\n+    \\\"    try:\\\\n\\\",\\n+    \\\"        resp = requests.get(url)\\\\n\\\",\\n+    \\\"        resp.raise_for_status()\\\\n\\\",\\n+    \\\"        return resp.json()\\\\n\\\",\\n+    \\\"    except requests.exceptions.RequestException as e:\\\\n\\\",\\n+    \\\"        print(f\\\\\\\"[\\u26a0\\ufe0f Error] Failed to fetch DB metadata for {db_id}: {e}\\\\\\\")\\\\n\\\",\\n+    \\\"        return {}  # or return None, depending on what your app prefers\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def log_db_metadata(db_meta: dict):\\\\n\\\",\\n+    \\\"    # 1) Core DB fields as params, defaulting to empty string if key is missing\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"database.id\\\\\\\",          db_meta.get(\\\\\\\"id\\\\\\\", \\\\\\\"\\\\\\\"))\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"database.name\\\\\\\",        db_meta.get(\\\\\\\"name\\\\\\\", \\\\\\\"\\\\\\\"))\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"database.description\\\\\\\", db_meta.get(\\\\\\\"description\\\\\\\", \\\\\\\"\\\\\\\"))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 2) Handle nested keys safely for owner\\\\n\\\",\\n+    \\\"    try:\\\\n\\\",\\n+    \\\"        owner = db_meta.get(\\\\\\\"tables\\\\\\\", [{}])[0].get(\\\\\\\"owner\\\\\\\", {}).get(\\\\\\\"username\\\\\\\", \\\\\\\"\\\\\\\")\\\\n\\\",\\n+    \\\"    except Exception:\\\\n\\\",\\n+    \\\"        owner = \\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"database.owner\\\\\\\", owner)\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"53cbd7eb-0d97-4326-9bfc-f6fcee14ef9c\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"MATADATA FROM: <ns0:OAI-PMH xmlns:ns0=\\\\\\\"http://www.openarchives.org/OAI/2.0/\\\\\\\" xmlns:xsi=\\\\\\\"http://www.w3.org/2001/XMLSchema-instance\\\\\\\" xsi:schemaLocation=\\\\\\\"http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd\\\\\\\">\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 34,\\n+   \\\"id\\\": \\\"296f307e-e01b-477a-9406-92cab9f2d7bf\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"<ns0:OAI-PMH xmlns:ns0=\\\\\\\"http://www.openarchives.org/OAI/2.0/\\\\\\\" xmlns:xsi=\\\\\\\"http://www.w3.org/2001/XMLSchema-instance\\\\\\\" xsi:schemaLocation=\\\\\\\"http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd\\\\\\\">\\\\n\\\",\\n+      \\\"    <ns0:responseDate>2025-04-25T09:55:50Z</ns0:responseDate>\\\\n\\\",\\n+      \\\"    <ns0:request verb=\\\\\\\"Identify\\\\\\\">https://localhost/api/oai</ns0:request>\\\\n\\\",\\n+      \\\"    <ns0:Identify>\\\\n\\\",\\n+      \\\"    <ns0:repositoryName>Database Repository</ns0:repositoryName>\\\\n\\\",\\n+      \\\"    <ns0:baseURL>http://localhost</ns0:baseURL>\\\\n\\\",\\n+      \\\"    <ns0:protocolVersion>2.0</ns0:protocolVersion>\\\\n\\\",\\n+      \\\"    <ns0:adminEmail>noreply@localhost</ns0:adminEmail>\\\\n\\\",\\n+      \\\"    <ns0:earliestDatestamp />\\\\n\\\",\\n+      \\\"    <ns0:deletedRecord>persistent</ns0:deletedRecord>\\\\n\\\",\\n+      \\\"    <ns0:granularity>YYYY-MM-DDThh:mm:ssZ</ns0:granularity>\\\\n\\\",\\n+      \\\"</ns0:Identify>\\\\n\\\",\\n+      \\\"</ns0:OAI-PMH>\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"# 1) Fetch your database metadata\\\\n\\\",\\n+    \\\"db_url = \\\\\\\"http://localhost/api/database/c3a42d17-42b7-43c9-a504-2363fb4c9c8d\\\\\\\"\\\\n\\\",\\n+    \\\"db_resp = requests.get(db_url)\\\\n\\\",\\n+    \\\"db_resp.raise_for_status()\\\\n\\\",\\n+    \\\"db_data = db_resp.json()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"db_id  = db_data[\\\\\\\"id\\\\\\\"]\\\\n\\\",\\n+    \\\"tbl_id = db_data[\\\\\\\"tables\\\\\\\"][0][\\\\\\\"id\\\\\\\"]\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# 2) Build the OAI-PMH URL, URL-encoding the `set` param\\\\n\\\",\\n+    \\\"set_param   = f\\\\\\\"Databases/{db_id}/Tables/{tbl_id}\\\\\\\"\\\\n\\\",\\n+    \\\"encoded_set = urllib.parse.quote(set_param, safe=\\\\\\\"\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"oai_url = (\\\\n\\\",\\n+    \\\"    \\\\\\\"http://localhost/api/oai\\\\\\\"\\\\n\\\",\\n+    \\\"    f\\\\\\\"?metadataPrefix=oai_dc\\\\\\\"\\\\n\\\",\\n+    \\\"    f\\\\\\\"&from=2025-03-01\\\\\\\"\\\\n\\\",\\n+    \\\"    f\\\\\\\"&until=2025-03-07\\\\\\\"\\\\n\\\",\\n+    \\\"    f\\\\\\\"&set={encoded_set}\\\\\\\"\\\\n\\\",\\n+    \\\"    f\\\\\\\"&resumptionToken=string\\\\\\\"\\\\n\\\",\\n+    \\\"    f\\\\\\\"&fromDate=2025-03-07T19%3A35%3A51.476Z\\\\\\\"\\\\n\\\",\\n+    \\\"    f\\\\\\\"&untilDate=2025-03-07T19%3A35%3A51.476Z\\\\\\\"\\\\n\\\",\\n+    \\\"    f\\\\\\\"&parametersString=string\\\\\\\"\\\\n\\\",\\n+    \\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# 3) Call and parse\\\\n\\\",\\n+    \\\"try:\\\\n\\\",\\n+    \\\"    resp = requests.get(oai_url)\\\\n\\\",\\n+    \\\"    resp.raise_for_status()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    if \\\\\\\"xml\\\\\\\" in resp.headers.get(\\\\\\\"Content-Type\\\\\\\", \\\\\\\"\\\\\\\"):\\\\n\\\",\\n+    \\\"        root = ET.fromstring(resp.text)\\\\n\\\",\\n+    \\\"        print(ET.tostring(root, encoding=\\\\\\\"utf-8\\\\\\\").decode())\\\\n\\\",\\n+    \\\"    else:\\\\n\\\",\\n+    \\\"        print(\\\\\\\"Non-XML response:\\\\\\\", resp.headers.get(\\\\\\\"Content-Type\\\\\\\"), resp.text)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"except requests.exceptions.RequestException as e:\\\\n\\\",\\n+    \\\"    print(\\\\\\\"Request failed:\\\\\\\", e)\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 35,\\n+   \\\"id\\\": \\\"61cc99ab-4a5c-4142-8725-e7c940673ffd\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"# \\u2026after you fetch & parse your XML into `root`\\u2026\\\\n\\\",\\n+    \\\"ns = {\\\\\\\"oai\\\\\\\": \\\\\\\"http://www.openarchives.org/OAI/2.0/\\\\\\\"}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"repo_name   = root.findtext(\\\\\\\"oai:Identify/oai:repositoryName\\\\\\\", namespaces=ns) or \\\\\\\"N/A\\\\\\\"\\\\n\\\",\\n+    \\\"base_url    = root.findtext(\\\\\\\"oai:Identify/oai:baseURL\\\\\\\", namespaces=ns) or \\\\\\\"N/A\\\\\\\"\\\\n\\\",\\n+    \\\"protocol    = root.findtext(\\\\\\\"oai:Identify/oai:protocolVersion\\\\\\\", namespaces=ns) or \\\\\\\"N/A\\\\\\\"\\\\n\\\",\\n+    \\\"admin_email = root.findtext(\\\\\\\"oai:Identify/oai:adminEmail\\\\\\\", namespaces=ns) or \\\\\\\"N/A\\\\\\\"\\\\n\\\",\\n+    \\\"gran        = root.findtext(\\\\\\\"oai:Identify/oai:granularity\\\\\\\", namespaces=ns) or \\\\\\\"N/A\\\\\\\"\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"74214aa7-c12f-414e-9feb-094a366b855b\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"METADATA ON History Logging\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 36,\\n+   \\\"id\\\": \\\"e9c74e9b-c9b0-4b4a-82eb-2a6e56456508\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"API Response: [{'timestamp': '2025-04-23T20:42:29.501Z', 'event': 'insert', 'total': 150}]\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"url = \\\\\\\"http://localhost/api/database/c3a42d17-42b7-43c9-a504-2363fb4c9c8d/table/5315e7da-64fb-4fdb-b493-95b4138c765f/history\\\\\\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"try:\\\\n\\\",\\n+    \\\"    # Send a GET request to the API\\\\n\\\",\\n+    \\\"    response = requests.get(url)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # Check if the request was successful\\\\n\\\",\\n+    \\\"    if response.status_code == 200:\\\\n\\\",\\n+    \\\"        # Parse the JSON response\\\\n\\\",\\n+    \\\"        data = response.json()\\\\n\\\",\\n+    \\\"        print(\\\\\\\"API Response:\\\\\\\", data)\\\\n\\\",\\n+    \\\"    else:\\\\n\\\",\\n+    \\\"        print(f\\\\\\\"Error: Received status code {response.status_code}\\\\\\\")\\\\n\\\",\\n+    \\\"        print(\\\\\\\"Response content:\\\\\\\", response.text)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"except requests.exceptions.RequestException as e:\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"Request failed: {e}\\\\\\\")\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 37,\\n+   \\\"id\\\": \\\"3630c954-5ad2-4759-b9a0-fa6e20e184ef\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"first   = data[0]\\\\n\\\",\\n+    \\\"last    = data[-1]\\\\n\\\",\\n+    \\\"count_0 = first[\\\\\\\"total\\\\\\\"]    # e.g. 149\\\\n\\\",\\n+    \\\"count_N = last[\\\\\\\"total\\\\\\\"]     # e.g. 149 again, or changed\\\\n\\\",\\n+    \\\"ts_last = last[\\\\\\\"timestamp\\\\\\\"]  # e.g. \\\\\\\"2025-03-28T17:42:38.058Z\\\\\\\"\\\\n\\\",\\n+    \\\"n_insert = sum(1 for ev in data if ev[\\\\\\\"event\\\\\\\"]==\\\\\\\"insert\\\\\\\")\\\\n\\\",\\n+    \\\"n_delete = sum(1 for ev in data if ev[\\\\\\\"event\\\\\\\"]==\\\\\\\"delete\\\\\\\")\\\\n\\\",\\n+    \\\"history = response.json()\\\\n\\\",\\n+    \\\"first, last = history[0], history[-1]\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# summary stats\\\\n\\\",\\n+    \\\"count_start = first[\\\\\\\"total\\\\\\\"]\\\\n\\\",\\n+    \\\"count_end   = last[\\\\\\\"total\\\\\\\"]\\\\n\\\",\\n+    \\\"ts_last     = last[\\\\\\\"timestamp\\\\\\\"]\\\\n\\\",\\n+    \\\"n_insert    = sum(1 for ev in history if ev[\\\\\\\"event\\\\\\\"]==\\\\\\\"insert\\\\\\\")\\\\n\\\",\\n+    \\\"n_delete    = sum(1 for ev in history if ev[\\\\\\\"event\\\\\\\"]==\\\\\\\"delete\\\\\\\")\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"1afd5dad-72d5-42e1-a0fa-b7bd3455937b\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"Dataset metadata fetching from ZONEDO or any public dataset repositories to gain more details\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 38,\\n+   \\\"id\\\": \\\"a7fa122a-c6e5-4b38-842a-dc81590a1f46\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"def fetch_and_log_dataset_metadata_nested(doi_url: str):\\\\n\\\",\\n+    \\\"    # 1) fetch the CSL+JSON\\\\n\\\",\\n+    \\\"    headers = {\\\\\\\"Accept\\\\\\\": \\\\\\\"application/vnd.citationstyles.csl+json\\\\\\\"}\\\\n\\\",\\n+    \\\"    r = requests.get(doi_url, headers=headers); r.raise_for_status()\\\\n\\\",\\n+    \\\"    meta = r.json()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 2) pull out what you care about\\\\n\\\",\\n+    \\\"    authors = [f\\\\\\\"{a.get('family','')} {a.get('given','')}\\\\\\\".strip()\\\\n\\\",\\n+    \\\"               for a in meta.get(\\\\\\\"author\\\\\\\", [])]\\\\n\\\",\\n+    \\\"    pubdate = \\\\\\\"-\\\\\\\".join(str(x) for x in meta.get(\\\\\\\"issued\\\\\\\",{}).get(\\\\\\\"date-parts\\\\\\\",[[]])[0])\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 3) assemble one nested dict\\\\n\\\",\\n+    \\\"    public_datasetRepository_metadata = {\\\\n\\\",\\n+    \\\"      \\\\\\\"zenodo\\\\\\\": {\\\\n\\\",\\n+    \\\"        \\\\\\\"title\\\\\\\":     meta.get(\\\\\\\"title\\\\\\\"),\\\\n\\\",\\n+    \\\"        \\\\\\\"doi\\\\\\\":       meta.get(\\\\\\\"DOI\\\\\\\"),\\\\n\\\",\\n+    \\\"        \\\\\\\"authors\\\\\\\":   authors,\\\\n\\\",\\n+    \\\"        \\\\\\\"published\\\\\\\": pubdate,\\\\n\\\",\\n+    \\\"        \\\\\\\"publisher\\\\\\\": meta.get(\\\\\\\"publisher\\\\\\\"),\\\\n\\\",\\n+    \\\"      },\\\\n\\\",\\n+    \\\"   \\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"      # 4) log it as a single JSON artifact\\\\n\\\",\\n+    \\\"    mlflow.log_dict(public_datasetRepository_metadata,\\\\n\\\",\\n+    \\\"                \\\\\\\"public_datasetRepository_metadata.json\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 2) Flatten and log the important bits as params:\\\\n\\\",\\n+    \\\"    z = public_datasetRepository_metadata[\\\\\\\"zenodo\\\\\\\"]\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"dataset.title\\\\\\\",     z[\\\\\\\"title\\\\\\\"])\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"dataset.doi\\\\\\\",       z[\\\\\\\"doi\\\\\\\"])\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"dataset.authors\\\\\\\",   json.dumps(z[\\\\\\\"authors\\\\\\\"]))\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"dataset.published\\\\\\\", z[\\\\\\\"published\\\\\\\"])\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"dataset.publisher\\\\\\\", z[\\\\\\\"publisher\\\\\\\"])\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"   \\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"58c92e13-eb57-418d-b354-83777f88aa98\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"##################################################################\\\\n\\\",\\n+    \\\"# DATA PREPROCESSING STEPS\\\\n\\\",\\n+    \\\"###################################################################\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"9832d0df-af0a-4eee-90d0-fab926e03e85\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"STEP 1: LOAD DATASET\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 39,\\n+   \\\"id\\\": \\\"77402d80-22d1-4bed-9489-768958c3e9fa\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"# \\u2500\\u2500 2) Load into a DataFrame \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"df = pd.DataFrame(dataset)\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"6862e341-3ea1-43f6-a1ac-9a51188fe614\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"STEP2: seperate Dependent and Independent variables and drop unnecessary columns like ID\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 40,\\n+   \\\"id\\\": \\\"01309a7b-53d2-4df4-b334-0f0db8b03333\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"Shapes: (150, 4) (150,)\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"target_col = df.columns[-1]      # e.g. \\\\\\\"species\\\\\\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# 2) extract y as the Series of labels\\\\n\\\",\\n+    \\\"y = df[target_col]               # length == n_samples\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# 3) build X by dropping just that one column\\\\n\\\",\\n+    \\\"X = df.drop(columns=[target_col])\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# 4) drop any ID column (case-insensitive)\\\\n\\\",\\n+    \\\"id_cols = [c for c in X.columns if c.lower() == \\\\\\\"id\\\\\\\"]\\\\n\\\",\\n+    \\\"if id_cols:\\\\n\\\",\\n+    \\\"    X = X.drop(columns=id_cols)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# 5) coerce numeric where possible\\\\n\\\",\\n+    \\\"for c in X.columns:\\\\n\\\",\\n+    \\\"    try:\\\\n\\\",\\n+    \\\"        X[c] = pd.to_numeric(X[c])\\\\n\\\",\\n+    \\\"    except Exception:\\\\n\\\",\\n+    \\\"        pass\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"print(\\\\\\\"Shapes:\\\\\\\", X.shape, y.shape)\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"367d6256-a30a-4f91-bc64-f20966d828ab\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"STEP3: Label Encoding as the target values are class names\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 41,\\n+   \\\"id\\\": \\\"11f5126d-6a03-48c6-9ecf-39ed0d43688c\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"Classes: ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"text/plain\\\": [\\n+       \\\"array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\\\n\\\",\\n+       \\\"       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\\\n\\\",\\n+       \\\"       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\\\n\\\",\\n+       \\\"       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\\\n\\\",\\n+       \\\"       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\\\n\\\",\\n+       \\\"       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\\\n\\\",\\n+       \\\"       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\\\"\\n+      ]\\n+     },\\n+     \\\"execution_count\\\": 41,\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"execute_result\\\"\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"le = LabelEncoder()\\\\n\\\",\\n+    \\\"y = le.fit_transform(y)  \\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# now y_enc is a 1d numpy array of ints 0,1,2\\\\n\\\",\\n+    \\\"print(\\\\\\\"Classes:\\\\\\\", le.classes_)  \\\\n\\\",\\n+    \\\"y\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 42,\\n+   \\\"id\\\": \\\"68d0a924-c65f-4a44-a5cc-bbb32d17e96f\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"# \\u2500\\u2500 4) Cast feature columns to numeric where possible \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"for col in X.columns:\\\\n\\\",\\n+    \\\"    try:\\\\n\\\",\\n+    \\\"        X[col] = pd.to_numeric(X[col])   # no errors=\\\\\\\"ignore\\\\\\\"\\\\n\\\",\\n+    \\\"    except ValueError:\\\\n\\\",\\n+    \\\"        # if it can\\u2019t be cast, just leave it as-is\\\\n\\\",\\n+    \\\"        pass\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 43,\\n+   \\\"id\\\": \\\"e17f39ce-3322-4626-83a6-079d304bbc04\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"# \\u2500\\u2500 5) Drop any \\u201cid\\u201d column (case-insensitive) \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"dropped = [c for c in X.columns if c.lower() == \\\\\\\"id\\\\\\\"]\\\\n\\\",\\n+    \\\"X = X.drop(columns=dropped, errors=\\\\\\\"ignore\\\\\\\")\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"6fcf2244-14dd-4e3d-b8cf-f7f3ba34f80f\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83d\\udcc2 Setup MLflow\\\\n\\\",\\n+    \\\"# ============================\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 44,\\n+   \\\"id\\\": \\\"cbe91ec0-6447-4586-b7cc-2c1f74d4218f\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"text/plain\\\": [\\n+       \\\"<Experiment: artifact_location='file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608', creation_time=1745329164532, experiment_id='615223710259862608', last_update_time=1745329164532, lifecycle_stage='active', name='RandomForest-Iris-CSV', tags={}>\\\"\\n+      ]\\n+     },\\n+     \\\"execution_count\\\": 44,\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"execute_result\\\"\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"project_dir = os.getcwd()\\\\n\\\",\\n+    \\\"mlflow.set_tracking_uri(\\\\\\\"mlrunlogs/mlflow.db\\\\\\\")\\\\n\\\",\\n+    \\\"mlflow.set_experiment(\\\\\\\"RandomForest-Iris-CSV\\\\\\\")\\\\n\\\",\\n+    \\\"# mlflow.sklearn.autolog()\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"af2c2c5f-cc36-41a3-9643-83ef95b9f55e\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83d\\udd04 Git Commit Hash for previous commit for metadata\\\\n\\\",\\n+    \\\"# ============================\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 45,\\n+   \\\"id\\\": \\\"838dd233-25dc-4725-974d-4da89c257782\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"repo_dir = \\\\\\\"C:/Users/reema/REPO\\\\\\\"\\\\n\\\",\\n+    \\\"previous_commit_repo = git.Repo(repo_dir)\\\\n\\\",\\n+    \\\"previous_commit_hash = previous_commit_repo.head.object.hexsha\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"430d15ef-3432-4e45-88fb-b7048a5b10a9\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# Make threadpoolctl safe so MLflow\\u2019s autologger won\\u2019t crash \\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"# ============================\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 46,\\n+   \\\"id\\\": \\\"9668451f-4352-4bdc-8b6b-bbe49074212a\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 11:57:54 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\\\\n\\\",\\n+      \\\"2025/04/25 11:57:54 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"try:\\\\n\\\",\\n+    \\\"    import threadpoolctl\\\\n\\\",\\n+    \\\"    _orig = threadpoolctl.threadpool_info\\\\n\\\",\\n+    \\\"    def _safe_threadpool_info(*args, **kwargs):\\\\n\\\",\\n+    \\\"        try:\\\\n\\\",\\n+    \\\"            return _orig(*args, **kwargs)\\\\n\\\",\\n+    \\\"        except Exception:\\\\n\\\",\\n+    \\\"            return []\\\\n\\\",\\n+    \\\"    threadpoolctl.threadpool_info = _safe_threadpool_info\\\\n\\\",\\n+    \\\"except ImportError:\\\\n\\\",\\n+    \\\"    pass  # if threadpoolctl isn\\u2019t installed, autolog will skip unsupported versions\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# \\u2500\\u2500\\u2500 1) Enable generic autolog (will auto-patch sklearn under the hood) \\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"import mlflow\\\\n\\\",\\n+    \\\"mlflow.autolog(\\\\n\\\",\\n+    \\\"    log_input_examples=True,\\\\n\\\",\\n+    \\\"    log_model_signatures=True\\\\n\\\",\\n+    \\\")\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"cba22f52-178f-48e6-a9e0-7ef23a886f01\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"#################################################\\\\n\\\",\\n+    \\\"# Justification LOGGER\\\\n\\\",\\n+    \\\"################################################\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 80,\\n+   \\\"id\\\": \\\"afb626b4-8532-4011-bdc8-7424a6289bb7\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": []\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"9058319a-adba-4a6b-93e9-d17080c0594d\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83d\\ude80 Start MLflow Run \\\\n\\\",\\n+    \\\"# ============================\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 83,\\n+   \\\"id\\\": \\\"14c62f08-a116-4060-9689-f69968e9f240\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `n_estimators` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `criterion` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `max_depth` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `min_samples_split` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `min_samples_leaf` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `max_features` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `bootstrap` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `oob_score` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `class_weight` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `random_state` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `verbose` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `n_jobs` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `model_choice`\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose RandomForestClassifier for this task?  easy model\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `target_variable`\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this column as the prediction target?  dataset info\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `test_split`\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why this train/test ratio (e.g., 80/20)?  makes sense\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `metric_choice`\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why accuracy/f1/ROC-AUC as your evaluation metric?  s\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `threshold_accuracy`\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why 0.95 as performance threshold?  fluid atm\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `dataset_version`\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why use this specific dataset version?  its available\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `drop_column_X`\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why drop any specific columns from the dataset?  id\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `experiment_name`\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Any context behind this experiment name or setup?  makes sense\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.2s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    0.2s finished\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"a64e4f0522d8493995df18aaaba889fc\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\\\\n\\\",\\n+      \\\"C:\\\\\\\\Users\\\\\\\\reema\\\\\\\\anaconda3\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\shap\\\\\\\\plots\\\\\\\\_beeswarm.py:1153: UserWarning: The figure layout has changed to tight\\\\n\\\",\\n+      \\\"  pl.tight_layout()\\\\n\\\",\\n+      \\\"C:\\\\\\\\Users\\\\\\\\reema\\\\\\\\anaconda3\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\shap\\\\\\\\plots\\\\\\\\_beeswarm.py:761: UserWarning: The figure layout has changed to tight\\\\n\\\",\\n+      \\\"  pl.tight_layout(pad=0, w_pad=0, h_pad=0.0)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2705 Commit successful.\\\\n\\\",\\n+      \\\"\\ud83d\\ude80 Push successful.\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"with mlflow.start_run() as run:\\\\n\\\",\\n+    \\\"    #################################################\\\\n\\\",\\n+    \\\"# Justification LOGGER\\\\n\\\",\\n+    \\\"################################################\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    def log_with_justification(log_func, key, value, context=\\\\\\\"\\\\\\\"):\\\\n\\\",\\n+    \\\"        \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"        Log a parameter/metric/tag using `log_func` and ask for justification via console.\\\\n\\\",\\n+    \\\"        \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"        log_func(key, value)\\\\n\\\",\\n+    \\\"        print(f\\\\\\\"\\\\\\\\n\\ud83d\\udcdd Justification for `{key}` ({context})\\\\\\\")\\\\n\\\",\\n+    \\\"        user_reason = input(\\\\\\\"\\u2192 Why did you choose this value? \\\\\\\")\\\\n\\\",\\n+    \\\"        mlflow.set_tag(f\\\\\\\"justification_{key}\\\\\\\", user_reason or \\\\\\\"No justification provided\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    def log_justification(key: str, question: str):\\\\n\\\",\\n+    \\\"        print(f\\\\\\\"\\\\\\\\n\\ud83d\\udcdd Justification for `{key}`\\\\\\\")\\\\n\\\",\\n+    \\\"        user_reason = input(f\\\\\\\"\\u2192 {question} \\\\\\\")\\\\n\\\",\\n+    \\\"        mlflow.set_tag(f\\\\\\\"justification_{key}\\\\\\\", user_reason or \\\\\\\"No justification provided\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    meta = fetch_and_log_dataset_metadata_nested(\\\\n\\\",\\n+    \\\"            \\\\\\\"https://doi.org/10.5281/zenodo.1404173\\\\\\\",\\\\n\\\",\\n+    \\\"           \\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    #Datasbase info logging\\\\n\\\",\\n+    \\\"    db_id = \\\\\\\"c3a42d17-42b7-43c9-a504-2363fb4c9c8d\\\\\\\"\\\\n\\\",\\n+    \\\"    db_meta = fetch_db_metadata(db_id)\\\\n\\\",\\n+    \\\"    log_db_metadata(db_meta)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    #OAI metadata logging from api endpoint\\\\n\\\",\\n+    \\\"    # log as tags\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dbrepo.repository_name\\\\\\\", repo_name)\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dbrepo.base_url\\\\\\\",       base_url)\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dbrepo.protocol_version\\\\\\\", protocol)\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dbrepo.admin_email\\\\\\\",     admin_email)\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dbrepo.granularity\\\\\\\",     gran)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    #From history API logging\\\\n\\\",\\n+    \\\"    # provenance tags\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dbrepo.table_last_modified\\\\\\\", ts_last)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # row-count metrics\\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"dbrepo.row_count_start\\\\\\\", count_start)\\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"dbrepo.row_count_end\\\\\\\",   count_end)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # change-event metrics\\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"dbrepo.num_inserts\\\\\\\", n_insert)\\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"dbrepo.num_deletes\\\\\\\", n_delete)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # 2) Capture raw metadata\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"data_source\\\\\\\", API_URL)\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"retrieval_time\\\\\\\", datetime.utcnow().isoformat())\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"n_records\\\\\\\", len(df))\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"columns_raw\\\\\\\", df.columns.tolist())\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"dropped_columns\\\\\\\", id_cols)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 4) Post\\u2010processing metadata\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"n_features_final\\\\\\\", X.shape[1])\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"feature_names\\\\\\\", X.columns.tolist())\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"target_name\\\\\\\", y)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"       # Label encoding\\\\n\\\",\\n+    \\\"    label_map = {int(idx): cls for idx, cls in enumerate(le.classes_)}\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # Save to an in-memory file\\\\n\\\",\\n+    \\\"    buffer = io.StringIO()\\\\n\\\",\\n+    \\\"    json.dump(label_map, buffer, indent=2)\\\\n\\\",\\n+    \\\"    buffer.seek(0)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # Log it to MLflow\\\\n\\\",\\n+    \\\"    mlflow.log_text(buffer.getvalue(), artifact_file=\\\\\\\"label_mapping.json\\\\\\\")\\\\n\\\",\\n+    \\\"   \\\\n\\\",\\n+    \\\"    ts = datetime.now().strftime(\\\\\\\"%Y%m%d_%H%M%S\\\\\\\")\\\\n\\\",\\n+    \\\"    model_name = f\\\\\\\"RandomForest_Iris_v{ts}\\\\\\\"\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"model_name\\\\\\\",model_name)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    train_start_ts = datetime.now().isoformat()\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"training_start_time\\\\\\\", train_start_ts)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    test_size    = 0.2\\\\n\\\",\\n+    \\\"    random_state = 42\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # \\ud83d\\udcc8 Model Training\\\\n\\\",\\n+    \\\"    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # \\u2500\\u2500 2) Log dataset split params \\u2500\\u2500\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"test_size\\\\\\\", test_size)\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"random_state\\\\\\\", random_state)\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"n_train_samples\\\\\\\", X_train.shape[0])\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"n_test_samples\\\\\\\",  X_test.shape[0])\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"n_features\\\\\\\",      X_train.shape[1])\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"     # 1) Define a more complex hyperparameter dict\\\\n\\\",\\n+    \\\"    hyperparams = {\\\\n\\\",\\n+    \\\"        \\\\\\\"n_estimators\\\\\\\":       200,\\\\n\\\",\\n+    \\\"        \\\\\\\"criterion\\\\\\\":          \\\\\\\"entropy\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"max_depth\\\\\\\":          12,\\\\n\\\",\\n+    \\\"        \\\\\\\"min_samples_split\\\\\\\":  5,\\\\n\\\",\\n+    \\\"        \\\\\\\"min_samples_leaf\\\\\\\":   2,\\\\n\\\",\\n+    \\\"        \\\\\\\"max_features\\\\\\\":       \\\\\\\"sqrt\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"bootstrap\\\\\\\":          True,\\\\n\\\",\\n+    \\\"        \\\\\\\"oob_score\\\\\\\":          False,\\\\n\\\",\\n+    \\\"        \\\\\\\"class_weight\\\\\\\":       None,\\\\n\\\",\\n+    \\\"        \\\\\\\"random_state\\\\\\\":       42,\\\\n\\\",\\n+    \\\"        \\\\\\\"verbose\\\\\\\":            1,\\\\n\\\",\\n+    \\\"        \\\\\\\"n_jobs\\\\\\\":             -1\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # 2) Log them ALL at once\\\\n\\\",\\n+    \\\"    mlflow.log_params(hyperparams)\\\\n\\\",\\n+    \\\"    model = RandomForestClassifier(**hyperparams)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    for key, val in hyperparams.items():\\\\n\\\",\\n+    \\\"        log_with_justification(mlflow.log_param, key, val, context=\\\\\\\"Hyperparameter configuration\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # Prompt for and log justifications for high-level modeling decisions\\\\n\\\",\\n+    \\\"    log_justification(\\\\\\\"model_choice\\\\\\\", \\\\\\\"Why did you choose RandomForestClassifier for this task?\\\\\\\")\\\\n\\\",\\n+    \\\"    log_justification(\\\\\\\"target_variable\\\\\\\", \\\\\\\"Why did you choose this column as the prediction target?\\\\\\\")\\\\n\\\",\\n+    \\\"    log_justification(\\\\\\\"test_split\\\\\\\", \\\\\\\"Why this train/test ratio (e.g., 80/20)?\\\\\\\")\\\\n\\\",\\n+    \\\"    log_justification(\\\\\\\"metric_choice\\\\\\\", \\\\\\\"Why accuracy/f1/ROC-AUC as your evaluation metric?\\\\\\\")\\\\n\\\",\\n+    \\\"    log_justification(\\\\\\\"threshold_accuracy\\\\\\\", \\\\\\\"Why 0.95 as performance threshold?\\\\\\\")\\\\n\\\",\\n+    \\\"    log_justification(\\\\\\\"dataset_version\\\\\\\", \\\\\\\"Why use this specific dataset version?\\\\\\\")\\\\n\\\",\\n+    \\\"    log_justification(\\\\\\\"drop_column_X\\\\\\\", \\\\\\\"Why drop any specific columns from the dataset?\\\\\\\")\\\\n\\\",\\n+    \\\"    log_justification(\\\\\\\"experiment_name\\\\\\\", \\\\\\\"Any context behind this experiment name or setup?\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    model.fit(X_train, y_train)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    train_end_ts = datetime.now().isoformat()\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"training_end_time\\\\\\\", train_end_ts)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"     # \\u2500\\u2500 6) Predict & log metrics \\u2500\\u2500\\\\n\\\",\\n+    \\\"    y_pred = model.predict(X_test)\\\\n\\\",\\n+    \\\"    y_proba = model.predict_proba(X_test)\\\\n\\\",\\n+    \\\"    acc = accuracy_score(y_test, y_pred)\\\\n\\\",\\n+    \\\"    auc = roc_auc_score(y_test, y_proba, multi_class=\\\\\\\"ovr\\\\\\\")\\\\n\\\",\\n+    \\\"    prec = precision_score(y_test, y_pred, average=\\\\\\\"macro\\\\\\\")\\\\n\\\",\\n+    \\\"    rec  = recall_score(y_test,    y_pred, average=\\\\\\\"macro\\\\\\\")\\\\n\\\",\\n+    \\\"    f1   = f1_score(y_test,      y_pred, average=\\\\\\\"macro\\\\\\\")\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"precision_macro\\\\\\\", prec)\\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"recall_macro\\\\\\\",    rec)\\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"f1_macro\\\\\\\",        f1)\\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"accuracy\\\\\\\", acc)\\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"roc_auc\\\\\\\",   auc)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # \\u2705 Log Environment Automatically\\\\n\\\",\\n+    \\\"    mlflow.log_params({\\\\n\\\",\\n+    \\\"        \\\\\\\"python_version\\\\\\\": platform.python_version(),\\\\n\\\",\\n+    \\\"        \\\\\\\"os_platform\\\\\\\": f\\\\\\\"{platform.system()} {platform.release()}\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"sklearn_version\\\\\\\": sklearn.__version__,\\\\n\\\",\\n+    \\\"        \\\\\\\"pandas_version\\\\\\\": pd.__version__,\\\\n\\\",\\n+    \\\"        \\\\\\\"numpy_version\\\\\\\": np.__version__,\\\\n\\\",\\n+    \\\"        \\\\\\\"matplotlib_version\\\\\\\": matplotlib.__version__,\\\\n\\\",\\n+    \\\"        \\\\\\\"seaborn_version\\\\\\\": sns.__version__,\\\\n\\\",\\n+    \\\"        \\\\\\\"shap_version\\\\\\\": shap.__version__,\\\\n\\\",\\n+    \\\"    })\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # \\u2705 Git and Notebook Metadata\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"notebook_name\\\\\\\", \\\\\\\"RQ1.ipynb\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # \\u2705 Dataset Metadata Tags\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dataset_name\\\\\\\", \\\\\\\"Iris\\\\\\\") #TODO\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dataset_version\\\\\\\", \\\\\\\"1.0.0\\\\\\\") #TODO\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dataset_id\\\\\\\", \\\\\\\"iris_local\\\\\\\") #TODO\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # \\u2500\\u2500\\u2500 2) Create a folder for this run\\u2019s plots \\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"    plot_dir = os.path.join(\\\\\\\"plots\\\\\\\", model_name)\\\\n\\\",\\n+    \\\"    os.makedirs(plot_dir, exist_ok=True)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 1) Feature Importance Bar Chart\\\\n\\\",\\n+    \\\"    importances = model.feature_importances_\\\\n\\\",\\n+    \\\"    try:\\\\n\\\",\\n+    \\\"        feature_names = X_train.columns\\\\n\\\",\\n+    \\\"    except AttributeError:\\\\n\\\",\\n+    \\\"        feature_names = [f\\\\\\\"f{i}\\\\\\\" for i in range(X_train.shape[1])]\\\\n\\\",\\n+    \\\"    fi_path = os.path.join(plot_dir, \\\\\\\"feature_importances.png\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    plt.figure(figsize=(8, 6))\\\\n\\\",\\n+    \\\"    sns.barplot(x=importances, y=feature_names)\\\\n\\\",\\n+    \\\"    plt.title(\\\\\\\"Feature Importances\\\\\\\")\\\\n\\\",\\n+    \\\"    plt.xlabel(\\\\\\\"Importance\\\\\\\")\\\\n\\\",\\n+    \\\"    plt.ylabel(\\\\\\\"Feature\\\\\\\")\\\\n\\\",\\n+    \\\"    plt.tight_layout()\\\\n\\\",\\n+    \\\"    plt.savefig(fi_path)\\\\n\\\",\\n+    \\\"    mlflow.log_artifact(fi_path)\\\\n\\\",\\n+    \\\"    plt.close()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# 2) Multi-class ROC Curves\\\\n\\\",\\n+    \\\"# Binarize labels for one-vs-rest\\\\n\\\",\\n+    \\\"    classes = np.unique(y_test)\\\\n\\\",\\n+    \\\"    y_test_bin = label_binarize(y_test, classes=classes)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    for idx, cls in enumerate(classes):\\\\n\\\",\\n+    \\\"        disp = RocCurveDisplay.from_predictions(\\\\n\\\",\\n+    \\\"            y_test_bin[:, idx], \\\\n\\\",\\n+    \\\"            y_proba[:, idx],\\\\n\\\",\\n+    \\\"            name=f\\\\\\\"ROC for class {cls}\\\\\\\"\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        roc_path = os.path.join(plot_dir, f\\\\\\\"roc_curve_cls_{cls}.png\\\\\\\")\\\\n\\\",\\n+    \\\"        disp.figure_.savefig(roc_path)\\\\n\\\",\\n+    \\\"        mlflow.log_artifact(roc_path)\\\\n\\\",\\n+    \\\"        plt.close(disp.figure_)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 3) Multi-class Precision-Recall Curves\\\\n\\\",\\n+    \\\"    for idx, cls in enumerate(classes):\\\\n\\\",\\n+    \\\"        disp = PrecisionRecallDisplay.from_predictions(\\\\n\\\",\\n+    \\\"            y_test_bin[:, idx], \\\\n\\\",\\n+    \\\"            y_proba[:, idx],\\\\n\\\",\\n+    \\\"            name=f\\\\\\\"PR curve for class {cls}\\\\\\\"\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        pr_path = os.path.join(plot_dir, f\\\\\\\"pr_curve_cls_{cls}.png\\\\\\\")\\\\n\\\",\\n+    \\\"        disp.figure_.savefig(pr_path)\\\\n\\\",\\n+    \\\"        mlflow.log_artifact(pr_path)\\\\n\\\",\\n+    \\\"        plt.close(disp.figure_)\\\\n\\\",\\n+    \\\"        \\\\n\\\",\\n+    \\\"    # \\u2705 Confusion Matrix Plot\\\\n\\\",\\n+    \\\"    cm_path = os.path.join(plot_dir, \\\\\\\"confusion_matrix.png\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    cm = confusion_matrix(y_test, y_pred)\\\\n\\\",\\n+    \\\"    plt.figure(figsize=(6, 6))\\\\n\\\",\\n+    \\\"    sns.heatmap(cm, annot=True, fmt=\\\\\\\"d\\\\\\\", cmap=\\\\\\\"Blues\\\\\\\")\\\\n\\\",\\n+    \\\"    plt.title(\\\\\\\"Confusion Matrix\\\\\\\")\\\\n\\\",\\n+    \\\"    plt.xlabel(\\\\\\\"Predicted\\\\\\\")\\\\n\\\",\\n+    \\\"    plt.ylabel(\\\\\\\"Actual\\\\\\\")\\\\n\\\",\\n+    \\\"    plt.savefig(cm_path)\\\\n\\\",\\n+    \\\"    mlflow.log_artifact(cm_path)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # \\u2705 SHAP Summary\\\\n\\\",\\n+    \\\"    shap_path = os.path.join(plot_dir, \\\\\\\"shap_summary.png\\\\\\\")\\\\n\\\",\\n+    \\\"    explainer = shap.TreeExplainer(model)\\\\n\\\",\\n+    \\\"    shap_values = explainer.shap_values(X_test)\\\\n\\\",\\n+    \\\"    shap.summary_plot(shap_values, X_test, show=False)\\\\n\\\",\\n+    \\\"    plt.savefig(shap_path)\\\\n\\\",\\n+    \\\"    mlflow.log_artifact(shap_path)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # \\u2500\\u2500\\u2500 1) Build a .pkl filename (you can include your model_name for clarity)\\\\n\\\",\\n+    \\\"    pkl_path = f\\\\\\\"Trained_models/{model_name}.pkl\\\\\\\"\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # \\u2500\\u2500\\u2500 2) Serialize your trained model to disk\\\\n\\\",\\n+    \\\"    with open(pkl_path, \\\\\\\"wb\\\\\\\") as f:\\\\n\\\",\\n+    \\\"        pickle.dump(model, f)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # \\u2500\\u2500\\u2500 3) Log that pickle file as an MLflow artifact\\\\n\\\",\\n+    \\\"    #     It will appear under Artifacts \\u2192 models/RandomForest_Iris_vYYYYMMDD_HHMMSS.pkl\\\\n\\\",\\n+    \\\"    mlflow.log_artifact(pkl_path, artifact_path=model_name)\\\\n\\\",\\n+    \\\"        \\\\n\\\",\\n+    \\\"    def get_latest_commit_hash(repo_path=\\\\\\\".\\\\\\\"):\\\\n\\\",\\n+    \\\"        # returns the full SHA of HEAD\\\\n\\\",\\n+    \\\"        res = subprocess.run(\\\\n\\\",\\n+    \\\"            [\\\\\\\"git\\\\\\\", \\\\\\\"-C\\\\\\\", repo_path, \\\\\\\"rev-parse\\\\\\\", \\\\\\\"HEAD\\\\\\\"],\\\\n\\\",\\n+    \\\"            capture_output=True, text=True, check=True)\\\\n\\\",\\n+    \\\"        \\\\n\\\",\\n+    \\\"        return res.stdout.strip()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    def get_remote_url(repo_path=\\\\\\\".\\\\\\\", remote=\\\\\\\"origin\\\\\\\"):\\\\n\\\",\\n+    \\\"        # returns something like git@github.com:user/repo.git or https://...\\\\n\\\",\\n+    \\\"        res = subprocess.run(\\\\n\\\",\\n+    \\\"            [\\\\\\\"git\\\\\\\", \\\\\\\"-C\\\\\\\", repo_path, \\\\\\\"config\\\\\\\", \\\\\\\"--get\\\\\\\", f\\\\\\\"remote.{remote}.url\\\\\\\"],\\\\n\\\",\\n+    \\\"            capture_output=True, text=True, check=True\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        return res.stdout.strip()\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    def make_commit_link(remote_url, commit_hash):\\\\n\\\",\\n+    \\\"        # handle GitHub/GitLab convention; strip \\u201c.git\\u201d if present\\\\n\\\",\\n+    \\\"        base = remote_url.rstrip(\\\\\\\".git\\\\\\\")\\\\n\\\",\\n+    \\\"        # if SSH form (git@github.com:owner/repo), convert to https\\\\n\\\",\\n+    \\\"        if base.startswith(\\\\\\\"git@\\\\\\\"):\\\\n\\\",\\n+    \\\"            base = base.replace(\\\\\\\":\\\\\\\", \\\\\\\"/\\\\\\\").replace(\\\\\\\"git@\\\\\\\", \\\\\\\"https://\\\\\\\")\\\\n\\\",\\n+    \\\"        return f\\\\\\\"{base}/commit/{commit_hash}\\\\\\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    def simple_commit_and_push_and_log(repo_path=\\\\\\\".\\\\\\\", message=\\\\\\\"Auto commit\\\\\\\", remote=\\\\\\\"origin\\\\\\\", branch=\\\\\\\"main\\\\\\\"):\\\\n\\\",\\n+    \\\"    # 1) Check for changes\\\\n\\\",\\n+    \\\"        status = subprocess.run(\\\\n\\\",\\n+    \\\"            [\\\\\\\"git\\\\\\\", \\\\\\\"-C\\\\\\\", repo_path, \\\\\\\"status\\\\\\\", \\\\\\\"--porcelain\\\\\\\"],\\\\n\\\",\\n+    \\\"            capture_output=True, text=True\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        if not status.stdout.strip():\\\\n\\\",\\n+    \\\"            print(\\\\\\\"\\ud83d\\udfe1 No changes to commit.\\\\\\\")\\\\n\\\",\\n+    \\\"            return None, None\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"        # 2) Stage everything\\\\n\\\",\\n+    \\\"        add = subprocess.run(\\\\n\\\",\\n+    \\\"            [\\\\\\\"git\\\\\\\", \\\\\\\"-C\\\\\\\", repo_path, \\\\\\\"add\\\\\\\", \\\\\\\"--all\\\\\\\"],\\\\n\\\",\\n+    \\\"            capture_output=True, text=True\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        if add.returncode:\\\\n\\\",\\n+    \\\"            print(\\\\\\\"\\u274c git add failed:\\\\\\\\n\\\\\\\", add.stderr)\\\\n\\\",\\n+    \\\"            return None, None\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"        # 3) Commit\\\\n\\\",\\n+    \\\"        commit = subprocess.run(\\\\n\\\",\\n+    \\\"            [\\\\\\\"git\\\\\\\", \\\\\\\"-C\\\\\\\", repo_path, \\\\\\\"commit\\\\\\\", \\\\\\\"-m\\\\\\\", message],\\\\n\\\",\\n+    \\\"            capture_output=True, text=True\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        if commit.returncode:\\\\n\\\",\\n+    \\\"            print(\\\\\\\"\\u274c git commit failed:\\\\\\\\n\\\\\\\", commit.stderr)\\\\n\\\",\\n+    \\\"            return None, None\\\\n\\\",\\n+    \\\"        print(\\\\\\\"\\u2705 Commit successful.\\\\\\\")\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"        # 4) Push\\\\n\\\",\\n+    \\\"        push = subprocess.run(\\\\n\\\",\\n+    \\\"            [\\\\\\\"git\\\\\\\", \\\\\\\"-C\\\\\\\", repo_path, \\\\\\\"push\\\\\\\", \\\\\\\"-u\\\\\\\", remote, branch],\\\\n\\\",\\n+    \\\"            capture_output=True, text=True\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        if push.returncode:\\\\n\\\",\\n+    \\\"            print(\\\\\\\"\\u274c git push failed:\\\\\\\\n\\\\\\\", push.stderr)\\\\n\\\",\\n+    \\\"        else:\\\\n\\\",\\n+    \\\"            print(\\\\\\\"\\ud83d\\ude80 Push successful.\\\\\\\")\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"        # 5) Retrieve hash & remote URL\\\\n\\\",\\n+    \\\"        sha = get_latest_commit_hash(repo_path)\\\\n\\\",\\n+    \\\"        url = get_remote_url(repo_path, remote)\\\\n\\\",\\n+    \\\"        link = make_commit_link(url, sha)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"        return sha, link\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"      \\\\n\\\",\\n+    \\\"    sha, link = simple_commit_and_push_and_log(\\\\n\\\",\\n+    \\\"        repo_path=\\\\\\\".\\\\\\\",\\\\n\\\",\\n+    \\\"        message=\\\\\\\"Auto commit after successful training\\\\\\\"\\\\n\\\",\\n+    \\\"    )\\\\n\\\",\\n+    \\\"    if sha and link:\\\\n\\\",\\n+    \\\"        diff_text = subprocess.check_output(\\\\n\\\",\\n+    \\\"            [\\\\\\\"git\\\\\\\", \\\\\\\"-C\\\\\\\", \\\\\\\".\\\\\\\", \\\\\\\"diff\\\\\\\", previous_commit_hash, sha],\\\\n\\\",\\n+    \\\"            encoding=\\\\\\\"utf-8\\\\\\\",\\\\n\\\",\\n+    \\\"            errors=\\\\\\\"ignore\\\\\\\"    # or \\\\\\\"replace\\\\\\\"\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"                \\\\n\\\",\\n+    \\\"        # 1) Get your repo\\u2019s remote URL and normalize to HTTPS\\\\n\\\",\\n+    \\\"        remote_url = subprocess.check_output(\\\\n\\\",\\n+    \\\"            [\\\\\\\"git\\\\\\\", \\\\\\\"config\\\\\\\", \\\\\\\"--get\\\\\\\", \\\\\\\"remote.origin.url\\\\\\\"],\\\\n\\\",\\n+    \\\"            text=True\\\\n\\\",\\n+    \\\"        ).strip().rstrip(\\\\\\\".git\\\\\\\")\\\\n\\\",\\n+    \\\"        if remote_url.startswith(\\\\\\\"git@\\\\\\\"):\\\\n\\\",\\n+    \\\"            # git@github.com:owner/repo.git \\u2192 https://github.com/owner/repo\\\\n\\\",\\n+    \\\"            remote_url = remote_url.replace(\\\\\\\":\\\\\\\", \\\\\\\"/\\\\\\\").replace(\\\\\\\"git@\\\\\\\", \\\\\\\"https://\\\\\\\")\\\\n\\\",\\n+    \\\"        \\\\n\\\",\\n+    \\\"        # 2) Build commit URLs\\\\n\\\",\\n+    \\\"        previous_commit_url  = f\\\\\\\"{remote_url}/commit/{previous_commit_hash}\\\\\\\"\\\\n\\\",\\n+    \\\"        current_commit_url = f\\\\\\\"{remote_url}/commit/{sha}\\\\\\\"\\\\n\\\",\\n+    \\\"        diff_data = {\\\\n\\\",\\n+    \\\"            \\\\\\\"previous_commit\\\\\\\":  previous_commit_hash,\\\\n\\\",\\n+    \\\"            \\\\\\\"previous_commit_url\\\\\\\":previous_commit_url,\\\\n\\\",\\n+    \\\"            \\\\\\\"current_commit_url\\\\\\\":current_commit_url,\\\\n\\\",\\n+    \\\"            \\\\\\\"current_commit\\\\\\\": sha,\\\\n\\\",\\n+    \\\"            \\\\\\\"diff\\\\\\\": diff_text\\\\n\\\",\\n+    \\\"        }\\\\n\\\",\\n+    \\\"        mlflow.log_dict(\\\\n\\\",\\n+    \\\"            diff_data,\\\\n\\\",\\n+    \\\"            artifact_file=\\\\\\\"commit_diff.json\\\\\\\"\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        mlflow.set_tag(\\\\\\\"git_previous_commit_hash\\\\\\\", previous_commit_hash)\\\\n\\\",\\n+    \\\"        mlflow.set_tag(\\\\\\\"git_current_commit_hash\\\\\\\", sha)\\\\n\\\",\\n+    \\\"        mlflow.set_tag(\\\\\\\"git__current_commit_url\\\\\\\", link) \\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    client   = MlflowClient()\\\\n\\\",\\n+    \\\"    run_id    = run.info.run_id\\\\n\\\",\\n+    \\\"    run_info  = client.get_run(run_id).info\\\\n\\\",\\n+    \\\"    run_data  = client.get_run(run_id).data\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # 1) params, metrics, tags\\\\n\\\",\\n+    \\\"    params  = dict(run_data.params)\\\\n\\\",\\n+    \\\"    metrics = dict(run_data.metrics)\\\\n\\\",\\n+    \\\"    tags    = dict(run_data.tags)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # (4) List artifacts under a specific subfolder\\\\n\\\",\\n+    \\\"    run_meta     = client.get_run(run_id).info\\\\n\\\",\\n+    \\\"    artifact_uri = run_meta.artifact_uri  # base URI for all artifacts\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    artifact_meta = []\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    def _gather(path=\\\\\\\"\\\\\\\"):\\\\n\\\",\\n+    \\\"        for af in client.list_artifacts(run_id, path):\\\\n\\\",\\n+    \\\"            # If it\\u2019s a directory, recurse\\\\n\\\",\\n+    \\\"            if af.is_dir:\\\\n\\\",\\n+    \\\"                _gather(af.path)\\\\n\\\",\\n+    \\\"                continue\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"            rel_path = af.path\\\\n\\\",\\n+    \\\"            uri      = f\\\\\\\"{artifact_uri}/{rel_path}\\\\\\\"\\\\n\\\",\\n+    \\\"            lower    = rel_path.lower()\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"            # 1) Text files \\u2192 download & embed contents\\\\n\\\",\\n+    \\\"            if lower.endswith((\\\\\\\".json\\\\\\\", \\\\\\\".txt\\\\\\\", \\\\\\\".patch\\\\\\\")):\\\\n\\\",\\n+    \\\"                local = client.download_artifacts(run_id, rel_path)\\\\n\\\",\\n+    \\\"                with open(local, \\\\\\\"r\\\\\\\", encoding=\\\\\\\"utf-8\\\\\\\") as f:\\\\n\\\",\\n+    \\\"                    content = f.read()\\\\n\\\",\\n+    \\\"                artifact_meta.append({\\\\n\\\",\\n+    \\\"                    \\\\\\\"path\\\\\\\":    rel_path,\\\\n\\\",\\n+    \\\"                    \\\\\\\"type\\\\\\\":    \\\\\\\"text\\\\\\\",\\\\n\\\",\\n+    \\\"                    \\\\\\\"content\\\\\\\": content\\\\n\\\",\\n+    \\\"                })\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"            # 2) Images \\u2192 surface a clickable URI\\\\n\\\",\\n+    \\\"            elif lower.endswith((\\\\\\\".png\\\\\\\", \\\\\\\".jpg\\\\\\\", \\\\\\\".jpeg\\\\\\\", \\\\\\\".svg\\\\\\\")):\\\\n\\\",\\n+    \\\"                artifact_meta.append({\\\\n\\\",\\n+    \\\"                    \\\\\\\"path\\\\\\\": rel_path,\\\\n\\\",\\n+    \\\"                    \\\\\\\"type\\\\\\\": \\\\\\\"image\\\\\\\",\\\\n\\\",\\n+    \\\"                    \\\\\\\"uri\\\\\\\":  uri\\\\n\\\",\\n+    \\\"                })\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"            # 3) Everything else \\u2192 just link\\\\n\\\",\\n+    \\\"            else:\\\\n\\\",\\n+    \\\"                artifact_meta.append({\\\\n\\\",\\n+    \\\"                    \\\\\\\"path\\\\\\\": rel_path,\\\\n\\\",\\n+    \\\"                    \\\\\\\"type\\\\\\\": \\\\\\\"other\\\\\\\",\\\\n\\\",\\n+    \\\"                    \\\\\\\"uri\\\\\\\":  uri\\\\n\\\",\\n+    \\\"                })\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # Run the gather\\\\n\\\",\\n+    \\\"    _gather()\\\\n\\\",\\n+    \\\"     \\\\n\\\",\\n+    \\\"    summary = {\\\\n\\\",\\n+    \\\"        \\\\\\\"run_id\\\\\\\":         run_id,\\\\n\\\",\\n+    \\\"        \\\\\\\"run_name\\\\\\\": run_info.run_name,\\\\n\\\",\\n+    \\\"        \\\\\\\"experiment_id\\\\\\\":  run_info.experiment_id,\\\\n\\\",\\n+    \\\"        \\\\\\\"start_time\\\\\\\":     run_info.start_time,\\\\n\\\",\\n+    \\\"        \\\\\\\"end_time\\\\\\\":       run_info.end_time,\\\\n\\\",\\n+    \\\"        \\\\\\\"params\\\\\\\":         params,\\\\n\\\",\\n+    \\\"        \\\\\\\"metrics\\\\\\\":        metrics,\\\\n\\\",\\n+    \\\"        \\\\\\\"tags\\\\\\\":           tags,\\\\n\\\",\\n+    \\\"        \\\\\\\"artifacts\\\\\\\":      artifact_meta\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 1) Determine notebook directory (where your .ipynb lives)\\\\n\\\",\\n+    \\\"    notebook_dir = os.getcwd()\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # \\u2705 Create a subdirectory inside MODEL_PROVENANCE for the model\\\\n\\\",\\n+    \\\"    summary_dir = os.path.join(os.getcwd(), \\\\\\\"MODEL_PROVENANCE\\\\\\\", model_name)\\\\n\\\",\\n+    \\\"    os.makedirs(summary_dir, exist_ok=True)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"   # 2) Pick a filename based on your model_name\\\\n\\\",\\n+    \\\"    summary_filename   = f\\\\\\\"{model_name}_run_summary.json\\\\\\\"\\\\n\\\",\\n+    \\\"    summary_local_path = os.path.join(summary_dir, summary_filename)\\\\n\\\",\\n+    \\\"   # 3) Write the JSON locally\\\\n\\\",\\n+    \\\"    with open(summary_local_path, \\\\\\\"w\\\\\\\", encoding=\\\\\\\"utf-8\\\\\\\") as f:\\\\n\\\",\\n+    \\\"        json.dump(summary, f, indent=2)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # 4) (Optional) Mirror it into MLflow artifacts under a single folder\\\\n\\\",\\n+    \\\"    mlflow.log_artifact(summary_local_path, artifact_path=\\\\\\\"run_summaries\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    mlflow.end_run()\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"d0c4e1b2-9fa9-4606-8128-6ac66b5c6e78\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"what does it create: \\\\n\\\",\\n+    \\\"lable_mapping in the current dir\\\\n\\\",\\n+    \\\"provenence file :REPO/notebooks/RQ_notebooks/MODEL_PROVENANCE/RandomForest_Iris_v20250425_120045_run_summary.json\\\\n\\\",\\n+    \\\"plots based on run:REPO/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_120045/shap_summary.png\\\\n\\\",\\n+    \\\"mlrun:REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/5d1fa0fc65af47128f3200628b1afaea\\\\n\\\",\\n+    \\\"trained model:REPO/notebooks/RQ_notebooks/Trained_models/RandomForest_Iris_v20250425_120852.pkl\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"7a5e3bbb-0288-47d0-9dc4-2855d7e4801a\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"1. Standards-compliant export (JSON-LD + Turtle)\\\\n\\\",\\n+    \\\"I already have your plain run_summary.json , wrap it in a JSON-LD context that maps your fields into PROV-O terms, then use rdflib to emit Turtle:\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"28ed1cfb-930a-4f17-a48f-30e4cffb7f3e\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"import json\\\\n\\\",\\n+    \\\"import pandas as pd\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Load the JSON file\\\\n\\\",\\n+    \\\"json_path = \\\\\\\"/mnt/data/REPO/notebooks/RQ_notebooks/MODEL_PROVENANCE/RandomForest_Iris_v20250425_125653/RandomForest_Iris_v20250425_125653_run_summary.json\\\\\\\"\\\\n\\\",\\n+    \\\"with open(json_path, \\\\\\\"r\\\\\\\", encoding=\\\\\\\"utf-8\\\\\\\") as file:\\\\n\\\",\\n+    \\\"    data = json.load(file)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Extract justification tags\\\\n\\\",\\n+    \\\"justifications = {\\\\n\\\",\\n+    \\\"    k: v for k, v in data.get(\\\\\\\"tags\\\\\\\", {}).items()\\\\n\\\",\\n+    \\\"    if k.startswith(\\\\\\\"justification_\\\\\\\")\\\\n\\\",\\n+    \\\"}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Create a DataFrame\\\\n\\\",\\n+    \\\"justification_df = pd.DataFrame([\\\\n\\\",\\n+    \\\"    {\\\\\\\"Decision\\\\\\\": k.replace(\\\\\\\"justification_\\\\\\\", \\\\\\\"\\\\\\\"), \\\\\\\"Justification\\\\\\\": v}\\\\n\\\",\\n+    \\\"    for k, v in justifications.items()\\\\n\\\",\\n+    \\\"])\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"import ace_tools as tools; tools.display_dataframe_to_user(name=\\\\\\\"Researcher Justifications\\\\\\\", dataframe=justification_df)\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 66,\\n+   \\\"id\\\": \\\"5cf88da4-69f8-4982-a594-28cf25e4f79a\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"Converted RandomForest_Iris_v20250425_121328_run_summary.json \\u2192 RandomForest_Iris_v20250425_121328.jsonld, RandomForest_Iris_v20250425_121328.ttl\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"def iso8601(ms):\\\\n\\\",\\n+    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"Convert milliseconds since epoch to ISO8601 UTC.\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    return datetime.fromtimestamp(ms / 1000, tz=timezone.utc).isoformat()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"for json_path in glob.glob(\\\\\\\"MODEL_PROVENANCE/*/*_run_summary.json\\\\\\\"):\\\\n\\\",\\n+    \\\"    basename   = os.path.basename(json_path)\\\\n\\\",\\n+    \\\"    model_name = basename.rsplit(\\\\\\\"_run_summary.json\\\\\\\", 1)[0]\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    with open(json_path, \\\\\\\"r\\\\\\\", encoding=\\\\\\\"utf-8\\\\\\\") as f:\\\\n\\\",\\n+    \\\"        summary = json.load(f)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    #\\u2013\\u2013 Minimal override context: keep all your flat fields as-is,\\\\n\\\",\\n+    \\\"    #\\u2013\\u2013 and only map the actual PROV terms to their IRIs.\\\\n\\\",\\n+    \\\"    ctx = {\\\\n\\\",\\n+    \\\"        # keep these flat\\\\n\\\",\\n+    \\\"        \\\\\\\"run_id\\\\\\\":       { \\\\\\\"@id\\\\\\\": \\\\\\\"run_id\\\\\\\" },\\\\n\\\",\\n+    \\\"        \\\\\\\"run_name\\\\\\\":     { \\\\\\\"@id\\\\\\\": \\\\\\\"run_name\\\\\\\" },\\\\n\\\",\\n+    \\\"        \\\\\\\"experiment_id\\\\\\\":{ \\\\\\\"@id\\\\\\\": \\\\\\\"experiment_id\\\\\\\" },\\\\n\\\",\\n+    \\\"        \\\\\\\"params\\\\\\\":       { \\\\\\\"@id\\\\\\\": \\\\\\\"params\\\\\\\" },\\\\n\\\",\\n+    \\\"        \\\\\\\"metrics\\\\\\\":      { \\\\\\\"@id\\\\\\\": \\\\\\\"metrics\\\\\\\" },\\\\n\\\",\\n+    \\\"        \\\\\\\"artifacts\\\\\\\":    { \\\\\\\"@id\\\\\\\": \\\\\\\"artifacts\\\\\\\" },\\\\n\\\",\\n+    \\\"        \\\\\\\"tags\\\\\\\":         { \\\\\\\"@id\\\\\\\": \\\\\\\"tags\\\\\\\" },\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        # provenance namespace\\\\n\\\",\\n+    \\\"        \\\\\\\"prov\\\\\\\": \\\\\\\"http://www.w3.org/ns/prov#\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"xsd\\\\\\\":  \\\\\\\"http://www.w3.org/2001/XMLSchema#\\\\\\\",\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        # map your timestamp fields into PROV\\\\n\\\",\\n+    \\\"        \\\\\\\"start_time\\\\\\\": { \\\\\\\"@id\\\\\\\": \\\\\\\"prov:startedAtTime\\\\\\\", \\\\\\\"@type\\\\\\\": \\\\\\\"xsd:dateTime\\\\\\\" },\\\\n\\\",\\n+    \\\"        \\\\\\\"end_time\\\\\\\":   { \\\\\\\"@id\\\\\\\": \\\\\\\"prov:endedAtTime\\\\\\\",   \\\\\\\"@type\\\\\\\": \\\\\\\"xsd:dateTime\\\\\\\" },\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        # PROV-used/generated\\\\n\\\",\\n+    \\\"        \\\\\\\"used\\\\\\\":      { \\\\\\\"@id\\\\\\\": \\\\\\\"prov:used\\\\\\\",      \\\\\\\"@type\\\\\\\": \\\\\\\"@id\\\\\\\" },\\\\n\\\",\\n+    \\\"        \\\\\\\"generated\\\\\\\": { \\\\\\\"@id\\\\\\\": \\\\\\\"prov:generated\\\\\\\", \\\\\\\"@type\\\\\\\": \\\\\\\"@id\\\\\\\" },\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        # JSON-LD boilerplate\\\\n\\\",\\n+    \\\"        \\\\\\\"@id\\\\\\\":   \\\\\\\"@id\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"@type\\\\\\\": \\\\\\\"@type\\\\\\\"\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    #\\u2013\\u2013 Build JSON-LD document, re-using your original keys verbatim\\\\n\\\",\\n+    \\\"    doc = {\\\\n\\\",\\n+    \\\"        \\\\\\\"@context\\\\\\\":      ctx,\\\\n\\\",\\n+    \\\"        \\\\\\\"run_id\\\\\\\":        summary[\\\\\\\"run_id\\\\\\\"],\\\\n\\\",\\n+    \\\"        \\\\\\\"run_name\\\\\\\":      summary.get(\\\\\\\"run_name\\\\\\\"),\\\\n\\\",\\n+    \\\"        \\\\\\\"experiment_id\\\\\\\": summary.get(\\\\\\\"experiment_id\\\\\\\"),\\\\n\\\",\\n+    \\\"        \\\\\\\"params\\\\\\\":        summary.get(\\\\\\\"params\\\\\\\", {}),\\\\n\\\",\\n+    \\\"        \\\\\\\"metrics\\\\\\\":       summary.get(\\\\\\\"metrics\\\\\\\", {}),\\\\n\\\",\\n+    \\\"        \\\\\\\"artifacts\\\\\\\":     summary.get(\\\\\\\"artifacts\\\\\\\", []),\\\\n\\\",\\n+    \\\"        \\\\\\\"tags\\\\\\\":          summary.get(\\\\\\\"tags\\\\\\\", {}),\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        # PROV fields:\\\\n\\\",\\n+    \\\"        \\\\\\\"start_time\\\\\\\": iso8601(summary[\\\\\\\"start_time\\\\\\\"])\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    if summary.get(\\\\\\\"end_time\\\\\\\") is not None:\\\\n\\\",\\n+    \\\"        doc[\\\\\\\"end_time\\\\\\\"] = iso8601(summary[\\\\\\\"end_time\\\\\\\"])\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # for used/generated, just point at your dataset/model URIs\\\\n\\\",\\n+    \\\"    # (or blank-node them if you prefer richer structure)\\\\n\\\",\\n+    \\\"    doc[\\\\\\\"used\\\\\\\"] = summary.get(\\\\\\\"tags\\\\\\\", {}).get(\\\\\\\"dataset_uri\\\\\\\") or []\\\\n\\\",\\n+    \\\"    doc[\\\\\\\"generated\\\\\\\"] = [\\\\n\\\",\\n+    \\\"        art.get(\\\\\\\"uri\\\\\\\") or art.get(\\\\\\\"path\\\\\\\")\\\\n\\\",\\n+    \\\"        for art in summary.get(\\\\\\\"artifacts\\\\\\\", [])\\\\n\\\",\\n+    \\\"    ]\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    #\\u2013\\u2013 write JSON-LD\\\\n\\\",\\n+    \\\"    out_jsonld = os.path.join(\\\\\\\"MODEL_PROVENANCE\\\\\\\", model_name, f\\\\\\\"{model_name}.jsonld\\\\\\\")\\\\n\\\",\\n+    \\\"    with open(out_jsonld, \\\\\\\"w\\\\\\\", encoding=\\\\\\\"utf-8\\\\\\\") as f:\\\\n\\\",\\n+    \\\"        json.dump(doc, f, indent=2)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    #\\u2013\\u2013 parse & serialize to Turtle\\\\n\\\",\\n+    \\\"    g = Graph().parse(data=json.dumps(doc), format=\\\\\\\"json-ld\\\\\\\")\\\\n\\\",\\n+    \\\"    out_ttl = os.path.join(\\\\\\\"MODEL_PROVENANCE\\\\\\\", model_name, f\\\\\\\"{model_name}.ttl\\\\\\\")\\\\n\\\",\\n+    \\\"    g.serialize(destination=out_ttl, format=\\\\\\\"turtle\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"Converted {basename} \\u2192 {os.path.basename(out_jsonld)}, {os.path.basename(out_ttl)}\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"83d6d524-01da-4f20-8131-0d4a3ac005e2\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"This code programatically, finds diff between generated Json file and created JsonLD and .TTL file to make it easier to understand if there is any discrepency\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 67,\\n+   \\\"id\\\": \\\"77a420c0-230d-41c0-9b63-f3dbbca1e670\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"== JSON-LD vs TTL ==\\\\n\\\",\\n+      \\\"Change summary:\\\\n\\\",\\n+      \\\"type\\\\n\\\",\\n+      \\\"changed    1 \\\\n\\\",\\n+      \\\"\\\\n\\\",\\n+      \\\"First 10 \\u2018changed\\u2019 entries:\\\\n\\\",\\n+      \\\"Top-level adds/removes:\\\\n\\\",\\n+      \\\"Empty DataFrame\\\\n\\\",\\n+      \\\"Columns: [path, type, a, b]\\\\n\\\",\\n+      \\\"Index: []\\\\n\\\",\\n+      \\\"\\\\n\\\",\\n+      \\\"== JSON vs JSON-LD ==\\\\n\\\",\\n+      \\\"Change summary:\\\\n\\\",\\n+      \\\"type\\\\n\\\",\\n+      \\\"added      3\\\\n\\\",\\n+      \\\"removed    1\\\\n\\\",\\n+      \\\"changed    1 \\\\n\\\",\\n+      \\\"\\\\n\\\",\\n+      \\\"First 10 \\u2018changed\\u2019 entries:\\\\n\\\",\\n+      \\\"Top-level adds/removes:\\\\n\\\",\\n+      \\\"Empty DataFrame\\\\n\\\",\\n+      \\\"Columns: [path, type, a, b]\\\\n\\\",\\n+      \\\"Index: []\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def load_as_dict(path):\\\\n\\\",\\n+    \\\"    if path.endswith((\\\\\\\".ttl\\\\\\\", \\\\\\\".turtle\\\\\\\")):\\\\n\\\",\\n+    \\\"        g = Graph()\\\\n\\\",\\n+    \\\"        g.parse(path, format=\\\\\\\"turtle\\\\\\\")\\\\n\\\",\\n+    \\\"        # normalize to JSON-LD dict\\\\n\\\",\\n+    \\\"        return json.loads(g.serialize(format=\\\\\\\"json-ld\\\\\\\", indent=2))\\\\n\\\",\\n+    \\\"    else:\\\\n\\\",\\n+    \\\"        with open(path, encoding=\\\\\\\"utf-8\\\\\\\") as f:\\\\n\\\",\\n+    \\\"            return json.load(f)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def compare_json(a, b, path=\\\\\\\"\\\\\\\"):\\\\n\\\",\\n+    \\\"    diffs = []\\\\n\\\",\\n+    \\\"    if isinstance(a, dict) and isinstance(b, dict):\\\\n\\\",\\n+    \\\"        all_keys = set(a) | set(b)\\\\n\\\",\\n+    \\\"        for k in all_keys:\\\\n\\\",\\n+    \\\"            new_path = f\\\\\\\"{path}/{k}\\\\\\\" if path else k\\\\n\\\",\\n+    \\\"            if k not in a:\\\\n\\\",\\n+    \\\"                diffs.append({\\\\\\\"path\\\\\\\": new_path, \\\\\\\"type\\\\\\\": \\\\\\\"added\\\\\\\",   \\\\\\\"a\\\\\\\": None,    \\\\\\\"b\\\\\\\": b[k]})\\\\n\\\",\\n+    \\\"            elif k not in b:\\\\n\\\",\\n+    \\\"                diffs.append({\\\\\\\"path\\\\\\\": new_path, \\\\\\\"type\\\\\\\": \\\\\\\"removed\\\\\\\", \\\\\\\"a\\\\\\\": a[k],   \\\\\\\"b\\\\\\\": None})\\\\n\\\",\\n+    \\\"            else:\\\\n\\\",\\n+    \\\"                diffs.extend(compare_json(a[k], b[k], new_path))\\\\n\\\",\\n+    \\\"    elif isinstance(a, list) and isinstance(b, list):\\\\n\\\",\\n+    \\\"        for i, (ia, ib) in enumerate(zip(a, b)):\\\\n\\\",\\n+    \\\"            diffs.extend(compare_json(ia, ib, f\\\\\\\"{path}[{i}]\\\\\\\"))\\\\n\\\",\\n+    \\\"        # handle length mismatches\\\\n\\\",\\n+    \\\"        if len(a) < len(b):\\\\n\\\",\\n+    \\\"            for i in range(len(a), len(b)):\\\\n\\\",\\n+    \\\"                diffs.append({\\\\\\\"path\\\\\\\": f\\\\\\\"{path}[{i}]\\\\\\\", \\\\\\\"type\\\\\\\": \\\\\\\"added\\\\\\\",   \\\\\\\"a\\\\\\\": None,  \\\\\\\"b\\\\\\\": b[i]})\\\\n\\\",\\n+    \\\"        elif len(a) > len(b):\\\\n\\\",\\n+    \\\"            for i in range(len(b), len(a)):\\\\n\\\",\\n+    \\\"                diffs.append({\\\\\\\"path\\\\\\\": f\\\\\\\"{path}[{i}]\\\\\\\", \\\\\\\"type\\\\\\\": \\\\\\\"removed\\\\\\\", \\\\\\\"a\\\\\\\": a[i],  \\\\\\\"b\\\\\\\": None})\\\\n\\\",\\n+    \\\"    else:\\\\n\\\",\\n+    \\\"        if a != b:\\\\n\\\",\\n+    \\\"            diffs.append({\\\\\\\"path\\\\\\\": path, \\\\\\\"type\\\\\\\": \\\\\\\"changed\\\\\\\", \\\\\\\"a\\\\\\\": a, \\\\\\\"b\\\\\\\": b})\\\\n\\\",\\n+    \\\"    return diffs\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# --- Usage example -----------------------------------------------\\\\n\\\",\\n+    \\\"# REPO/notebooks/RQ_notebooks/MODEL_PROVENANCE/RandomForest_Iris_v20250425_121328/RandomForest_Iris_v20250425_121328_run_summary.json\\\\n\\\",\\n+    \\\"# # Compare JSON-LD vs Turtle:\\\\n\\\",\\n+    \\\"# a = load_as_dict(\\\\\\\"MODEL_PROVENANCE/RandomForest_Iris_v20250423_230422_run_summary.json\\\\\\\")\\\\n\\\",\\n+    \\\"# b = load_as_dict(\\\\\\\"MODEL_PROVENANCE/RandomForest_Iris_v20250423_230422.ttl\\\\\\\")\\\\n\\\",\\n+    \\\"# diffs_jsonld_vs_ttl = compare_json(a, b)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# # Compare JSON vs JSON-LD:\\\\n\\\",\\n+    \\\"# c = load_as_dict(\\\\\\\"MODEL_PROVENANCE/RandomForest_Iris_v20250423_230422_run_summary.json\\\\\\\")\\\\n\\\",\\n+    \\\"# d = load_as_dict(\\\\\\\"MODEL_PROVENANCE/RandomForest_Iris_v20250423_230422.jsonld\\\\\\\")\\\\n\\\",\\n+    \\\"# diffs_json_vs_jsonld = compare_json(c, d)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Define base directory\\\\n\\\",\\n+    \\\"base_dir = os.path.join(\\\\\\\"MODEL_PROVENANCE\\\\\\\", model_name)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Build full paths for the files to compare\\\\n\\\",\\n+    \\\"summary_json    = os.path.join(base_dir, f\\\\\\\"{model_name}_run_summary.json\\\\\\\")\\\\n\\\",\\n+    \\\"turtle_file     = os.path.join(base_dir, f\\\\\\\"{model_name}.ttl\\\\\\\")\\\\n\\\",\\n+    \\\"jsonld_file     = os.path.join(base_dir, f\\\\\\\"{model_name}.jsonld\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Load files\\\\n\\\",\\n+    \\\"a = load_as_dict(summary_json)\\\\n\\\",\\n+    \\\"b = load_as_dict(turtle_file)\\\\n\\\",\\n+    \\\"c = load_as_dict(summary_json)\\\\n\\\",\\n+    \\\"d = load_as_dict(jsonld_file)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Perform comparisons\\\\n\\\",\\n+    \\\"diffs_jsonld_vs_ttl = compare_json(a, b)\\\\n\\\",\\n+    \\\"diffs_json_vs_jsonld = compare_json(c, d)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Build DataFrames for interactive inspection\\\\n\\\",\\n+    \\\"df1 = pd.DataFrame(diffs_jsonld_vs_ttl)\\\\n\\\",\\n+    \\\"df2 = pd.DataFrame(diffs_json_vs_jsonld)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# --- Summaries & Filtering ---------------------------------------\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def summarize_and_preview(df, preview_n=10):\\\\n\\\",\\n+    \\\"    print(\\\\\\\"Change summary:\\\\\\\")\\\\n\\\",\\n+    \\\"    print(df['type'].value_counts().to_string(), \\\\\\\"\\\\\\\\n\\\\\\\")\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    print(f\\\\\\\"First {preview_n} \\u2018changed\\u2019 entries:\\\\\\\")\\\\n\\\",\\n+    \\\"    # print(df[df['type']==\\\\\\\"changed\\\\\\\"].head(preview_n).to_string(index=False), \\\\\\\"\\\\\\\\n\\\\\\\")\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # Top\\u2010level (one slash) adds/removes\\\\n\\\",\\n+    \\\"    top = df[df['path'].str.count(\\\\\\\"/\\\\\\\") == 1]\\\\n\\\",\\n+    \\\"    print(\\\\\\\"Top-level adds/removes:\\\\\\\")\\\\n\\\",\\n+    \\\"    print(top[top['type'].isin(['added','removed'])].to_string(index=False))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"print(\\\\\\\"== JSON-LD vs TTL ==\\\\\\\")\\\\n\\\",\\n+    \\\"summarize_and_preview(df1)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"print(\\\\\\\"\\\\\\\\n== JSON vs JSON-LD ==\\\\\\\")\\\\n\\\",\\n+    \\\"summarize_and_preview(df2)\\\\n\\\",\\n+    \\\"\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 68,\\n+   \\\"id\\\": \\\"41af9d6e-c683-45f9-bac1-296611b4d0b9\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"Removed in JSON-LD comparison:\\\\n\\\",\\n+      \\\"    path\\\\n\\\",\\n+      \\\"end_time\\\\n\\\",\\n+      \\\"\\\\n\\\",\\n+      \\\"Added in JSON-LD comparison:\\\\n\\\",\\n+      \\\"     path\\\\n\\\",\\n+      \\\" @context\\\\n\\\",\\n+      \\\"     used\\\\n\\\",\\n+      \\\"generated\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"# show all the removed paths (in JSON but not in JSON-LD)\\\\n\\\",\\n+    \\\"print(\\\\\\\"Removed in JSON-LD comparison:\\\\\\\")\\\\n\\\",\\n+    \\\"print(df2[df2['type']==\\\\\\\"removed\\\\\\\"][['path']].to_string(index=False))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# show all the added paths (in JSON-LD but not in JSON)\\\\n\\\",\\n+    \\\"print(\\\\\\\"\\\\\\\\nAdded in JSON-LD comparison:\\\\\\\")\\\\n\\\",\\n+    \\\"print(df2[df2['type']==\\\\\\\"added\\\\\\\"][['path']].to_string(index=False))\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 69,\\n+   \\\"id\\\": \\\"f0d6f92a-5cd9-4c78-9c2a-0cd3247137c6\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"Removed in .ttl comparison:\\\\n\\\",\\n+      \\\"Empty DataFrame\\\\n\\\",\\n+      \\\"Columns: [path]\\\\n\\\",\\n+      \\\"Index: []\\\\n\\\",\\n+      \\\"\\\\n\\\",\\n+      \\\"Added in .ttl comparison:\\\\n\\\",\\n+      \\\"Empty DataFrame\\\\n\\\",\\n+      \\\"Columns: [path]\\\\n\\\",\\n+      \\\"Index: []\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"# show all the removed paths (in JSON but not in JSON-LD)\\\\n\\\",\\n+    \\\"print(\\\\\\\"Removed in .ttl comparison:\\\\\\\")\\\\n\\\",\\n+    \\\"print(df1[df1['type']==\\\\\\\"removed\\\\\\\"][['path']].to_string(index=False))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# show all the added paths (in JSON-LD but not in JSON)\\\\n\\\",\\n+    \\\"print(\\\\\\\"\\\\\\\\nAdded in .ttl comparison:\\\\\\\")\\\\n\\\",\\n+    \\\"print(df1[df1['type']==\\\\\\\"added\\\\\\\"][['path']].to_string(index=False))\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"69efd0d0-9277-4efa-88cf-d2fd1b90d74c\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"Checks for completeness and mapping and time taken, needs work #TODO\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 70,\\n+   \\\"id\\\": \\\"165a13eb-7679-4f4c-b346-24f25da72cce\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"0/0 runs passed completeness checks (0.0%).\\\\n\\\",\\n+      \\\"\\\\n\\\",\\n+      \\\"Mapping integrity: 0/0 runs have zero diffs \\u2014 0.0%\\\\n\\\",\\n+      \\\"Overall quality score: 0.0%\\\\n\\\",\\n+      \\\"\\\\n\\\",\\n+      \\\"Benchmarking train_and_log() overhead:\\\\n\\\",\\n+      \\\"  \\u2022 No MLflow : 0.502s\\\\n\\\",\\n+      \\\"  \\u2022 With MLflow: 0.601s\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"# \\u2500\\u2500 User configuration \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Which keys must appear in every run_summary.json?\\\\n\\\",\\n+    \\\"REQUIRED_TOPLEVEL = {\\\\n\\\",\\n+    \\\"    \\\\\\\"run_id\\\\\\\", \\\\\\\"start_time\\\\\\\", \\\\\\\"end_time\\\\\\\",\\\\n\\\",\\n+    \\\"    \\\\\\\"params\\\\\\\", \\\\\\\"metrics\\\\\\\", \\\\\\\"tags\\\\\\\", \\\\\\\"artifacts\\\\\\\"\\\\n\\\",\\n+    \\\"}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# A couple of sub-fields we also want to spot-check:\\\\n\\\",\\n+    \\\"REQUIRED_PARAMS  = {\\\\\\\"random_state\\\\\\\"}\\\\n\\\",\\n+    \\\"REQUIRED_METRICS = {\\\\\\\"accuracy\\\\\\\"}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"JSON_SUMMARIES = glob.glob(\\\\\\\"MODEL_PROVENANCE/*_run_summary.json\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# \\u2500\\u2500 Helpers \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def iso8601(ms):\\\\n\\\",\\n+    \\\"    return datetime.fromtimestamp(ms/1000, tz=timezone.utc).isoformat()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def load_json(path):\\\\n\\\",\\n+    \\\"    with open(path, \\\\\\\"r\\\\\\\", encoding=\\\\\\\"utf-8\\\\\\\") as f:\\\\n\\\",\\n+    \\\"        return json.load(f)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def write_json(path, obj):\\\\n\\\",\\n+    \\\"    with open(path, \\\\\\\"w\\\\\\\", encoding=\\\\\\\"utf-8\\\\\\\") as f:\\\\n\\\",\\n+    \\\"        json.dump(obj, f, indent=2)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def convert_to_jsonld_and_ttl(summary, basename):\\\\n\\\",\\n+    \\\"    # build @context\\\\n\\\",\\n+    \\\"    ctx = {\\\\n\\\",\\n+    \\\"        \\\\\\\"prov\\\\\\\":    \\\\\\\"http://www.w3.org/ns/prov#\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"xsd\\\\\\\":     \\\\\\\"http://www.w3.org/2001/XMLSchema#\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"run\\\\\\\":     \\\\\\\"prov:Activity\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"start\\\\\\\":   \\\\\\\"prov:startedAtTime\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"end\\\\\\\":     \\\\\\\"prov:endedAtTime\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"used\\\\\\\":    \\\\\\\"prov:used\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"gen\\\\\\\":     \\\\\\\"prov:generated\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"param\\\\\\\":   \\\\\\\"prov:hadParameter\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"metric\\\\\\\":  \\\\\\\"prov:hadQuality\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"entity\\\\\\\":  \\\\\\\"prov:Entity\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"label\\\\\\\":   \\\\\\\"prov:label\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"value\\\\\\\":   \\\\\\\"prov:value\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"version\\\\\\\": \\\\\\\"prov:hadRevision\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"id\\\\\\\":      \\\\\\\"@id\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"type\\\\\\\":    \\\\\\\"@type\\\\\\\"\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    jsonld = {\\\\n\\\",\\n+    \\\"        \\\\\\\"@context\\\\\\\": ctx,\\\\n\\\",\\n+    \\\"        \\\\\\\"@id\\\\\\\":      f\\\\\\\"urn:run:{summary['run_id']}\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"@type\\\\\\\":    \\\\\\\"run\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"start\\\\\\\": {\\\\n\\\",\\n+    \\\"            \\\\\\\"@type\\\\\\\":  \\\\\\\"xsd:dateTime\\\\\\\",\\\\n\\\",\\n+    \\\"            \\\\\\\"@value\\\\\\\": iso8601(summary[\\\\\\\"start_time\\\\\\\"])\\\\n\\\",\\n+    \\\"        }\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"    if summary.get(\\\\\\\"end_time\\\\\\\") is not None:\\\\n\\\",\\n+    \\\"        jsonld[\\\\\\\"end\\\\\\\"] = {\\\\n\\\",\\n+    \\\"            \\\\\\\"@type\\\\\\\":  \\\\\\\"xsd:dateTime\\\\\\\",\\\\n\\\",\\n+    \\\"            \\\\\\\"@value\\\\\\\": iso8601(summary[\\\\\\\"end_time\\\\\\\"])\\\\n\\\",\\n+    \\\"        }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # params\\\\n\\\",\\n+    \\\"    jsonld[\\\\\\\"param\\\\\\\"] = [\\\\n\\\",\\n+    \\\"        {\\\\\\\"@type\\\\\\\":\\\\\\\"entity\\\\\\\",\\\\\\\"label\\\\\\\":k,\\\\\\\"value\\\\\\\":str(v)}\\\\n\\\",\\n+    \\\"        for k,v in summary.get(\\\\\\\"params\\\\\\\",{}).items()\\\\n\\\",\\n+    \\\"    ]\\\\n\\\",\\n+    \\\"    # metrics\\\\n\\\",\\n+    \\\"    jsonld[\\\\\\\"metric\\\\\\\"] = [\\\\n\\\",\\n+    \\\"        {\\\\\\\"@type\\\\\\\":\\\\\\\"entity\\\\\\\",\\\\\\\"label\\\\\\\":k,\\\\n\\\",\\n+    \\\"         \\\\\\\"value\\\\\\\":{\\\\\\\"@type\\\\\\\":\\\\\\\"xsd:decimal\\\\\\\",\\\\\\\"@value\\\\\\\":v}}\\\\n\\\",\\n+    \\\"        for k,v in summary.get(\\\\\\\"metrics\\\\\\\",{}).items()\\\\n\\\",\\n+    \\\"    ]\\\\n\\\",\\n+    \\\"    # artifacts\\\\n\\\",\\n+    \\\"    jsonld[\\\\\\\"gen\\\\\\\"] = [\\\\n\\\",\\n+    \\\"        {\\\\n\\\",\\n+    \\\"            \\\\\\\"@type\\\\\\\":\\\\\\\"entity\\\\\\\",\\\\n\\\",\\n+    \\\"            \\\\\\\"label\\\\\\\": art.get(\\\\\\\"path\\\\\\\") or art.get(\\\\\\\"label\\\\\\\"),\\\\n\\\",\\n+    \\\"            \\\\\\\"prov:location\\\\\\\": (\\\\n\\\",\\n+    \\\"                art.get(\\\\\\\"uri\\\\\\\")\\\\n\\\",\\n+    \\\"                or (art.get(\\\\\\\"content\\\\\\\",\\\\\\\"\\\\\\\")[:30]+\\\\\\\"\\u2026\\\\\\\")\\\\n\\\",\\n+    \\\"                if isinstance(art.get(\\\\\\\"content\\\\\\\"),str)\\\\n\\\",\\n+    \\\"                else \\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"            )\\\\n\\\",\\n+    \\\"        }\\\\n\\\",\\n+    \\\"        for art in summary.get(\\\\\\\"artifacts\\\\\\\",[])\\\\n\\\",\\n+    \\\"    ]\\\\n\\\",\\n+    \\\"    # dataset used\\\\n\\\",\\n+    \\\"    jsonld[\\\\\\\"used\\\\\\\"] = {\\\\n\\\",\\n+    \\\"        \\\\\\\"@type\\\\\\\":\\\\\\\"entity\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"label\\\\\\\": summary[\\\\\\\"tags\\\\\\\"].get(\\\\\\\"dataset_name\\\\\\\"),\\\\n\\\",\\n+    \\\"        \\\\\\\"version\\\\\\\": summary[\\\\\\\"tags\\\\\\\"].get(\\\\\\\"dataset_version\\\\\\\")\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # write JSON-LD\\\\n\\\",\\n+    \\\"    out_jsonld = f\\\\\\\"MODEL_PROVENANCE/{basename}.jsonld\\\\\\\"\\\\n\\\",\\n+    \\\"    write_json(out_jsonld, jsonld)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # serialize TTL\\\\n\\\",\\n+    \\\"    g = Graph().parse(data=json.dumps(jsonld), format=\\\\\\\"json-ld\\\\\\\")\\\\n\\\",\\n+    \\\"    out_ttl = f\\\\\\\"MODEL_PROVENANCE/{basename}.ttl\\\\\\\"\\\\n\\\",\\n+    \\\"    g.serialize(destination=out_ttl, format=\\\\\\\"turtle\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    return out_jsonld, out_ttl\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def normalize_jsonld(js):\\\\n\\\",\\n+    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"Simple deep-sort so compare_json doesn\\u2019t trip over ordering.\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    if isinstance(js, dict):\\\\n\\\",\\n+    \\\"        return {k: normalize_jsonld(js[k]) for k in sorted(js)}\\\\n\\\",\\n+    \\\"    if isinstance(js, list):\\\\n\\\",\\n+    \\\"        return sorted((normalize_jsonld(el) for el in js),\\\\n\\\",\\n+    \\\"                      key=lambda x: json.dumps(x, sort_keys=True))\\\\n\\\",\\n+    \\\"    return js\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def diff_roundtrip(orig_json, jsonld_path, ttl_path):\\\\n\\\",\\n+    \\\"    orig = load_json(orig_json)\\\\n\\\",\\n+    \\\"    ld   = load_json(jsonld_path)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # parse TTL back to JSON-LD\\\\n\\\",\\n+    \\\"    g = Graph().parse(ttl_path, format=\\\\\\\"turtle\\\\\\\")\\\\n\\\",\\n+    \\\"    ttl_as_ld = json.loads(g.serialize(format=\\\\\\\"json-ld\\\\\\\"))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # normalize\\\\n\\\",\\n+    \\\"    nl = normalize_jsonld(ld)\\\\n\\\",\\n+    \\\"    nt = normalize_jsonld(ttl_as_ld)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    return {\\\\n\\\",\\n+    \\\"        \\\\\\\"orig_vs_jsonld\\\\\\\":   compare_json(orig, ld),\\\\n\\\",\\n+    \\\"        \\\\\\\"jsonld_vs_ttl_ld\\\\\\\": compare_json(nl, nt)\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# \\u2500\\u2500 Main flow \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def main():\\\\n\\\",\\n+    \\\"    ok = 0\\\\n\\\",\\n+    \\\"    total = len(JSON_SUMMARIES)\\\\n\\\",\\n+    \\\"    missing_reports = []\\\\n\\\",\\n+    \\\"    cases = {}  # store diff results per run\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    for js_path in JSON_SUMMARIES:\\\\n\\\",\\n+    \\\"        summary = load_json(js_path)\\\\n\\\",\\n+    \\\"        base    = os.path.basename(js_path).split(\\\\\\\"_run_summary.json\\\\\\\")[0]\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        # 1) completeness check\\\\n\\\",\\n+    \\\"        if not REQUIRED_TOPLEVEL.issubset(summary):\\\\n\\\",\\n+    \\\"            missing = REQUIRED_TOPLEVEL - set(summary)\\\\n\\\",\\n+    \\\"            missing_reports.append((js_path, f\\\\\\\"missing fields {missing}\\\\\\\"))\\\\n\\\",\\n+    \\\"            continue\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        if not (REQUIRED_PARAMS <= summary[\\\\\\\"params\\\\\\\"].keys()):\\\\n\\\",\\n+    \\\"            missing_reports.append((js_path, f\\\\\\\"params missing {REQUIRED_PARAMS - summary['params'].keys()}\\\\\\\"))\\\\n\\\",\\n+    \\\"            continue\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        if not (REQUIRED_METRICS <= summary[\\\\\\\"metrics\\\\\\\"].keys()):\\\\n\\\",\\n+    \\\"            missing_reports.append((js_path, f\\\\\\\"metrics missing {REQUIRED_METRICS - summary['metrics'].keys()}\\\\\\\"))\\\\n\\\",\\n+    \\\"            continue\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        ok += 1\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        # 2) convert\\\\n\\\",\\n+    \\\"        jsonld_path, ttl_path = convert_to_jsonld_and_ttl(summary, base)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        # 3) diff\\\\n\\\",\\n+    \\\"        diffs = diff_roundtrip(js_path, jsonld_path, ttl_path)\\\\n\\\",\\n+    \\\"        cases[base] = diffs\\\\n\\\",\\n+    \\\"        print(f\\\\\\\"\\\\\\\\n\\u2500\\u2500 {base} diffs \\u2500\\u2500\\\\\\\")\\\\n\\\",\\n+    \\\"        print(\\\\\\\"  \\u2022 JSON \\u2192 JSON-LD:\\\\\\\", len(diffs[\\\\\\\"orig_vs_jsonld\\\\\\\"]), \\\\\\\"differences\\\\\\\")\\\\n\\\",\\n+    \\\"        print(\\\\\\\"  \\u2022 JSON-LD \\u2192 TTL \\u2192 JSON-LD:\\\\\\\", len(diffs[\\\\\\\"jsonld_vs_ttl_ld\\\\\\\"]), \\\\\\\"differences\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 4) completeness summary\\\\n\\\",\\n+    \\\"    completeness_pct = (100 * ok / total) if total else 0\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"\\\\\\\\n{ok}/{total} runs passed completeness checks ({completeness_pct:.1f}%).\\\\\\\")\\\\n\\\",\\n+    \\\"    if missing_reports:\\\\n\\\",\\n+    \\\"        print(\\\\\\\"\\\\\\\\nFailures:\\\\\\\")\\\\n\\\",\\n+    \\\"        for path, reason in missing_reports:\\\\n\\\",\\n+    \\\"            print(f\\\\\\\" \\u2022 {path}: {reason}\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 5) integrity check\\\\n\\\",\\n+    \\\"    total_runs = len(cases)\\\\n\\\",\\n+    \\\"    zero_diff_runs = sum(\\\\n\\\",\\n+    \\\"        1\\\\n\\\",\\n+    \\\"        for diffs in cases.values()\\\\n\\\",\\n+    \\\"        if not diffs[\\\\\\\"orig_vs_jsonld\\\\\\\"] and not diffs[\\\\\\\"jsonld_vs_ttl_ld\\\\\\\"]\\\\n\\\",\\n+    \\\"    )\\\\n\\\",\\n+    \\\"    integrity_pct = (100 * zero_diff_runs / total_runs) if total_runs else 0\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"\\\\\\\\nMapping integrity: {zero_diff_runs}/{total_runs} runs have zero diffs \\u2014 {integrity_pct:.1f}%\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 6) overall quality score\\\\n\\\",\\n+    \\\"    overall_score = (completeness_pct + integrity_pct) / 2\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"Overall quality score: {overall_score:.1f}%\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 7) Benchmark your training fn\\\\n\\\",\\n+    \\\"    print(\\\\\\\"\\\\\\\\nBenchmarking train_and_log() overhead:\\\\\\\")\\\\n\\\",\\n+    \\\"    def train_and_log(use_mlflow=False):\\\\n\\\",\\n+    \\\"        # \\u2190 your real instrumentation + fit logic here\\\\n\\\",\\n+    \\\"        time.sleep(0.5 + (0.1 if use_mlflow else 0))  # stub\\\\n\\\",\\n+    \\\"        return\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    for flag in (False, True):\\\\n\\\",\\n+    \\\"        start = time.time()\\\\n\\\",\\n+    \\\"        train_and_log(use_mlflow=flag)\\\\n\\\",\\n+    \\\"        elapsed = time.time() - start\\\\n\\\",\\n+    \\\"        label = \\\\\\\"With MLflow\\\\\\\" if flag else \\\\\\\"No MLflow\\\\\\\"\\\\n\\\",\\n+    \\\"        print(f\\\\\\\"  \\u2022 {label:10s}: {elapsed:.3f}s\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"if __name__ == \\\\\\\"__main__\\\\\\\":\\\\n\\\",\\n+    \\\"    main()\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"5883f673-371e-415e-a73e-5c9c88b56fb1\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"RQ2  implementation\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 72,\\n+   \\\"id\\\": \\\"6d07ac1c-ea80-4787-bcb9-da047d12167d\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"text/plain\\\": [\\n+       \\\"Index(['run_id', 'param_bootstrap', 'param_ccp_alpha', 'param_class_weight',\\\\n\\\",\\n+       \\\"       'param_columns_raw', 'param_criterion', 'param_database.description',\\\\n\\\",\\n+       \\\"       'param_database.id', 'param_database.name', 'param_database.owner',\\\\n\\\",\\n+       \\\"       'param_dataset.authors', 'param_dataset.doi', 'param_dataset.published',\\\\n\\\",\\n+       \\\"       'param_dataset.publisher', 'param_dataset.title',\\\\n\\\",\\n+       \\\"       'param_dropped_columns', 'param_feature_names',\\\\n\\\",\\n+       \\\"       'param_matplotlib_version', 'param_max_depth', 'param_max_features',\\\\n\\\",\\n+       \\\"       'param_max_leaf_nodes', 'param_max_samples',\\\\n\\\",\\n+       \\\"       'param_min_impurity_decrease', 'param_min_samples_leaf',\\\\n\\\",\\n+       \\\"       'param_min_samples_split', 'param_min_weight_fraction_leaf',\\\\n\\\",\\n+       \\\"       'param_numpy_version', 'param_n_estimators', 'param_n_features',\\\\n\\\",\\n+       \\\"       'param_n_features_final', 'param_n_jobs', 'param_n_records',\\\\n\\\",\\n+       \\\"       'param_n_test_samples', 'param_n_train_samples', 'param_oob_score',\\\\n\\\",\\n+       \\\"       'param_os_platform', 'param_pandas_version', 'param_python_version',\\\\n\\\",\\n+       \\\"       'param_random_state', 'param_retrieval_time', 'param_seaborn_version',\\\\n\\\",\\n+       \\\"       'param_shap_version', 'param_sklearn_version', 'param_test_size',\\\\n\\\",\\n+       \\\"       'param_verbose', 'param_warm_start', 'metric_accuracy',\\\\n\\\",\\n+       \\\"       'metric_accuracy_score_X_test', 'metric_dbrepo.num_deletes',\\\\n\\\",\\n+       \\\"       'metric_dbrepo.num_inserts', 'metric_dbrepo.row_count_end',\\\\n\\\",\\n+       \\\"       'metric_dbrepo.row_count_start', 'metric_f1_macro',\\\\n\\\",\\n+       \\\"       'metric_f1_score_X_test', 'metric_precision_macro',\\\\n\\\",\\n+       \\\"       'metric_precision_score_X_test', 'metric_recall_macro',\\\\n\\\",\\n+       \\\"       'metric_recall_score_X_test', 'metric_roc_auc',\\\\n\\\",\\n+       \\\"       'metric_roc_auc_score_X_test', 'metric_training_accuracy_score',\\\\n\\\",\\n+       \\\"       'metric_training_f1_score', 'metric_training_log_loss',\\\\n\\\",\\n+       \\\"       'metric_training_precision_score', 'metric_training_recall_score',\\\\n\\\",\\n+       \\\"       'metric_training_roc_auc', 'metric_training_score', 'tag_dataset_id',\\\\n\\\",\\n+       \\\"       'tag_dataset_name', 'tag_dataset_version', 'tag_data_source',\\\\n\\\",\\n+       \\\"       'tag_dbrepo.admin_email', 'tag_dbrepo.base_url',\\\\n\\\",\\n+       \\\"       'tag_dbrepo.granularity', 'tag_dbrepo.protocol_version',\\\\n\\\",\\n+       \\\"       'tag_dbrepo.repository_name', 'tag_dbrepo.table_last_modified',\\\\n\\\",\\n+       \\\"       'tag_estimator_class', 'tag_estimator_name',\\\\n\\\",\\n+       \\\"       'tag_git_current_commit_hash', 'tag_git_previous_commit_hash',\\\\n\\\",\\n+       \\\"       'tag_git__current_commit_url', 'tag_mlflow.log-model.history',\\\\n\\\",\\n+       \\\"       'tag_mlflow.runName', 'tag_mlflow.source.name',\\\\n\\\",\\n+       \\\"       'tag_mlflow.source.type', 'tag_mlflow.user', 'tag_model_name',\\\\n\\\",\\n+       \\\"       'tag_notebook_name', 'tag_target_name', 'tag_training_end_time',\\\\n\\\",\\n+       \\\"       'tag_training_start_time'],\\\\n\\\",\\n+       \\\"      dtype='object')\\\"\\n+      ]\\n+     },\\n+     \\\"execution_count\\\": 72,\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"execute_result\\\"\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"# Load all run summary JSON files\\\\n\\\",\\n+    \\\"files = glob.glob(\\\\\\\"MODEL_PROVENANCE/*/*_run_summary.json\\\\\\\")\\\\n\\\",\\n+    \\\"rows = []\\\\n\\\",\\n+    \\\"for f in files:\\\\n\\\",\\n+    \\\"    with open(f) as fh:\\\\n\\\",\\n+    \\\"        summary = json.load(fh)\\\\n\\\",\\n+    \\\"    # Flatten parameters and metrics\\\\n\\\",\\n+    \\\"    row = {\\\\\\\"run_id\\\\\\\": summary[\\\\\\\"run_id\\\\\\\"]}\\\\n\\\",\\n+    \\\"    row.update({f\\\\\\\"param_{k}\\\\\\\": v for k, v in summary.get(\\\\\\\"params\\\\\\\", {}).items()})\\\\n\\\",\\n+    \\\"    row.update({f\\\\\\\"metric_{k}\\\\\\\": v for k, v in summary.get(\\\\\\\"metrics\\\\\\\", {}).items()})\\\\n\\\",\\n+    \\\"    row.update({f\\\\\\\"tag_{k}\\\\\\\": v for k, v in summary.get(\\\\\\\"tags\\\\\\\", {}).items()})\\\\n\\\",\\n+    \\\"    rows.append(row)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Create DataFrame\\\\n\\\",\\n+    \\\"df = pd.DataFrame(rows)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Display the DataFrame\\\\n\\\",\\n+    \\\"df.columns\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"ba148da6-6ce5-45cf-a985-f164a53c969b\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"1) Tracing preprocessing steps\\\\n\\\",\\n+    \\\":\\\\n\\\",\\n+    \\\"Here are the top 4 Iris\\u2010focused preprocessing\\u2010tracing use cases I\\u2019d tackle first:\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"Reconstruct a run\\u2019s exact preprocessing\\\\n\\\",\\n+    \\\"Fetch a run\\u2019s run_id, columns_raw, dropped_columns, feature_names and test_size so you can replay the exact data pull & split.\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"Feature\\u2010drop impact analysis\\\\n\\\",\\n+    \\\"Identify runs where one or more measurements (e.g. petalwidthcm) were dropped and compare their test accuracies.\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"Best feature subset discovery\\\\n\\\",\\n+    \\\"Group runs by which features they used (sepals only vs petals only vs both) and rank them by test F1 or accuracy.\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"Common steps in high-accuracy runs\\\\n\\\",\\n+    \\\"Filter for runs with accuracy_score_X_test \\u2265 0.95 and list the shared preprocessing settings (dropped columns, test_size, etc.).\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 73,\\n+   \\\"id\\\": \\\"6e147555-afbf-4bba-b6da-7e90ff391920\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 12:23:44 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'df84c36b36cc4ebd90a999db3ebc4ad4', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"[{'run_id': '28f01e38b7f04d2f948fe21f57f41d0c', 'param_dataset.title': 'Scikit-Learn Iris', 'param_columns_raw': \\\\\\\"['id', 'sepallengthcm', 'sepalwidthcm', 'petallengthcm', 'petalwidthcm', 'species']\\\\\\\", 'param_dropped_columns': \\\\\\\"['id']\\\\\\\", 'param_feature_names': \\\\\\\"['sepallengthcm', 'sepalwidthcm', 'petallengthcm', 'petalwidthcm']\\\\\\\", 'param_dataset.authors': '[\\\\\\\"Marshall Michael\\\\\\\"]', 'param_dataset.doi': '10.5281/ZENODO.1404173', 'param_dataset.published': '2018-8-27', 'param_test_size': '0.2', 'param_criterion': 'entropy', 'param_max_depth': '12', 'param_max_leaf_nodes': 'None', 'param_max_samples': 'None', 'metric_accuracy': 1.0, 'metric_f1_macro': 1.0, 'metric_roc_auc': 1.0}]\\\\n\\\",\\n+      \\\"[]\\\\n\\\",\\n+      \\\"[{'param_dropped_columns': \\\\\\\"['id']\\\\\\\", 'param_test_size': '0.2', 'param_feature_names': \\\\\\\"['sepallengthcm', 'sepalwidthcm', 'petallengthcm', 'petalwidthcm']\\\\\\\"}]\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"d931b602947d4db8872f254d48e22027\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 12:23:52 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '41261519e1a643c5b1335701aee1bf95', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"{'features': ['sepallengthcm', 'sepalwidthcm'], 'accuracy': 0.7666666666666667}\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"83ff1224205a4a8eb0c351a7f299dd93\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 12:23:58 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'e0955d231fa6488e9339086b5845064c', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"1eaa6c141e064593b73b6c72ce0b00cf\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 12:24:04 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '21d299b426ac42a0ad799604e9e7ff88', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"{'dropped_feature': 'petallengthcm', 'baseline_acc': 1.0, 'dropped_acc': 0.9666666666666667, 'impact': 0.033333333333333326}\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"4c04ce12f62f49a29f48509b1483f16b\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 12:24:10 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '8ca4591a1b53402f854187104d1e7ee0', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"66bf06c45648410daa144c12f85658c6\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 12:24:16 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '0c8a66e5e4b244f9a6a8e9fa02d26828', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"0d71b6d9b58d4e5a9db241baeaa79d53\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 12:24:22 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '53994143a51e481abd23e988be2466b1', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"3fa13db4a66940d59cf37a30cb7a3cbc\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 12:24:28 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '35588f1cd8c34ce28770848de714d3c4', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"273c026f2e0b464f98090472792b3a87\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"[{'dropped_feature': 'sepallengthcm', 'baseline_acc': 1.0, 'dropped_acc': 1.0, 'impact': 0.0}, {'dropped_feature': 'sepalwidthcm', 'baseline_acc': 1.0, 'dropped_acc': 1.0, 'impact': 0.0}, {'dropped_feature': 'petallengthcm', 'baseline_acc': 1.0, 'dropped_acc': 0.9666666666666667, 'impact': 0.0333}, {'dropped_feature': 'petalwidthcm', 'baseline_acc': 1.0, 'dropped_acc': 0.9666666666666667, 'impact': 0.0333}]\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Helper to get the \\u201cofficial\\u201d feature_names from your summary DF\\\\n\\\",\\n+    \\\"def _get_all_features(df):\\\\n\\\",\\n+    \\\"    # assumes every row has the same param_feature_names\\\\n\\\",\\n+    \\\"    raw = df.loc[0, 'param_feature_names']\\\\n\\\",\\n+    \\\"    return ast.literal_eval(raw)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Train & eval RF on just these columns of Iris\\\\n\\\",\\n+    \\\"def evaluate_subset(features, test_size=0.2, random_state=42, n_estimators=200):\\\\n\\\",\\n+    \\\"    iris = load_iris()\\\\n\\\",\\n+    \\\"    X = pd.DataFrame(iris.data, columns=iris.feature_names)\\\\n\\\",\\n+    \\\"    # map sklearn\\u2019s names to your param names, e.g. \\\\\\\"sepal length (cm)\\\\\\\" \\u2192 \\\\\\\"sepallengthcm\\\\\\\"\\\\n\\\",\\n+    \\\"    canon = _get_all_features(df)\\\\n\\\",\\n+    \\\"    mapping = dict(zip(iris.feature_names, canon))\\\\n\\\",\\n+    \\\"    X = X.rename(columns=mapping)\\\\n\\\",\\n+    \\\"    X_sub = X[features]\\\\n\\\",\\n+    \\\"    y = iris.target\\\\n\\\",\\n+    \\\"    Xtr, Xte, ytr, yte = train_test_split(X_sub, y, test_size=test_size, random_state=random_state)\\\\n\\\",\\n+    \\\"    m = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\\\\n\\\",\\n+    \\\"    m.fit(Xtr, ytr)\\\\n\\\",\\n+    \\\"    return accuracy_score(yte, m.predict(Xte))\\\\n\\\",\\n+    \\\"def trace_preprocessing(df, run_id=None):\\\\n\\\",\\n+    \\\"    cols = ['run_id',\\\\n\\\",\\n+    \\\"            'param_dataset.title',\\\\n\\\",\\n+    \\\"            'param_columns_raw',\\\\n\\\",\\n+    \\\"            'param_dropped_columns',\\\\n\\\",\\n+    \\\"            'param_feature_names',\\\\n\\\",\\n+    \\\"            'param_dataset.authors', 'param_dataset.doi', 'param_dataset.published',\\\\n\\\",\\n+    \\\"            'param_test_size',\\\\n\\\",\\n+    \\\"            'param_criterion',\\\\n\\\",\\n+    \\\"            'param_max_depth','param_max_leaf_nodes', 'param_max_samples',\\\\n\\\",\\n+    \\\"           'metric_accuracy','metric_f1_macro','metric_roc_auc']\\\\n\\\",\\n+    \\\"    if run_id is None:\\\\n\\\",\\n+    \\\"        subset = df.loc[:, cols]\\\\n\\\",\\n+    \\\"    else:\\\\n\\\",\\n+    \\\"        subset = df.loc[df['run_id'] == run_id, cols]\\\\n\\\",\\n+    \\\"    return subset.to_dict(orient='records')\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def drop_impact(df, feature, **_):\\\\n\\\",\\n+    \\\"    all_feats = _get_all_features(df)\\\\n\\\",\\n+    \\\"    baseline = evaluate_subset(all_feats)\\\\n\\\",\\n+    \\\"    without = [f for f in all_feats if f!=feature]\\\\n\\\",\\n+    \\\"    dropped = evaluate_subset(without)\\\\n\\\",\\n+    \\\"    return {\\\\n\\\",\\n+    \\\"      'dropped_feature': feature,\\\\n\\\",\\n+    \\\"      'baseline_acc': baseline,\\\\n\\\",\\n+    \\\"      'dropped_acc': dropped,\\\\n\\\",\\n+    \\\"      'impact': baseline - dropped\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def drop_impact_all(df: pd.DataFrame) -> List[Dict[str, Any]]:\\\\n\\\",\\n+    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    Compute drop-impact for every feature in the dataset.\\\\n\\\",\\n+    \\\"    Returns list of dicts with dropped_feature, baseline_acc, dropped_acc, impact.\\\\n\\\",\\n+    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    feats = _get_all_features(df)\\\\n\\\",\\n+    \\\"    baseline = evaluate_subset(feats)\\\\n\\\",\\n+    \\\"    summary = []\\\\n\\\",\\n+    \\\"    for feat in feats:\\\\n\\\",\\n+    \\\"        without = [f for f in feats if f != feat]\\\\n\\\",\\n+    \\\"        acc = evaluate_subset(without)\\\\n\\\",\\n+    \\\"        summary.append({\\\\n\\\",\\n+    \\\"            'dropped_feature': feat,\\\\n\\\",\\n+    \\\"            'baseline_acc': baseline,\\\\n\\\",\\n+    \\\"            'dropped_acc': acc,\\\\n\\\",\\n+    \\\"            'impact': round(baseline - acc, 4)\\\\n\\\",\\n+    \\\"        })\\\\n\\\",\\n+    \\\"    return summary\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def best_feature_subset(df, features, **_):\\\\n\\\",\\n+    \\\"    acc = evaluate_subset(features)\\\\n\\\",\\n+    \\\"    return {'features': features, 'accuracy': acc}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def common_high_accuracy(df: pd.DataFrame, threshold: float = 0.95) -> List[Dict[str, Any]]:\\\\n\\\",\\n+    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    Filter runs with test accuracy >= threshold and list unique shared preprocessing settings.\\\\n\\\",\\n+    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    high = df[df['metric_accuracy_score_X_test'] >= threshold]\\\\n\\\",\\n+    \\\"    cols = ['param_dropped_columns', 'param_test_size', 'param_feature_names']\\\\n\\\",\\n+    \\\"    return high[cols].drop_duplicates().to_dict(orient='records')\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# --------------------------------------------\\\\n\\\",\\n+    \\\"# Use Case Registry with parameter order for minimal input\\\\n\\\",\\n+    \\\"# --------------------------------------------\\\\n\\\",\\n+    \\\"USE_CASES = {\\\\n\\\",\\n+    \\\"    'trace_preprocessing': {\\\\n\\\",\\n+    \\\"        'func': trace_preprocessing,\\\\n\\\",\\n+    \\\"        'required_params': [],            # none strictly required\\\\n\\\",\\n+    \\\"        'optional_params': ['run_id'],    # run_id can be supplied or not\\\\n\\\",\\n+    \\\"    },\\\\n\\\",\\n+    \\\"    'drop_impact': {\\\\n\\\",\\n+    \\\"        'func': drop_impact,\\\\n\\\",\\n+    \\\"        'required_params': ['feature'],\\\\n\\\",\\n+    \\\"        'optional_params': [],\\\\n\\\",\\n+    \\\"    },\\\\n\\\",\\n+    \\\"     'drop_impact_all': {\\\\n\\\",\\n+    \\\"        'func': drop_impact_all,\\\\n\\\",\\n+    \\\"        'required_params': [],\\\\n\\\",\\n+    \\\"        'optional_params': [],\\\\n\\\",\\n+    \\\"    },\\\\n\\\",\\n+    \\\"    'best_feature_subset': {\\\\n\\\",\\n+    \\\"        'func': best_feature_subset,\\\\n\\\",\\n+    \\\"        'required_params': ['features'],\\\\n\\\",\\n+    \\\"        'optional_params': [],\\\\n\\\",\\n+    \\\"    },\\\\n\\\",\\n+    \\\"    'common_high_accuracy': {\\\\n\\\",\\n+    \\\"        'func': common_high_accuracy,\\\\n\\\",\\n+    \\\"        'required_params': ['threshold'],\\\\n\\\",\\n+    \\\"        'optional_params': [],\\\\n\\\",\\n+    \\\"    },\\\\n\\\",\\n+    \\\"}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def call_use_case(df, use_case_name, **kwargs):\\\\n\\\",\\n+    \\\"    if use_case_name not in USE_CASES:\\\\n\\\",\\n+    \\\"        raise ValueError(f\\\\\\\"Unknown use case: {use_case_name}\\\\\\\")\\\\n\\\",\\n+    \\\"    case = USE_CASES[use_case_name]\\\\n\\\",\\n+    \\\"    func = case['func']\\\\n\\\",\\n+    \\\"    # check required\\\\n\\\",\\n+    \\\"    missing = [p for p in case['required_params'] if p not in kwargs]\\\\n\\\",\\n+    \\\"    if missing:\\\\n\\\",\\n+    \\\"        raise ValueError(f\\\\\\\"{use_case_name} missing required params: {missing}\\\\\\\")\\\\n\\\",\\n+    \\\"    # build args\\\\n\\\",\\n+    \\\"    args = {p: kwargs[p] for p in case['required_params']}\\\\n\\\",\\n+    \\\"    for p in case['optional_params']:\\\\n\\\",\\n+    \\\"        args[p] = kwargs.get(p)\\\\n\\\",\\n+    \\\"    return func(df, **args)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# --------------------------------------------\\\\n\\\",\\n+    \\\"# Example Usage\\\\n\\\",\\n+    \\\"# --------------------------------------------\\\\n\\\",\\n+    \\\"if __name__ == '__main__':\\\\n\\\",\\n+    \\\"   # # 1) trace_preprocessing for all runs\\\\n\\\",\\n+    \\\"    print(call_use_case(df, 'trace_preprocessing'))\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # 2) trace_preprocessing for a single run_id\\\\n\\\",\\n+    \\\"    print(call_use_case(df, 'trace_preprocessing', run_id='361daa12f99f4129a06cd20b78dd6fa7'))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 5) common_high_accuracy\\\\n\\\",\\n+    \\\"    print(call_use_case(df, 'common_high_accuracy', threshold=0.99))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 4) Best\\u2010subset on just sepals:\\\\n\\\",\\n+    \\\"    print(call_use_case(df, 'best_feature_subset', features=['sepallengthcm','sepalwidthcm']))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 3) Drop\\u2010impact for \\u201cpetallengthcm\\u201d:\\\\n\\\",\\n+    \\\"    print(call_use_case(df, 'drop_impact', feature='petallengthcm'))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    print(call_use_case(df, 'drop_impact_all'))  # summary for all features\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"96f912d6-0e84-4155-858a-9668bef63f6e\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\" \\u2022 Detecting models trained with deprecated code versions\\\\n\\\",\\n+    \\\" \\u2022 Mapping models to specific datasets used during training\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 74,\\n+   \\\"id\\\": \\\"34a02c9a-5459-478f-a3c5-7f7a58ff22b0\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"[{'param_dataset.doi': '10.5281/ZENODO.1404173',\\\\n\\\",\\n+      \\\"  'param_dataset.published': '2018-8-27',\\\\n\\\",\\n+      \\\"  'param_dataset.publisher': 'Zenodo',\\\\n\\\",\\n+      \\\"  'param_dataset.title': 'Scikit-Learn Iris',\\\\n\\\",\\n+      \\\"  'run_id': '28f01e38b7f04d2f948fe21f57f41d0c',\\\\n\\\",\\n+      \\\"  'tag_model_name': 'RandomForest_Iris_v20250425_121328'}]\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"def detect_deprecated_code(df: pd.DataFrame, deprecated_commits: List[str], **_) -> List[Dict[str, Any]]:\\\\n\\\",\\n+    \\\"    # we know the column is called tag_git_current_commit_hash\\\\n\\\",\\n+    \\\"    commit_col = 'tag_git_current_commit_hash'\\\\n\\\",\\n+    \\\"    if commit_col not in df.columns:\\\\n\\\",\\n+    \\\"        raise KeyError(f\\\\\\\"Missing {commit_col} in DataFrame\\\\\\\")\\\\n\\\",\\n+    \\\"    out = df[df[commit_col].isin(deprecated_commits)]\\\\n\\\",\\n+    \\\"    # include run_id and notebook/runName for context\\\\n\\\",\\n+    \\\"    cols = ['run_id', commit_col, 'tag_notebook_name', 'tag_mlflow.runName']\\\\n\\\",\\n+    \\\"    # drop any that don\\u2019t exist\\\\n\\\",\\n+    \\\"    cols = [c for c in cols if c in df.columns]\\\\n\\\",\\n+    \\\"    return out[cols].to_dict(orient='records')\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def map_model_dataset(df: pd.DataFrame, **_) -> List[Dict[str, Any]]:\\\\n\\\",\\n+    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    For each run, return its model name (or run_id) alongside the dataset\\\\n\\\",\\n+    \\\"    title, DOI, published date and publisher.\\\\n\\\",\\n+    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    # pick whichever model-name column you have\\\\n\\\",\\n+    \\\"    model_col = 'tag_model_name' if 'tag_model_name' in df.columns else 'param_model_name'\\\\n\\\",\\n+    \\\"    cols = [\\\\n\\\",\\n+    \\\"        'run_id',\\\\n\\\",\\n+    \\\"        model_col,\\\\n\\\",\\n+    \\\"        'param_dataset.title',\\\\n\\\",\\n+    \\\"        'param_dataset.doi',\\\\n\\\",\\n+    \\\"        'param_dataset.published',\\\\n\\\",\\n+    \\\"        'param_dataset.publisher'\\\\n\\\",\\n+    \\\"    ]\\\\n\\\",\\n+    \\\"    # filter out any columns that don\\u2019t actually exist\\\\n\\\",\\n+    \\\"    cols = [c for c in cols if c in df.columns]\\\\n\\\",\\n+    \\\"    return df[cols].to_dict(orient='records')\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# --------------------------------------------\\\\n\\\",\\n+    \\\"# Extend Use-Case Registry\\\\n\\\",\\n+    \\\"# --------------------------------------------\\\\n\\\",\\n+    \\\"USE_CASES.update({\\\\n\\\",\\n+    \\\"    'detect_deprecated_code': {\\\\n\\\",\\n+    \\\"        'func': detect_deprecated_code,\\\\n\\\",\\n+    \\\"        'required_params': ['deprecated_commits'],\\\\n\\\",\\n+    \\\"        'optional_params': []\\\\n\\\",\\n+    \\\"    },\\\\n\\\",\\n+    \\\"    'map_model_dataset': {\\\\n\\\",\\n+    \\\"        'func': map_model_dataset,\\\\n\\\",\\n+    \\\"        'required_params': [],\\\\n\\\",\\n+    \\\"        'optional_params': []\\\\n\\\",\\n+    \\\"    },\\\\n\\\",\\n+    \\\"})\\\\n\\\",\\n+    \\\"# 1) Detect runs on deprecated commits:\\\\n\\\",\\n+    \\\"deprecated = [\\\\n\\\",\\n+    \\\"    \\\\\\\"a07434af4f547af2daab044d6873eb7081162293\\\\\\\",\\\\n\\\",\\n+    \\\"    \\\\\\\"d329c92495e196ec0f39fbb19dfdd367131a77d9\\\\\\\"\\\\n\\\",\\n+    \\\"]\\\\n\\\",\\n+    \\\"# print(call_use_case(df, \\\\\\\"detect_deprecated_code\\\\\\\", deprecated_commits=deprecated))\\\\n\\\",\\n+    \\\"pprint(call_use_case(df, 'map_model_dataset'))\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"c52607ad-5849-4a2d-97ef-e8fc1ca16dc7\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"Goal: Notify collaborators who have forked the GitHub repo if their fork is outdated (i.e., behind the current commit used to train a model).\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"f29c8ad9-00bb-4c1e-ac3b-ee6861991acd\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"\\ud83e\\udde0 What We Need\\\\n\\\",\\n+    \\\"Current training run\\u2019s Git commit hash\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"GitHub API to fetch all forks of your repo\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"Compare each fork\\u2019s main or master branch head commit\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"Create an issue on their fork or on your repo tagging them if they\\u2019re behind\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"c72bed50-fb56-442d-a21e-bb7991892d07\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\": Notify via issues on your own repo\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 75,\\n+   \\\"id\\\": \\\"852f147c-9d0a-4d7f-a4ab-545d1e2375fb\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"Do you want to notify collaborators whose forks are behind? (y/N):  N\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"No action taken.\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"def notify_outdated_forks():\\\\n\\\",\\n+    \\\"    load_dotenv()\\\\n\\\",\\n+    \\\"    token     = os.getenv(\\\\\\\"THESIS_TOKEN\\\\\\\")\\\\n\\\",\\n+    \\\"    owner     = \\\\\\\"reema-dass26\\\\\\\"\\\\n\\\",\\n+    \\\"    repo      = \\\\\\\"REPO\\\\\\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    if not token:\\\\n\\\",\\n+    \\\"        print(\\\\\\\"\\u26a0\\ufe0f GITHUB_TOKEN not set.\\\\\\\")\\\\n\\\",\\n+    \\\"        return\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    headers = {\\\\n\\\",\\n+    \\\"        \\\\\\\"Authorization\\\\\\\": f\\\\\\\"token {token}\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"Accept\\\\\\\":        \\\\\\\"application/vnd.github.v3+json\\\\\\\"\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 1) Get latest upstream commit\\\\n\\\",\\n+    \\\"    main_commits = requests.get(\\\\n\\\",\\n+    \\\"        f\\\\\\\"https://api.github.com/repos/{owner}/{repo}/commits\\\\\\\",\\\\n\\\",\\n+    \\\"        headers=headers,\\\\n\\\",\\n+    \\\"        params={\\\\\\\"per_page\\\\\\\": 1}\\\\n\\\",\\n+    \\\"    )\\\\n\\\",\\n+    \\\"    main_commits.raise_for_status()\\\\n\\\",\\n+    \\\"    new_commit_hash = main_commits.json()[0][\\\\\\\"sha\\\\\\\"]\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"Latest upstream commit: {new_commit_hash}\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 2) List forks\\\\n\\\",\\n+    \\\"    forks_resp = requests.get(f\\\\\\\"https://api.github.com/repos/{owner}/{repo}/forks\\\\\\\", headers=headers)\\\\n\\\",\\n+    \\\"    forks_resp.raise_for_status()\\\\n\\\",\\n+    \\\"    forks = forks_resp.json()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 3) Compare each fork\\\\n\\\",\\n+    \\\"    outdated = []\\\\n\\\",\\n+    \\\"    for fork in forks:\\\\n\\\",\\n+    \\\"        fork_owner = fork[\\\\\\\"owner\\\\\\\"][\\\\\\\"login\\\\\\\"]\\\\n\\\",\\n+    \\\"        fork_comm = requests.get(\\\\n\\\",\\n+    \\\"            fork[\\\\\\\"url\\\\\\\"] + \\\\\\\"/commits\\\\\\\",\\\\n\\\",\\n+    \\\"            headers=headers,\\\\n\\\",\\n+    \\\"            params={\\\\\\\"per_page\\\\\\\": 1}\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        if fork_comm.status_code != 200:\\\\n\\\",\\n+    \\\"            print(f\\\\\\\"\\u00a0\\u00a0\\u2013 could not fetch commits for {fork_owner}, skipping.\\\\\\\")\\\\n\\\",\\n+    \\\"            continue\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        fork_sha = fork_comm.json()[0][\\\\\\\"sha\\\\\\\"]\\\\n\\\",\\n+    \\\"        if fork_sha != new_commit_hash:\\\\n\\\",\\n+    \\\"            outdated.append(f\\\\\\\"@{fork_owner}\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 4) Open an issue if any are behind\\\\n\\\",\\n+    \\\"    if outdated:\\\\n\\\",\\n+    \\\"        title = \\\\\\\"\\ud83d\\udd14 Notification: Your fork is behind the latest commit\\\\\\\"\\\\n\\\",\\n+    \\\"        body  = (\\\\n\\\",\\n+    \\\"            f\\\\\\\"Hi {' '.join(outdated)},\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n\\\",\\n+    \\\"            f\\\\\\\"The main repository has been updated to commit `{new_commit_hash}`.\\\\\\\\n\\\\\\\"\\\\n\\\",\\n+    \\\"            \\\\\\\"Please consider pulling the latest changes to stay in sync.\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n\\\",\\n+    \\\"            \\\\\\\"Thanks!\\\\\\\"\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        issues_url = f\\\\\\\"https://api.github.com/repos/{owner}/{repo}/issues\\\\\\\"\\\\n\\\",\\n+    \\\"        resp = requests.post(\\\\n\\\",\\n+    \\\"        issues_url,\\\\n\\\",\\n+    \\\"        headers=headers,\\\\n\\\",\\n+    \\\"        json={\\\\\\\"title\\\\\\\": title, \\\\\\\"body\\\\\\\": body}\\\\n\\\",\\n+    \\\"    )\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # DEBUGGING OUTPUT\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"\\u2192 POST {issues_url}\\\\\\\")\\\\n\\\",\\n+    \\\"    print(\\\\\\\"\\u2192 Status code:\\\\\\\", resp.status_code)\\\\n\\\",\\n+    \\\"    print(\\\\\\\"\\u2192 Response headers:\\\\\\\", resp.headers)\\\\n\\\",\\n+    \\\"    try:\\\\n\\\",\\n+    \\\"        data = resp.json()\\\\n\\\",\\n+    \\\"        print(\\\\\\\"\\u2192 Response JSON:\\\\\\\", data)\\\\n\\\",\\n+    \\\"        print(\\\\\\\"\\u2192 html_url field:\\\\\\\", data.get(\\\\\\\"html_url\\\\\\\"))\\\\n\\\",\\n+    \\\"    except ValueError:\\\\n\\\",\\n+    \\\"        print(\\\\\\\"\\u2192 No JSON response body; raw text:\\\\\\\", resp.text)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"if __name__ == \\\\\\\"__main__\\\\\\\":\\\\n\\\",\\n+    \\\"    answer = input(\\\\\\\"Do you want to notify collaborators whose forks are behind? (y/N): \\\\\\\").strip().lower()\\\\n\\\",\\n+    \\\"    if answer in (\\\\\\\"y\\\\\\\", \\\\\\\"yes\\\\\\\"):\\\\n\\\",\\n+    \\\"        notify_outdated_forks()\\\\n\\\",\\n+    \\\"    else:\\\\n\\\",\\n+    \\\"        print(\\\\\\\"No action taken.\\\\\\\")\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"cda31f16-fbe9-40ce-ac1b-9ebc898c8820\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"INVENIO INTEGRETION to upload the necessary files and publish\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"c2e5a7fc-3b03-45c8-bc90-817ea5ba7352\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"############################################################################################\\\\n\\\",\\n+    \\\"# TEST CODE FOR INVENIO INTEGRETION\\\\n\\\",\\n+    \\\"#############################################################################################\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# API_BASE = \\\\\\\"https://127.0.0.1:5000\\\\\\\"\\\\n\\\",\\n+    \\\"# TOKEN    = \\\\\\\"8LnqJuz3TsBHffnDJ3isPLHYHtRbWrC0M667Nb5haEbnXpWqGbFRyfDApymr\\\\\\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# # 1) Test read\\u2010scope by listing records (no size param or size=1)\\\\n\\\",\\n+    \\\"# resp = requests.get(\\\\n\\\",\\n+    \\\"#     f\\\\\\\"{API_BASE}/api/records\\\\\\\",\\\\n\\\",\\n+    \\\"#     headers={\\\\\\\"Authorization\\\\\\\": f\\\\\\\"Bearer {TOKEN}\\\\\\\"},\\\\n\\\",\\n+    \\\"#     verify=False\\\\n\\\",\\n+    \\\"# )\\\\n\\\",\\n+    \\\"# print(resp.status_code)\\\\n\\\",\\n+    \\\"# # should be 200 and a JSON page of records\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# # or explicitly:\\\\n\\\",\\n+    \\\"# resp = requests.get(\\\\n\\\",\\n+    \\\"#     f\\\\\\\"{API_BASE}/api/records?size=1\\\\\\\",\\\\n\\\",\\n+    \\\"#     headers={\\\\\\\"Authorization\\\\\\\": f\\\\\\\"Bearer {TOKEN}\\\\\\\"},\\\\n\\\",\\n+    \\\"#     verify=False\\\\n\\\",\\n+    \\\"# )\\\\n\\\",\\n+    \\\"# print(resp.status_code, resp.json())\\\\n\\\",\\n+    \\\"# #################################################################################################\\\\n\\\",\\n+    \\\"# API_BASE = \\\\\\\"https://127.0.0.1:5000\\\\\\\"\\\\n\\\",\\n+    \\\"# TOKEN    = \\\\\\\"8LnqJuz3TsBHffnDJ3isPLHYHtRbWrC0M667Nb5haEbnXpWqGbFRyfDApymr\\\\\\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# resp = requests.options(\\\\n\\\",\\n+    \\\"#     f\\\\\\\"{API_BASE}/api/records\\\\\\\",\\\\n\\\",\\n+    \\\"#     headers={\\\\\\\"Authorization\\\\\\\": f\\\\\\\"Bearer {TOKEN}\\\\\\\"},\\\\n\\\",\\n+    \\\"#     verify=False\\\\n\\\",\\n+    \\\"# )\\\\n\\\",\\n+    \\\"# print(\\\\\\\"Allowed methods:\\\\\\\", resp.headers.get(\\\\\\\"Allow\\\\\\\"))\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"7e5b2cc5-ecf3-4e13-8cac-47f57f12cbdd\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"# Configuration\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"API_BASE   = \\\\\\\"https://127.0.0.1:5000\\\\\\\"\\\\n\\\",\\n+    \\\"TOKEN      = \\\\\\\"8LnqJuz3TsBHffnDJ3isPLHYHtRbWrC0M667Nb5haEbnXpWqGbFRyfDApymr\\\\\\\"\\\\n\\\",\\n+    \\\"VERIFY_SSL = False  # only for self\\u2010signed dev\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"HEADERS_JSON = {\\\\n\\\",\\n+    \\\"    \\\\\\\"Accept\\\\\\\":        \\\\\\\"application/json\\\\\\\",\\\\n\\\",\\n+    \\\"    \\\\\\\"Content-Type\\\\\\\":  \\\\\\\"application/json\\\\\\\",\\\\n\\\",\\n+    \\\"    \\\\\\\"Authorization\\\\\\\": f\\\\\\\"Bearer {TOKEN}\\\\\\\",\\\\n\\\",\\n+    \\\"}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"HEADERS_OCTET = {\\\\n\\\",\\n+    \\\"    \\\\\\\"Content-Type\\\\\\\":  \\\\\\\"application/octet-stream\\\\\\\",\\\\n\\\",\\n+    \\\"    \\\\\\\"Authorization\\\\\\\": f\\\\\\\"Bearer {TOKEN}\\\\\\\",\\\\n\\\",\\n+    \\\"}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# The folders you want to walk & upload:\\\\n\\\",\\n+    \\\"TO_UPLOAD = [\\\\\\\"Trained_models\\\\\\\", \\\\\\\"plots\\\\\\\", \\\\\\\"MODEL_PROVENANCE\\\\\\\"]\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"# 1) Create draft with ALL required metadata\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"def create_draft():\\\\n\\\",\\n+    \\\"    payload = {\\\\n\\\",\\n+    \\\"  \\\\\\\"metadata\\\\\\\": {\\\\n\\\",\\n+    \\\"    \\\\\\\"title\\\\\\\":            \\\\\\\"RandomForest Iris Model Artifacts\\\\\\\",\\\\n\\\",\\n+    \\\"    \\\\\\\"creators\\\\\\\": [ {\\\\n\\\",\\n+    \\\"      \\\\\\\"person_or_org\\\\\\\": {\\\\n\\\",\\n+    \\\"        \\\\\\\"type\\\\\\\":        \\\\\\\"personal\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"given_name\\\\\\\":  \\\\\\\"Reema\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"family_name\\\\\\\": \\\\\\\"Dass\\\\\\\"\\\\n\\\",\\n+    \\\"      }\\\\n\\\",\\n+    \\\"    } ],\\\\n\\\",\\n+    \\\"    \\\\\\\"publication_date\\\\\\\": \\\\\\\"2025-04-24\\\\\\\",\\\\n\\\",\\n+    \\\"    \\\\\\\"resource_type\\\\\\\":    { \\\\\\\"id\\\\\\\": \\\\\\\"software\\\\\\\" },\\\\n\\\",\\n+    \\\"    \\\\\\\"access\\\\\\\": {\\\\n\\\",\\n+    \\\"      \\\\\\\"record\\\\\\\": \\\\\\\"public\\\\\\\",\\\\n\\\",\\n+    \\\"      \\\\\\\"files\\\\\\\":  \\\\\\\"public\\\\\\\"\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"  }\\\\n\\\",\\n+    \\\"}\\\\n\\\",\\n+    \\\"    r = requests.post(f\\\\\\\"{API_BASE}/api/records\\\\\\\",\\\\n\\\",\\n+    \\\"                      headers=HEADERS_JSON,\\\\n\\\",\\n+    \\\"                      json=payload,\\\\n\\\",\\n+    \\\"                      verify=VERIFY_SSL)\\\\n\\\",\\n+    \\\"    r.raise_for_status()\\\\n\\\",\\n+    \\\"    draft = r.json()\\\\n\\\",\\n+    \\\"    print(\\\\\\\"\\u2705 Draft created:\\\\\\\", draft[\\\\\\\"id\\\\\\\"])\\\\n\\\",\\n+    \\\"    return draft[\\\\\\\"id\\\\\\\"], draft[\\\\\\\"links\\\\\\\"]\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"# 2) Register, upload and commit a single file\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"def upload_and_commit(links, key, path):\\\\n\\\",\\n+    \\\"    # 2a) register the filename in the draft\\\\n\\\",\\n+    \\\"    r1 = requests.post(links[\\\\\\\"files\\\\\\\"],\\\\n\\\",\\n+    \\\"                       headers=HEADERS_JSON,\\\\n\\\",\\n+    \\\"                       json=[{\\\\\\\"key\\\\\\\": key}],\\\\n\\\",\\n+    \\\"                       verify=VERIFY_SSL)\\\\n\\\",\\n+    \\\"    r1.raise_for_status()\\\\n\\\",\\n+    \\\"    entry = next(e for e in r1.json()[\\\\\\\"entries\\\\\\\"] if e[\\\\\\\"key\\\\\\\"] == key)\\\\n\\\",\\n+    \\\"    file_links = entry[\\\\\\\"links\\\\\\\"]\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 2b) upload the bytes\\\\n\\\",\\n+    \\\"    with open(path, \\\\\\\"rb\\\\\\\") as fp:\\\\n\\\",\\n+    \\\"        r2 = requests.put(file_links[\\\\\\\"content\\\\\\\"],\\\\n\\\",\\n+    \\\"                          headers=HEADERS_OCTET,\\\\n\\\",\\n+    \\\"                          data=fp,\\\\n\\\",\\n+    \\\"                          verify=VERIFY_SSL)\\\\n\\\",\\n+    \\\"    r2.raise_for_status()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 2c) commit the upload\\\\n\\\",\\n+    \\\"    r3 = requests.post(file_links[\\\\\\\"commit\\\\\\\"],\\\\n\\\",\\n+    \\\"                       headers=HEADERS_JSON,\\\\n\\\",\\n+    \\\"                       verify=VERIFY_SSL)\\\\n\\\",\\n+    \\\"    r3.raise_for_status()\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"  \\u2022 Uploaded {key}\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"# 3) Walk each folder and upload every file\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"def upload_folder(links):\\\\n\\\",\\n+    \\\"    for folder in TO_UPLOAD:\\\\n\\\",\\n+    \\\"        if not os.path.isdir(folder):\\\\n\\\",\\n+    \\\"            print(f\\\\\\\"\\u26a0\\ufe0f Skipping missing folder {folder}\\\\\\\")\\\\n\\\",\\n+    \\\"            continue\\\\n\\\",\\n+    \\\"        base = os.path.dirname(folder) or folder\\\\n\\\",\\n+    \\\"        for root, _, files in os.walk(folder):\\\\n\\\",\\n+    \\\"            for fn in files:\\\\n\\\",\\n+    \\\"                local = os.path.join(root, fn)\\\\n\\\",\\n+    \\\"                # create a POSIX\\u2010style key preserving subfolders\\\\n\\\",\\n+    \\\"                key = os.path.relpath(local, start=base).replace(os.sep, \\\\\\\"/\\\\\\\")\\\\n\\\",\\n+    \\\"                upload_and_commit(links, key, local)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"# 4) Publish the draft\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"def publish(links):\\\\n\\\",\\n+    \\\"    r = requests.post(links[\\\\\\\"publish\\\\\\\"],\\\\n\\\",\\n+    \\\"                      headers=HEADERS_JSON,\\\\n\\\",\\n+    \\\"                      verify=VERIFY_SSL)\\\\n\\\",\\n+    \\\"    if not r.ok:\\\\n\\\",\\n+    \\\"        print(\\\\\\\"\\u274c Publish failed:\\\\\\\", r.status_code, r.text)\\\\n\\\",\\n+    \\\"        try: print(r.json())\\\\n\\\",\\n+    \\\"        except: pass\\\\n\\\",\\n+    \\\"        r.raise_for_status()\\\\n\\\",\\n+    \\\"    print(\\\\\\\"\\u2705 Published:\\\\\\\", r.json()[\\\\\\\"id\\\\\\\"])\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"# 5) Fetch metadata and save to a file\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"def fetch_metadata(record_id):\\\\n\\\",\\n+    \\\"    r = requests.get(f\\\\\\\"{API_BASE}/api/records/{record_id}\\\\\\\",\\\\n\\\",\\n+    \\\"                     headers=HEADERS_JSON,\\\\n\\\",\\n+    \\\"                     verify=VERIFY_SSL)\\\\n\\\",\\n+    \\\"    r.raise_for_status()\\\\n\\\",\\n+    \\\"    metadata = r.json()\\\\n\\\",\\n+    \\\"    print(\\\\\\\"\\u2705 Metadata fetched successfully\\\\\\\")\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # Save the metadata to a file\\\\n\\\",\\n+    \\\"    with open(f\\\\\\\"metadata_{record_id}.json\\\\\\\", \\\\\\\"w\\\\\\\") as f:\\\\n\\\",\\n+    \\\"        json.dump(metadata, f, indent=4)\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"\\u2705 Metadata saved as metadata_{record_id}.json\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"# Main\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"if __name__ == \\\\\\\"__main__\\\\\\\":\\\\n\\\",\\n+    \\\"    recid, links = create_draft()\\\\n\\\",\\n+    \\\"    upload_folder(links)\\\\n\\\",\\n+    \\\"    publish(links)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # Fetch and save metadata after publishing\\\\n\\\",\\n+    \\\"    print(fetch_metadata(recid))\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"2f7423f2-0ff3-4104-913e-50eeb32d9d0f\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"METADATA EXTRACTION FROM INVENIO:\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"0013878b-37da-4a22-9586-3773531bfd01\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"# Function to dynamically extract and structure metadata from the original JSON\\\\n\\\",\\n+    \\\"def extract_metadata(metadata):\\\\n\\\",\\n+    \\\"    # Debug: Check if metadata is loaded correctly\\\\n\\\",\\n+    \\\"    print(\\\\\\\"Debug: Metadata loaded successfully\\\\\\\")\\\\n\\\",\\n+    \\\"    print(metadata.get(\\\\\\\"id\\\\\\\", \\\\\\\"\\\\\\\"))  # Check if 'id' is being fetched\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # Check if the required fields are in the metadata\\\\n\\\",\\n+    \\\"    print(\\\\\\\"Debug: Extracting fields from metadata...\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    extracted_data = {\\\\n\\\",\\n+    \\\"        \\\\\\\"invenio_metadata\\\\\\\": {\\\\n\\\",\\n+    \\\"            \\\\\\\"id\\\\\\\": metadata.get(\\\\\\\"id\\\\\\\", \\\\\\\"\\\\\\\"),\\\\n\\\",\\n+    \\\"            \\\\\\\"title\\\\\\\": metadata.get(\\\\\\\"metadata\\\\\\\", {}).get(\\\\\\\"title\\\\\\\", \\\\\\\"\\\\\\\"),\\\\n\\\",\\n+    \\\"            \\\\\\\"creator\\\\\\\": \\\\\\\", \\\\\\\".join([creator[\\\\\\\"person_or_org\\\\\\\"].get(\\\\\\\"name\\\\\\\", \\\\\\\"\\\\\\\") for creator in metadata.get(\\\\\\\"metadata\\\\\\\", {}).get(\\\\\\\"creators\\\\\\\", [])]),\\\\n\\\",\\n+    \\\"            \\\\\\\"publication_date\\\\\\\": metadata.get(\\\\\\\"metadata\\\\\\\", {}).get(\\\\\\\"publication_date\\\\\\\", \\\\\\\"\\\\\\\"),\\\\n\\\",\\n+    \\\"            \\\\\\\"files\\\\\\\": [],  # Initialize 'files' as a list\\\\n\\\",\\n+    \\\"            \\\\\\\"pids\\\\\\\": metadata.get(\\\\\\\"pids\\\\\\\", {}),\\\\n\\\",\\n+    \\\"            \\\\\\\"version_info\\\\\\\": metadata.get(\\\\\\\"versions\\\\\\\", {}),\\\\n\\\",\\n+    \\\"            \\\\\\\"status\\\\\\\": metadata.get(\\\\\\\"status\\\\\\\", \\\\\\\"\\\\\\\"),\\\\n\\\",\\n+    \\\"            \\\\\\\"views\\\\\\\": metadata.get(\\\\\\\"stats\\\\\\\", {}).get(\\\\\\\"this_version\\\\\\\", {}).get(\\\\\\\"views\\\\\\\", 0),\\\\n\\\",\\n+    \\\"            \\\\\\\"downloads\\\\\\\": metadata.get(\\\\\\\"stats\\\\\\\", {}).get(\\\\\\\"this_version\\\\\\\", {}).get(\\\\\\\"downloads\\\\\\\", 0),\\\\n\\\",\\n+    \\\"        }\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # Extract file details from the metadata\\\\n\\\",\\n+    \\\"    for key, file_info in metadata.get(\\\\\\\"files\\\\\\\", {}).get(\\\\\\\"entries\\\\\\\", {}).items():\\\\n\\\",\\n+    \\\"        file_detail = {\\\\n\\\",\\n+    \\\"            \\\\\\\"key\\\\\\\": key,\\\\n\\\",\\n+    \\\"            \\\\\\\"url\\\\\\\": file_info[\\\\\\\"links\\\\\\\"].get(\\\\\\\"content\\\\\\\", \\\\\\\"\\\\\\\"),\\\\n\\\",\\n+    \\\"            \\\\\\\"size\\\\\\\": file_info.get(\\\\\\\"size\\\\\\\", 0),\\\\n\\\",\\n+    \\\"            \\\\\\\"mimetype\\\\\\\": file_info.get(\\\\\\\"mimetype\\\\\\\", \\\\\\\"\\\\\\\"),\\\\n\\\",\\n+    \\\"            \\\\\\\"checksum\\\\\\\": file_info.get(\\\\\\\"checksum\\\\\\\", \\\\\\\"\\\\\\\"),\\\\n\\\",\\n+    \\\"            \\\\\\\"metadata\\\\\\\": file_info.get(\\\\\\\"metadata\\\\\\\", {}),\\\\n\\\",\\n+    \\\"        }\\\\n\\\",\\n+    \\\"        extracted_data[\\\\\\\"invenio_metadata\\\\\\\"][\\\\\\\"files\\\\\\\"].append(file_detail)  # Append to the 'files' list\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    return extracted_data\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Load the original metadata from the JSON file (replace with your actual file path)\\\\n\\\",\\n+    \\\"with open('metadata_p8a8y-1bn93.json', 'r') as f: \\\\n\\\",\\n+    \\\"    original_metadata = json.load(f)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Debugging: print out the first part of the original metadata to verify its structure\\\\n\\\",\\n+    \\\"print(\\\\\\\"Debug: Original Metadata (start):\\\\\\\", json.dumps(original_metadata, indent=4)[:1000])  # Print only the start for review\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Extract relevant details dynamically\\\\n\\\",\\n+    \\\"extracted_metadata = extract_metadata(original_metadata)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Debugging: print the extracted metadata to verify it's correct\\\\n\\\",\\n+    \\\"print(\\\\\\\"Debug: Extracted Metadata:\\\\\\\", json.dumps(extracted_metadata, indent=4))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Load the existing JSON file (replace with your actual file path)\\\\n\\\",\\n+    \\\"with open('MODEL_PROVENANCE/RandomForest_Iris_v20250424_111946_run_summary.json', 'r') as f:\\\\n\\\",\\n+    \\\"    existing_metadata = json.load(f)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Add the extracted metadata as a new node\\\\n\\\",\\n+    \\\"existing_metadata.update(extracted_metadata)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Save the updated metadata back to the file\\\\n\\\",\\n+    \\\"with open('updated_metadata.json', 'w') as f:\\\\n\\\",\\n+    \\\"    json.dump(existing_metadata, f, indent=4)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"print(\\\\\\\"\\u2705 New dynamic metadata added successfully!\\\\\\\")\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"38a807a7-6ecd-4ea7-93ac-78c0f853825c\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"# import mlflow\\\\n\\\",\\n+    \\\"# import mlflow.sklearn\\\\n\\\",\\n+    \\\"# from sklearn.datasets import load_iris\\\\n\\\",\\n+    \\\"# from sklearn.ensemble import RandomForestClassifier\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# X, y = load_iris(return_X_y=True)\\\\n\\\",\\n+    \\\"# mlflow.sklearn.autolog()\\\\n\\\",\\n+    \\\"# with mlflow.start_run() as run:\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"#     model = RandomForestClassifier(**hyperparams)\\\\n\\\",\\n+    \\\"#     model.fit(X_train, y_train)\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"570fa169-a5e2-47b3-b7f5-44f9577f22ad\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": []\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"f67b7a46-a70d-44ea-976c-322a1a795311\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": []\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"4211bdef-5785-472d-8ea5-0bc24a3faf3c\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": []\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"9d4d71f2-ef66-4e04-9d9b-c4b381d45590\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": []\\n+  }\\n+ ],\\n+ \\\"metadata\\\": {\\n+  \\\"kernelspec\\\": {\\n+   \\\"display_name\\\": \\\"Python 3 (ipykernel)\\\",\\n+   \\\"language\\\": \\\"python\\\",\\n+   \\\"name\\\": \\\"python3\\\"\\n+  },\\n+  \\\"language_info\\\": {\\n+   \\\"codemirror_mode\\\": {\\n+    \\\"name\\\": \\\"ipython\\\",\\n+    \\\"version\\\": 3\\n+   },\\n+   \\\"file_extension\\\": \\\".py\\\",\\n+   \\\"mimetype\\\": \\\"text/x-python\\\",\\n+   \\\"name\\\": \\\"python\\\",\\n+   \\\"nbconvert_exporter\\\": \\\"python\\\",\\n+   \\\"pygments_lexer\\\": \\\"ipython3\\\",\\n+   \\\"version\\\": \\\"3.11.5\\\"\\n+  }\\n+ },\\n+ \\\"nbformat\\\": 4,\\n+ \\\"nbformat_minor\\\": 5\\n+}\\ndiff --git a/notebooks/RQ_notebooks/RQ1_updated_integretingsklearn_autolog.ipynb b/notebooks/RQ_notebooks/RQ1_updated_integretingsklearn_autolog.ipynb\\nnew file mode 100644\\nindex 0000000..1ff819b\\n--- /dev/null\\n+++ b/notebooks/RQ_notebooks/RQ1_updated_integretingsklearn_autolog.ipynb\\n@@ -0,0 +1,3270 @@\\n+{\\n+ \\\"cells\\\": [\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 8,\\n+   \\\"id\\\": \\\"12fa6f59-927c-4003-964f-83e53793fd36\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"# TODO: atm the mlflow autolog isnt capturing metrics n params\\\\n\\\",\\n+    \\\"# and sklearn.autolog throws error( posted the issue on github)\\\\n\\\",\\n+    \\\"# Ideally, I should be able to fetch most of the imp detail via MLFLOW AUTOLOG. will check that later in time\\\\n\\\",\\n+    \\\"#============================\\\\n\\\",\\n+    \\\"# \\ud83e\\udde0 MLflow Autologging\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# mlflow.autolog(log_input_examples=True, log_model_signatures=True)\\\\n\\\",\\n+    \\\"# mlflow.sklearn.autolog() \\\\n\\\",\\n+    \\\"# mlflow.sklearn.autolog(\\\\n\\\",\\n+    \\\"#     log_input_examples=True,\\\\n\\\",\\n+    \\\"#     log_model_signatures=True,\\\\n\\\",\\n+    \\\"#     log_post_training_metrics=True,        # calls model.score() \\u2192 accuracy\\\\n\\\",\\n+    \\\"#     disable_for_unsupported_versions=True,  # skips if versions still wonky\\\\n\\\",\\n+    \\\"#     exclusive=True                          # only patch the sklearn integration\\\\n\\\",\\n+    \\\"# )\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 9,\\n+   \\\"id\\\": \\\"1ce1a579-f08b-40bd-b4db-21b388aaea74\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\u2699\\ufe0f Install Dependencies (if needed )\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# !pip install mlflow scikit-learn pandas numpy matplotlib seaborn shap requests GitPython\\\\n\\\",\\n+    \\\"# !pip install --upgrade threadpoolctl\\\\n\\\",\\n+    \\\"# !pip install setuptools\\\\n\\\",\\n+    \\\"# !pip install ace_tools \\\\n\\\",\\n+    \\\"# !pip install rdflib\\\\n\\\",\\n+    \\\"# !pip install streamlit-option-menu\\\\n\\\",\\n+    \\\"# !pip install streamlit-agraph\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"1ca4f0ae-39ee-4b22-b013-dfb1fa1b5694\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"LIBRARY IMPORTS:\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 51,\\n+   \\\"id\\\": \\\"8ca332e5-6501-4310-920b-2b769477b46e\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83d\\udce6 Standard Library Imports\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"import os\\\\n\\\",\\n+    \\\"import glob\\\\n\\\",\\n+    \\\"import io\\\\n\\\",\\n+    \\\"import json\\\\n\\\",\\n+    \\\"import time\\\\n\\\",\\n+    \\\"import ast\\\\n\\\",\\n+    \\\"import pickle\\\\n\\\",\\n+    \\\"import platform\\\\n\\\",\\n+    \\\"import subprocess\\\\n\\\",\\n+    \\\"from datetime import datetime, timezone\\\\n\\\",\\n+    \\\"from pprint import pprint\\\\n\\\",\\n+    \\\"from typing import List, Dict, Any\\\\n\\\",\\n+    \\\"import xml.etree.ElementTree as ET\\\\n\\\",\\n+    \\\"import urllib.parse\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83d\\udcca Data and Visualization\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"import pandas as pd\\\\n\\\",\\n+    \\\"import numpy as np\\\\n\\\",\\n+    \\\"import seaborn as sns\\\\n\\\",\\n+    \\\"import matplotlib\\\\n\\\",\\n+    \\\"import matplotlib.pyplot as plt\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83e\\udd16 Machine Learning\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"import sklearn\\\\n\\\",\\n+    \\\"from sklearn.datasets import load_iris\\\\n\\\",\\n+    \\\"from sklearn.ensemble import RandomForestClassifier\\\\n\\\",\\n+    \\\"from sklearn.model_selection import train_test_split\\\\n\\\",\\n+    \\\"from sklearn.preprocessing import LabelEncoder, label_binarize\\\\n\\\",\\n+    \\\"from sklearn.metrics import (\\\\n\\\",\\n+    \\\"    accuracy_score,\\\\n\\\",\\n+    \\\"    roc_auc_score,\\\\n\\\",\\n+    \\\"    confusion_matrix,\\\\n\\\",\\n+    \\\"    precision_score,\\\\n\\\",\\n+    \\\"    recall_score,\\\\n\\\",\\n+    \\\"    f1_score,\\\\n\\\",\\n+    \\\"    RocCurveDisplay,\\\\n\\\",\\n+    \\\"    PrecisionRecallDisplay\\\\n\\\",\\n+    \\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83d\\udd2c Experiment Tracking\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"import mlflow\\\\n\\\",\\n+    \\\"import mlflow.sklearn\\\\n\\\",\\n+    \\\"from mlflow import MlflowClient\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83c\\udf10 Web / API / Networking\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"import requests\\\\n\\\",\\n+    \\\"from dotenv import load_dotenv\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83e\\uddea Git & Version Control\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"import git\\\\n\\\",\\n+    \\\"from git import Repo, GitCommandError\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83e\\udde0 SHAP for Explainability\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"import shap\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83e\\uddec RDF & Provenance (rdflib)\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"from rdflib import Graph, URIRef, Literal\\\\n\\\",\\n+    \\\"from rdflib.namespace import PROV, XSD\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\u2699\\ufe0f System Monitoring\\\\n\\\",\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"import psutil\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"61d4d6b8-34a9-47b5-974d-5927c0ee2256\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"DBREPO INTEGRETION\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 32,\\n+   \\\"id\\\": \\\"8e3570e2-9a60-45b4-8653-28060071e728\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"API Response: [{'id': '1', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '2', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '3', 'sepallengthcm': '4.700000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '4', 'sepallengthcm': '4.600000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '5', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.600000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '6', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.900000000000000000', 'petallengthcm': '1.700000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '7', 'sepallengthcm': '4.600000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '8', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '9', 'sepallengthcm': '4.400000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '10', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '11', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.700000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '12', 'sepallengthcm': '4.800000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '13', 'sepallengthcm': '4.800000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '14', 'sepallengthcm': '4.300000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.100000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '15', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '4.000000000000000000', 'petallengthcm': '1.200000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '16', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '4.400000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '17', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.900000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '18', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '19', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '1.700000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '20', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '21', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.700000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '22', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.700000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '23', 'sepallengthcm': '4.600000000000000000', 'sepalwidthcm': '3.600000000000000000', 'petallengthcm': '1.000000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '24', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '1.700000000000000000', 'petalwidthcm': '0.500000000000000000', 'species': 'Iris-setosa'}, {'id': '25', 'sepallengthcm': '4.800000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.900000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '26', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '27', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '28', 'sepallengthcm': '5.200000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '29', 'sepallengthcm': '5.200000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '30', 'sepallengthcm': '4.700000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '31', 'sepallengthcm': '4.800000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '32', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '33', 'sepallengthcm': '5.200000000000000000', 'sepalwidthcm': '4.100000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '34', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '4.200000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '35', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '36', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '1.200000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '37', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '38', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '39', 'sepallengthcm': '4.400000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '40', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '41', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '42', 'sepallengthcm': '4.500000000000000000', 'sepalwidthcm': '2.300000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '43', 'sepallengthcm': '4.400000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '44', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.600000000000000000', 'species': 'Iris-setosa'}, {'id': '45', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '1.900000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '46', 'sepallengthcm': '4.800000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '47', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '48', 'sepallengthcm': '4.600000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '49', 'sepallengthcm': '5.300000000000000000', 'sepalwidthcm': '3.700000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '50', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '51', 'sepallengthcm': '7.000000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '4.700000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '52', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '53', 'sepallengthcm': '6.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '4.900000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '54', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '2.300000000000000000', 'petallengthcm': '4.000000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '55', 'sepallengthcm': '6.500000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.600000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '56', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '57', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '4.700000000000000000', 'petalwidthcm': '1.600000000000000000', 'species': 'Iris-versicolor'}, {'id': '58', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '2.400000000000000000', 'petallengthcm': '3.300000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '59', 'sepallengthcm': '6.600000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.600000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '60', 'sepallengthcm': '5.200000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '3.900000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '61', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '2.000000000000000000', 'petallengthcm': '3.500000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '62', 'sepallengthcm': '5.900000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.200000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '63', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '2.200000000000000000', 'petallengthcm': '4.000000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '64', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.700000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '65', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '3.600000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '66', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '4.400000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '67', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '68', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '4.100000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '69', 'sepallengthcm': '6.200000000000000000', 'sepalwidthcm': '2.200000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '70', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '3.900000000000000000', 'petalwidthcm': '1.100000000000000000', 'species': 'Iris-versicolor'}, {'id': '71', 'sepallengthcm': '5.900000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '4.800000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-versicolor'}, {'id': '72', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.000000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '73', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '4.900000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '74', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.700000000000000000', 'petalwidthcm': '1.200000000000000000', 'species': 'Iris-versicolor'}, {'id': '75', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.300000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '76', 'sepallengthcm': '6.600000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.400000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '77', 'sepallengthcm': '6.800000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.800000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '78', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.000000000000000000', 'petalwidthcm': '1.700000000000000000', 'species': 'Iris-versicolor'}, {'id': '79', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '80', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '2.600000000000000000', 'petallengthcm': '3.500000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '81', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '2.400000000000000000', 'petallengthcm': '3.800000000000000000', 'petalwidthcm': '1.100000000000000000', 'species': 'Iris-versicolor'}, {'id': '82', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '2.400000000000000000', 'petallengthcm': '3.700000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '83', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '3.900000000000000000', 'petalwidthcm': '1.200000000000000000', 'species': 'Iris-versicolor'}, {'id': '84', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '1.600000000000000000', 'species': 'Iris-versicolor'}, {'id': '85', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '86', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.600000000000000000', 'species': 'Iris-versicolor'}, {'id': '87', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '4.700000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '88', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.300000000000000000', 'petallengthcm': '4.400000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '89', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.100000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '90', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '4.000000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '91', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '2.600000000000000000', 'petallengthcm': '4.400000000000000000', 'petalwidthcm': '1.200000000000000000', 'species': 'Iris-versicolor'}, {'id': '92', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.600000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '93', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.600000000000000000', 'petallengthcm': '4.000000000000000000', 'petalwidthcm': '1.200000000000000000', 'species': 'Iris-versicolor'}, {'id': '94', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '2.300000000000000000', 'petallengthcm': '3.300000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '95', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '4.200000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '96', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.200000000000000000', 'petalwidthcm': '1.200000000000000000', 'species': 'Iris-versicolor'}, {'id': '97', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.200000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '98', 'sepallengthcm': '6.200000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.300000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '99', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '3.000000000000000000', 'petalwidthcm': '1.100000000000000000', 'species': 'Iris-versicolor'}, {'id': '100', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.100000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '101', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '6.000000000000000000', 'petalwidthcm': '2.500000000000000000', 'species': 'Iris-virginica'}, {'id': '102', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '1.900000000000000000', 'species': 'Iris-virginica'}, {'id': '103', 'sepallengthcm': '7.100000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.900000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '104', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '105', 'sepallengthcm': '6.500000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.800000000000000000', 'petalwidthcm': '2.200000000000000000', 'species': 'Iris-virginica'}, {'id': '106', 'sepallengthcm': '7.600000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '6.600000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '107', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.700000000000000000', 'species': 'Iris-virginica'}, {'id': '108', 'sepallengthcm': '7.300000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '6.300000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '109', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '5.800000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '110', 'sepallengthcm': '7.200000000000000000', 'sepalwidthcm': '3.600000000000000000', 'petallengthcm': '6.100000000000000000', 'petalwidthcm': '2.500000000000000000', 'species': 'Iris-virginica'}, {'id': '111', 'sepallengthcm': '6.500000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '112', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '5.300000000000000000', 'petalwidthcm': '1.900000000000000000', 'species': 'Iris-virginica'}, {'id': '113', 'sepallengthcm': '6.800000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.500000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '114', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '5.000000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '115', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '2.400000000000000000', 'species': 'Iris-virginica'}, {'id': '116', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '5.300000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '117', 'sepallengthcm': '6.500000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.500000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '118', 'sepallengthcm': '7.700000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '6.700000000000000000', 'petalwidthcm': '2.200000000000000000', 'species': 'Iris-virginica'}, {'id': '119', 'sepallengthcm': '7.700000000000000000', 'sepalwidthcm': '2.600000000000000000', 'petallengthcm': '6.900000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '120', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '2.200000000000000000', 'petallengthcm': '5.000000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-virginica'}, {'id': '121', 'sepallengthcm': '6.900000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '5.700000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '122', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.900000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '123', 'sepallengthcm': '7.700000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '6.700000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '124', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '4.900000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '125', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '5.700000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '126', 'sepallengthcm': '7.200000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '6.000000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '127', 'sepallengthcm': '6.200000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.800000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '128', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.900000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '129', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '130', 'sepallengthcm': '7.200000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.800000000000000000', 'petalwidthcm': '1.600000000000000000', 'species': 'Iris-virginica'}, {'id': '131', 'sepallengthcm': '7.400000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '6.100000000000000000', 'petalwidthcm': '1.900000000000000000', 'species': 'Iris-virginica'}, {'id': '132', 'sepallengthcm': '7.900000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '6.400000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '133', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '2.200000000000000000', 'species': 'Iris-virginica'}, {'id': '134', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-virginica'}, {'id': '135', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '2.600000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-virginica'}, {'id': '136', 'sepallengthcm': '7.700000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '6.100000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '137', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '2.400000000000000000', 'species': 'Iris-virginica'}, {'id': '138', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '5.500000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '139', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.800000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '140', 'sepallengthcm': '6.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '5.400000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '141', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '2.400000000000000000', 'species': 'Iris-virginica'}, {'id': '142', 'sepallengthcm': '6.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '143', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '1.900000000000000000', 'species': 'Iris-virginica'}, {'id': '144', 'sepallengthcm': '6.800000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '5.900000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '145', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '5.700000000000000000', 'petalwidthcm': '2.500000000000000000', 'species': 'Iris-virginica'}, {'id': '146', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.200000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '147', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '5.000000000000000000', 'petalwidthcm': '1.900000000000000000', 'species': 'Iris-virginica'}, {'id': '148', 'sepallengthcm': '6.500000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.200000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '149', 'sepallengthcm': '6.200000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '5.400000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '150', 'sepallengthcm': '5.900000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}]\\\\n\\\",\\n+      \\\"<built-in method count of list object at 0x000001B2518C1700>\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"# API endpoint URL\\\\n\\\",\\n+    \\\"API_URL = \\\\\\\"http://localhost/api/database/c3a42d17-42b7-43c9-a504-2363fb4c9c8d/table/5315e7da-64fb-4fdb-b493-95b4138c765f/data?size=100000&page=0\\\\\\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Define the headers\\\\n\\\",\\n+    \\\"headers = {\\\\n\\\",\\n+    \\\"    \\\\\\\"Accept\\\\\\\": \\\\\\\"application/json\\\\\\\"  # Specify the expected response format\\\\n\\\",\\n+    \\\"}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"try:\\\\n\\\",\\n+    \\\"    # Send a GET request to the API with the Accept header\\\\n\\\",\\n+    \\\"    response = requests.get(API_URL, headers=headers)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # Check if the request was successful\\\\n\\\",\\n+    \\\"    if response.status_code == 200:\\\\n\\\",\\n+    \\\"        # Parse the JSON response\\\\n\\\",\\n+    \\\"        dataset = response.json()\\\\n\\\",\\n+    \\\"        print(\\\\\\\"API Response:\\\\\\\", dataset)\\\\n\\\",\\n+    \\\"        print( dataset.count)\\\\n\\\",\\n+    \\\"    else:\\\\n\\\",\\n+    \\\"        print(f\\\\\\\"Error: Received status code {response.status_code}\\\\\\\")\\\\n\\\",\\n+    \\\"        print(\\\\\\\"Response content:\\\\\\\", response.text)\\\\n\\\",\\n+    \\\"       \\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"except requests.exceptions.RequestException as e:\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"Request failed: {e}\\\\\\\")\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"09557f94-325c-4bd6-882a-069a9e3c5ecd\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"replacing dynamic fetching of data When and if DBREPO isnt running \\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 11,\\n+   \\\"id\\\": \\\"ce6e020d-cb80-49ec-8bcc-687b1e08885c\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"# 1. Read the JSON file id the API isnt available this data is saved locally but the data is from the API endpoint\\\\n\\\",\\n+    \\\"with open(\\\\\\\"iris_data.json\\\\\\\", \\\\\\\"r\\\\\\\") as f:\\\\n\\\",\\n+    \\\"    dataset = json.load(f)\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"a6c6007a-2126-4b1a-90ee-3326eb39a362\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"Metadata fetching from db repo API CALLS\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"9165f478-a44e-4125-8929-a8d77fdcb4c5\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"METADATA ON DATABASE LEVEL\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 33,\\n+   \\\"id\\\": \\\"abe912e7-bf9b-4bbd-8e43-6046745ade3f\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"DB_API = \\\\\\\"http://localhost/api/database/{db_id}\\\\\\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def fetch_db_metadata(db_id: str) -> dict:\\\\n\\\",\\n+    \\\"    url = DB_API.format(db_id=db_id)\\\\n\\\",\\n+    \\\"    try:\\\\n\\\",\\n+    \\\"        resp = requests.get(url)\\\\n\\\",\\n+    \\\"        resp.raise_for_status()\\\\n\\\",\\n+    \\\"        return resp.json()\\\\n\\\",\\n+    \\\"    except requests.exceptions.RequestException as e:\\\\n\\\",\\n+    \\\"        print(f\\\\\\\"[\\u26a0\\ufe0f Error] Failed to fetch DB metadata for {db_id}: {e}\\\\\\\")\\\\n\\\",\\n+    \\\"        return {}  # or return None, depending on what your app prefers\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def log_db_metadata(db_meta: dict):\\\\n\\\",\\n+    \\\"    # 1) Core DB fields as params, defaulting to empty string if key is missing\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"database.id\\\\\\\",          db_meta.get(\\\\\\\"id\\\\\\\", \\\\\\\"\\\\\\\"))\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"database.name\\\\\\\",        db_meta.get(\\\\\\\"name\\\\\\\", \\\\\\\"\\\\\\\"))\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"database.description\\\\\\\", db_meta.get(\\\\\\\"description\\\\\\\", \\\\\\\"\\\\\\\"))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 2) Handle nested keys safely for owner\\\\n\\\",\\n+    \\\"    try:\\\\n\\\",\\n+    \\\"        owner = db_meta.get(\\\\\\\"tables\\\\\\\", [{}])[0].get(\\\\\\\"owner\\\\\\\", {}).get(\\\\\\\"username\\\\\\\", \\\\\\\"\\\\\\\")\\\\n\\\",\\n+    \\\"    except Exception:\\\\n\\\",\\n+    \\\"        owner = \\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"database.owner\\\\\\\", owner)\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"53cbd7eb-0d97-4326-9bfc-f6fcee14ef9c\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"MATADATA FROM: <ns0:OAI-PMH xmlns:ns0=\\\\\\\"http://www.openarchives.org/OAI/2.0/\\\\\\\" xmlns:xsi=\\\\\\\"http://www.w3.org/2001/XMLSchema-instance\\\\\\\" xsi:schemaLocation=\\\\\\\"http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd\\\\\\\">\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 34,\\n+   \\\"id\\\": \\\"296f307e-e01b-477a-9406-92cab9f2d7bf\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"<ns0:OAI-PMH xmlns:ns0=\\\\\\\"http://www.openarchives.org/OAI/2.0/\\\\\\\" xmlns:xsi=\\\\\\\"http://www.w3.org/2001/XMLSchema-instance\\\\\\\" xsi:schemaLocation=\\\\\\\"http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd\\\\\\\">\\\\n\\\",\\n+      \\\"    <ns0:responseDate>2025-04-25T09:55:50Z</ns0:responseDate>\\\\n\\\",\\n+      \\\"    <ns0:request verb=\\\\\\\"Identify\\\\\\\">https://localhost/api/oai</ns0:request>\\\\n\\\",\\n+      \\\"    <ns0:Identify>\\\\n\\\",\\n+      \\\"    <ns0:repositoryName>Database Repository</ns0:repositoryName>\\\\n\\\",\\n+      \\\"    <ns0:baseURL>http://localhost</ns0:baseURL>\\\\n\\\",\\n+      \\\"    <ns0:protocolVersion>2.0</ns0:protocolVersion>\\\\n\\\",\\n+      \\\"    <ns0:adminEmail>noreply@localhost</ns0:adminEmail>\\\\n\\\",\\n+      \\\"    <ns0:earliestDatestamp />\\\\n\\\",\\n+      \\\"    <ns0:deletedRecord>persistent</ns0:deletedRecord>\\\\n\\\",\\n+      \\\"    <ns0:granularity>YYYY-MM-DDThh:mm:ssZ</ns0:granularity>\\\\n\\\",\\n+      \\\"</ns0:Identify>\\\\n\\\",\\n+      \\\"</ns0:OAI-PMH>\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"# 1) Fetch your database metadata\\\\n\\\",\\n+    \\\"db_url = \\\\\\\"http://localhost/api/database/c3a42d17-42b7-43c9-a504-2363fb4c9c8d\\\\\\\"\\\\n\\\",\\n+    \\\"db_resp = requests.get(db_url)\\\\n\\\",\\n+    \\\"db_resp.raise_for_status()\\\\n\\\",\\n+    \\\"db_data = db_resp.json()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"db_id  = db_data[\\\\\\\"id\\\\\\\"]\\\\n\\\",\\n+    \\\"tbl_id = db_data[\\\\\\\"tables\\\\\\\"][0][\\\\\\\"id\\\\\\\"]\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# 2) Build the OAI-PMH URL, URL-encoding the `set` param\\\\n\\\",\\n+    \\\"set_param   = f\\\\\\\"Databases/{db_id}/Tables/{tbl_id}\\\\\\\"\\\\n\\\",\\n+    \\\"encoded_set = urllib.parse.quote(set_param, safe=\\\\\\\"\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"oai_url = (\\\\n\\\",\\n+    \\\"    \\\\\\\"http://localhost/api/oai\\\\\\\"\\\\n\\\",\\n+    \\\"    f\\\\\\\"?metadataPrefix=oai_dc\\\\\\\"\\\\n\\\",\\n+    \\\"    f\\\\\\\"&from=2025-03-01\\\\\\\"\\\\n\\\",\\n+    \\\"    f\\\\\\\"&until=2025-03-07\\\\\\\"\\\\n\\\",\\n+    \\\"    f\\\\\\\"&set={encoded_set}\\\\\\\"\\\\n\\\",\\n+    \\\"    f\\\\\\\"&resumptionToken=string\\\\\\\"\\\\n\\\",\\n+    \\\"    f\\\\\\\"&fromDate=2025-03-07T19%3A35%3A51.476Z\\\\\\\"\\\\n\\\",\\n+    \\\"    f\\\\\\\"&untilDate=2025-03-07T19%3A35%3A51.476Z\\\\\\\"\\\\n\\\",\\n+    \\\"    f\\\\\\\"&parametersString=string\\\\\\\"\\\\n\\\",\\n+    \\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# 3) Call and parse\\\\n\\\",\\n+    \\\"try:\\\\n\\\",\\n+    \\\"    resp = requests.get(oai_url)\\\\n\\\",\\n+    \\\"    resp.raise_for_status()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    if \\\\\\\"xml\\\\\\\" in resp.headers.get(\\\\\\\"Content-Type\\\\\\\", \\\\\\\"\\\\\\\"):\\\\n\\\",\\n+    \\\"        root = ET.fromstring(resp.text)\\\\n\\\",\\n+    \\\"        print(ET.tostring(root, encoding=\\\\\\\"utf-8\\\\\\\").decode())\\\\n\\\",\\n+    \\\"    else:\\\\n\\\",\\n+    \\\"        print(\\\\\\\"Non-XML response:\\\\\\\", resp.headers.get(\\\\\\\"Content-Type\\\\\\\"), resp.text)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"except requests.exceptions.RequestException as e:\\\\n\\\",\\n+    \\\"    print(\\\\\\\"Request failed:\\\\\\\", e)\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 35,\\n+   \\\"id\\\": \\\"61cc99ab-4a5c-4142-8725-e7c940673ffd\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"# \\u2026after you fetch & parse your XML into `root`\\u2026\\\\n\\\",\\n+    \\\"ns = {\\\\\\\"oai\\\\\\\": \\\\\\\"http://www.openarchives.org/OAI/2.0/\\\\\\\"}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"repo_name   = root.findtext(\\\\\\\"oai:Identify/oai:repositoryName\\\\\\\", namespaces=ns) or \\\\\\\"N/A\\\\\\\"\\\\n\\\",\\n+    \\\"base_url    = root.findtext(\\\\\\\"oai:Identify/oai:baseURL\\\\\\\", namespaces=ns) or \\\\\\\"N/A\\\\\\\"\\\\n\\\",\\n+    \\\"protocol    = root.findtext(\\\\\\\"oai:Identify/oai:protocolVersion\\\\\\\", namespaces=ns) or \\\\\\\"N/A\\\\\\\"\\\\n\\\",\\n+    \\\"admin_email = root.findtext(\\\\\\\"oai:Identify/oai:adminEmail\\\\\\\", namespaces=ns) or \\\\\\\"N/A\\\\\\\"\\\\n\\\",\\n+    \\\"gran        = root.findtext(\\\\\\\"oai:Identify/oai:granularity\\\\\\\", namespaces=ns) or \\\\\\\"N/A\\\\\\\"\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"74214aa7-c12f-414e-9feb-094a366b855b\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"METADATA ON History Logging\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 36,\\n+   \\\"id\\\": \\\"e9c74e9b-c9b0-4b4a-82eb-2a6e56456508\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"API Response: [{'timestamp': '2025-04-23T20:42:29.501Z', 'event': 'insert', 'total': 150}]\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"url = \\\\\\\"http://localhost/api/database/c3a42d17-42b7-43c9-a504-2363fb4c9c8d/table/5315e7da-64fb-4fdb-b493-95b4138c765f/history\\\\\\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"try:\\\\n\\\",\\n+    \\\"    # Send a GET request to the API\\\\n\\\",\\n+    \\\"    response = requests.get(url)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # Check if the request was successful\\\\n\\\",\\n+    \\\"    if response.status_code == 200:\\\\n\\\",\\n+    \\\"        # Parse the JSON response\\\\n\\\",\\n+    \\\"        data = response.json()\\\\n\\\",\\n+    \\\"        print(\\\\\\\"API Response:\\\\\\\", data)\\\\n\\\",\\n+    \\\"    else:\\\\n\\\",\\n+    \\\"        print(f\\\\\\\"Error: Received status code {response.status_code}\\\\\\\")\\\\n\\\",\\n+    \\\"        print(\\\\\\\"Response content:\\\\\\\", response.text)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"except requests.exceptions.RequestException as e:\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"Request failed: {e}\\\\\\\")\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 37,\\n+   \\\"id\\\": \\\"3630c954-5ad2-4759-b9a0-fa6e20e184ef\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"first   = data[0]\\\\n\\\",\\n+    \\\"last    = data[-1]\\\\n\\\",\\n+    \\\"count_0 = first[\\\\\\\"total\\\\\\\"]    # e.g. 149\\\\n\\\",\\n+    \\\"count_N = last[\\\\\\\"total\\\\\\\"]     # e.g. 149 again, or changed\\\\n\\\",\\n+    \\\"ts_last = last[\\\\\\\"timestamp\\\\\\\"]  # e.g. \\\\\\\"2025-03-28T17:42:38.058Z\\\\\\\"\\\\n\\\",\\n+    \\\"n_insert = sum(1 for ev in data if ev[\\\\\\\"event\\\\\\\"]==\\\\\\\"insert\\\\\\\")\\\\n\\\",\\n+    \\\"n_delete = sum(1 for ev in data if ev[\\\\\\\"event\\\\\\\"]==\\\\\\\"delete\\\\\\\")\\\\n\\\",\\n+    \\\"history = response.json()\\\\n\\\",\\n+    \\\"first, last = history[0], history[-1]\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# summary stats\\\\n\\\",\\n+    \\\"count_start = first[\\\\\\\"total\\\\\\\"]\\\\n\\\",\\n+    \\\"count_end   = last[\\\\\\\"total\\\\\\\"]\\\\n\\\",\\n+    \\\"ts_last     = last[\\\\\\\"timestamp\\\\\\\"]\\\\n\\\",\\n+    \\\"n_insert    = sum(1 for ev in history if ev[\\\\\\\"event\\\\\\\"]==\\\\\\\"insert\\\\\\\")\\\\n\\\",\\n+    \\\"n_delete    = sum(1 for ev in history if ev[\\\\\\\"event\\\\\\\"]==\\\\\\\"delete\\\\\\\")\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"1afd5dad-72d5-42e1-a0fa-b7bd3455937b\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"Dataset metadata fetching from ZONEDO or any public dataset repositories to gain more details\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 38,\\n+   \\\"id\\\": \\\"a7fa122a-c6e5-4b38-842a-dc81590a1f46\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"def fetch_and_log_dataset_metadata_nested(doi_url: str):\\\\n\\\",\\n+    \\\"    # 1) fetch the CSL+JSON\\\\n\\\",\\n+    \\\"    headers = {\\\\\\\"Accept\\\\\\\": \\\\\\\"application/vnd.citationstyles.csl+json\\\\\\\"}\\\\n\\\",\\n+    \\\"    r = requests.get(doi_url, headers=headers); r.raise_for_status()\\\\n\\\",\\n+    \\\"    meta = r.json()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 2) pull out what you care about\\\\n\\\",\\n+    \\\"    authors = [f\\\\\\\"{a.get('family','')} {a.get('given','')}\\\\\\\".strip()\\\\n\\\",\\n+    \\\"               for a in meta.get(\\\\\\\"author\\\\\\\", [])]\\\\n\\\",\\n+    \\\"    pubdate = \\\\\\\"-\\\\\\\".join(str(x) for x in meta.get(\\\\\\\"issued\\\\\\\",{}).get(\\\\\\\"date-parts\\\\\\\",[[]])[0])\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 3) assemble one nested dict\\\\n\\\",\\n+    \\\"    public_datasetRepository_metadata = {\\\\n\\\",\\n+    \\\"      \\\\\\\"zenodo\\\\\\\": {\\\\n\\\",\\n+    \\\"        \\\\\\\"title\\\\\\\":     meta.get(\\\\\\\"title\\\\\\\"),\\\\n\\\",\\n+    \\\"        \\\\\\\"doi\\\\\\\":       meta.get(\\\\\\\"DOI\\\\\\\"),\\\\n\\\",\\n+    \\\"        \\\\\\\"authors\\\\\\\":   authors,\\\\n\\\",\\n+    \\\"        \\\\\\\"published\\\\\\\": pubdate,\\\\n\\\",\\n+    \\\"        \\\\\\\"publisher\\\\\\\": meta.get(\\\\\\\"publisher\\\\\\\"),\\\\n\\\",\\n+    \\\"      },\\\\n\\\",\\n+    \\\"   \\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"      # 4) log it as a single JSON artifact\\\\n\\\",\\n+    \\\"    mlflow.log_dict(public_datasetRepository_metadata,\\\\n\\\",\\n+    \\\"                \\\\\\\"public_datasetRepository_metadata.json\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 2) Flatten and log the important bits as params:\\\\n\\\",\\n+    \\\"    z = public_datasetRepository_metadata[\\\\\\\"zenodo\\\\\\\"]\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"dataset.title\\\\\\\",     z[\\\\\\\"title\\\\\\\"])\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"dataset.doi\\\\\\\",       z[\\\\\\\"doi\\\\\\\"])\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"dataset.authors\\\\\\\",   json.dumps(z[\\\\\\\"authors\\\\\\\"]))\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"dataset.published\\\\\\\", z[\\\\\\\"published\\\\\\\"])\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"dataset.publisher\\\\\\\", z[\\\\\\\"publisher\\\\\\\"])\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"   \\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"58c92e13-eb57-418d-b354-83777f88aa98\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"##################################################################\\\\n\\\",\\n+    \\\"# DATA PREPROCESSING STEPS\\\\n\\\",\\n+    \\\"###################################################################\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"9832d0df-af0a-4eee-90d0-fab926e03e85\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"STEP 1: LOAD DATASET\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 39,\\n+   \\\"id\\\": \\\"77402d80-22d1-4bed-9489-768958c3e9fa\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"# \\u2500\\u2500 2) Load into a DataFrame \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"df = pd.DataFrame(dataset)\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"6862e341-3ea1-43f6-a1ac-9a51188fe614\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"STEP2: seperate Dependent and Independent variables and drop unnecessary columns like ID\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 40,\\n+   \\\"id\\\": \\\"01309a7b-53d2-4df4-b334-0f0db8b03333\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"Shapes: (150, 4) (150,)\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"target_col = df.columns[-1]      # e.g. \\\\\\\"species\\\\\\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# 2) extract y as the Series of labels\\\\n\\\",\\n+    \\\"y = df[target_col]               # length == n_samples\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# 3) build X by dropping just that one column\\\\n\\\",\\n+    \\\"X = df.drop(columns=[target_col])\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# 4) drop any ID column (case-insensitive)\\\\n\\\",\\n+    \\\"id_cols = [c for c in X.columns if c.lower() == \\\\\\\"id\\\\\\\"]\\\\n\\\",\\n+    \\\"if id_cols:\\\\n\\\",\\n+    \\\"    X = X.drop(columns=id_cols)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# 5) coerce numeric where possible\\\\n\\\",\\n+    \\\"for c in X.columns:\\\\n\\\",\\n+    \\\"    try:\\\\n\\\",\\n+    \\\"        X[c] = pd.to_numeric(X[c])\\\\n\\\",\\n+    \\\"    except Exception:\\\\n\\\",\\n+    \\\"        pass\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"print(\\\\\\\"Shapes:\\\\\\\", X.shape, y.shape)\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"367d6256-a30a-4f91-bc64-f20966d828ab\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"STEP3: Label Encoding as the target values are class names\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 41,\\n+   \\\"id\\\": \\\"11f5126d-6a03-48c6-9ecf-39ed0d43688c\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"Classes: ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"text/plain\\\": [\\n+       \\\"array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\\\n\\\",\\n+       \\\"       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\\\n\\\",\\n+       \\\"       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\\\n\\\",\\n+       \\\"       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\\\n\\\",\\n+       \\\"       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\\\n\\\",\\n+       \\\"       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\\\n\\\",\\n+       \\\"       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\\\"\\n+      ]\\n+     },\\n+     \\\"execution_count\\\": 41,\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"execute_result\\\"\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"le = LabelEncoder()\\\\n\\\",\\n+    \\\"y = le.fit_transform(y)  \\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# now y_enc is a 1d numpy array of ints 0,1,2\\\\n\\\",\\n+    \\\"print(\\\\\\\"Classes:\\\\\\\", le.classes_)  \\\\n\\\",\\n+    \\\"y\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 42,\\n+   \\\"id\\\": \\\"68d0a924-c65f-4a44-a5cc-bbb32d17e96f\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"# \\u2500\\u2500 4) Cast feature columns to numeric where possible \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"for col in X.columns:\\\\n\\\",\\n+    \\\"    try:\\\\n\\\",\\n+    \\\"        X[col] = pd.to_numeric(X[col])   # no errors=\\\\\\\"ignore\\\\\\\"\\\\n\\\",\\n+    \\\"    except ValueError:\\\\n\\\",\\n+    \\\"        # if it can\\u2019t be cast, just leave it as-is\\\\n\\\",\\n+    \\\"        pass\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 43,\\n+   \\\"id\\\": \\\"e17f39ce-3322-4626-83a6-079d304bbc04\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"# \\u2500\\u2500 5) Drop any \\u201cid\\u201d column (case-insensitive) \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"dropped = [c for c in X.columns if c.lower() == \\\\\\\"id\\\\\\\"]\\\\n\\\",\\n+    \\\"X = X.drop(columns=dropped, errors=\\\\\\\"ignore\\\\\\\")\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"6fcf2244-14dd-4e3d-b8cf-f7f3ba34f80f\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83d\\udcc2 Setup MLflow\\\\n\\\",\\n+    \\\"# ============================\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 44,\\n+   \\\"id\\\": \\\"cbe91ec0-6447-4586-b7cc-2c1f74d4218f\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"text/plain\\\": [\\n+       \\\"<Experiment: artifact_location='file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608', creation_time=1745329164532, experiment_id='615223710259862608', last_update_time=1745329164532, lifecycle_stage='active', name='RandomForest-Iris-CSV', tags={}>\\\"\\n+      ]\\n+     },\\n+     \\\"execution_count\\\": 44,\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"execute_result\\\"\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"project_dir = os.getcwd()\\\\n\\\",\\n+    \\\"mlflow.set_tracking_uri(\\\\\\\"mlrunlogs/mlflow.db\\\\\\\")\\\\n\\\",\\n+    \\\"mlflow.set_experiment(\\\\\\\"RandomForest-Iris-CSV\\\\\\\")\\\\n\\\",\\n+    \\\"# mlflow.sklearn.autolog()\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"af2c2c5f-cc36-41a3-9643-83ef95b9f55e\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83d\\udd04 Git Commit Hash for previous commit for metadata\\\\n\\\",\\n+    \\\"# ============================\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 45,\\n+   \\\"id\\\": \\\"838dd233-25dc-4725-974d-4da89c257782\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"repo_dir = \\\\\\\"C:/Users/reema/REPO\\\\\\\"\\\\n\\\",\\n+    \\\"previous_commit_repo = git.Repo(repo_dir)\\\\n\\\",\\n+    \\\"previous_commit_hash = previous_commit_repo.head.object.hexsha\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"430d15ef-3432-4e45-88fb-b7048a5b10a9\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# Make threadpoolctl safe so MLflow\\u2019s autologger won\\u2019t crash \\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"# ============================\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 46,\\n+   \\\"id\\\": \\\"9668451f-4352-4bdc-8b6b-bbe49074212a\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 11:57:54 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\\\\n\\\",\\n+      \\\"2025/04/25 11:57:54 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"try:\\\\n\\\",\\n+    \\\"    import threadpoolctl\\\\n\\\",\\n+    \\\"    _orig = threadpoolctl.threadpool_info\\\\n\\\",\\n+    \\\"    def _safe_threadpool_info(*args, **kwargs):\\\\n\\\",\\n+    \\\"        try:\\\\n\\\",\\n+    \\\"            return _orig(*args, **kwargs)\\\\n\\\",\\n+    \\\"        except Exception:\\\\n\\\",\\n+    \\\"            return []\\\\n\\\",\\n+    \\\"    threadpoolctl.threadpool_info = _safe_threadpool_info\\\\n\\\",\\n+    \\\"except ImportError:\\\\n\\\",\\n+    \\\"    pass  # if threadpoolctl isn\\u2019t installed, autolog will skip unsupported versions\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# \\u2500\\u2500\\u2500 1) Enable generic autolog (will auto-patch sklearn under the hood) \\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"import mlflow\\\\n\\\",\\n+    \\\"mlflow.autolog(\\\\n\\\",\\n+    \\\"    log_input_examples=True,\\\\n\\\",\\n+    \\\"    log_model_signatures=True\\\\n\\\",\\n+    \\\")\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"cba22f52-178f-48e6-a9e0-7ef23a886f01\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"#################################################\\\\n\\\",\\n+    \\\"# Justification LOGGER\\\\n\\\",\\n+    \\\"################################################\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 80,\\n+   \\\"id\\\": \\\"afb626b4-8532-4011-bdc8-7424a6289bb7\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": []\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"9058319a-adba-4a6b-93e9-d17080c0594d\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"# ============================\\\\n\\\",\\n+    \\\"# \\ud83d\\ude80 Start MLflow Run \\\\n\\\",\\n+    \\\"# ============================\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 83,\\n+   \\\"id\\\": \\\"14c62f08-a116-4060-9689-f69968e9f240\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `n_estimators` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `criterion` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `max_depth` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `min_samples_split` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `min_samples_leaf` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `max_features` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `bootstrap` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `oob_score` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `class_weight` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `random_state` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `verbose` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `n_jobs` (Hyperparameter configuration)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this value?  GRID SEARCH suggesstion\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `model_choice`\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose RandomForestClassifier for this task?  easy model\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `target_variable`\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why did you choose this column as the prediction target?  dataset info\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `test_split`\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why this train/test ratio (e.g., 80/20)?  makes sense\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `metric_choice`\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why accuracy/f1/ROC-AUC as your evaluation metric?  s\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `threshold_accuracy`\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why 0.95 as performance threshold?  fluid atm\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `dataset_version`\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why use this specific dataset version?  its available\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `drop_column_X`\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Why drop any specific columns from the dataset?  id\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"\\ud83d\\udcdd Justification for `experiment_name`\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2192 Any context behind this experiment name or setup?  makes sense\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.2s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    0.2s finished\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"a64e4f0522d8493995df18aaaba889fc\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\\\\n\\\",\\n+      \\\"[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\\\\n\\\",\\n+      \\\"C:\\\\\\\\Users\\\\\\\\reema\\\\\\\\anaconda3\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\shap\\\\\\\\plots\\\\\\\\_beeswarm.py:1153: UserWarning: The figure layout has changed to tight\\\\n\\\",\\n+      \\\"  pl.tight_layout()\\\\n\\\",\\n+      \\\"C:\\\\\\\\Users\\\\\\\\reema\\\\\\\\anaconda3\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\shap\\\\\\\\plots\\\\\\\\_beeswarm.py:761: UserWarning: The figure layout has changed to tight\\\\n\\\",\\n+      \\\"  pl.tight_layout(pad=0, w_pad=0, h_pad=0.0)\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\u2705 Commit successful.\\\\n\\\",\\n+      \\\"\\ud83d\\ude80 Push successful.\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"with mlflow.start_run() as run:\\\\n\\\",\\n+    \\\"    #################################################\\\\n\\\",\\n+    \\\"# Justification LOGGER\\\\n\\\",\\n+    \\\"################################################\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    def log_with_justification(log_func, key, value, context=\\\\\\\"\\\\\\\"):\\\\n\\\",\\n+    \\\"        \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"        Log a parameter/metric/tag using `log_func` and ask for justification via console.\\\\n\\\",\\n+    \\\"        \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"        log_func(key, value)\\\\n\\\",\\n+    \\\"        print(f\\\\\\\"\\\\\\\\n\\ud83d\\udcdd Justification for `{key}` ({context})\\\\\\\")\\\\n\\\",\\n+    \\\"        user_reason = input(\\\\\\\"\\u2192 Why did you choose this value? \\\\\\\")\\\\n\\\",\\n+    \\\"        mlflow.set_tag(f\\\\\\\"justification_{key}\\\\\\\", user_reason or \\\\\\\"No justification provided\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    def log_justification(key: str, question: str):\\\\n\\\",\\n+    \\\"        print(f\\\\\\\"\\\\\\\\n\\ud83d\\udcdd Justification for `{key}`\\\\\\\")\\\\n\\\",\\n+    \\\"        user_reason = input(f\\\\\\\"\\u2192 {question} \\\\\\\")\\\\n\\\",\\n+    \\\"        mlflow.set_tag(f\\\\\\\"justification_{key}\\\\\\\", user_reason or \\\\\\\"No justification provided\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    meta = fetch_and_log_dataset_metadata_nested(\\\\n\\\",\\n+    \\\"            \\\\\\\"https://doi.org/10.5281/zenodo.1404173\\\\\\\",\\\\n\\\",\\n+    \\\"           \\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    #Datasbase info logging\\\\n\\\",\\n+    \\\"    db_id = \\\\\\\"c3a42d17-42b7-43c9-a504-2363fb4c9c8d\\\\\\\"\\\\n\\\",\\n+    \\\"    db_meta = fetch_db_metadata(db_id)\\\\n\\\",\\n+    \\\"    log_db_metadata(db_meta)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    #OAI metadata logging from api endpoint\\\\n\\\",\\n+    \\\"    # log as tags\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dbrepo.repository_name\\\\\\\", repo_name)\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dbrepo.base_url\\\\\\\",       base_url)\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dbrepo.protocol_version\\\\\\\", protocol)\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dbrepo.admin_email\\\\\\\",     admin_email)\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dbrepo.granularity\\\\\\\",     gran)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    #From history API logging\\\\n\\\",\\n+    \\\"    # provenance tags\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dbrepo.table_last_modified\\\\\\\", ts_last)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # row-count metrics\\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"dbrepo.row_count_start\\\\\\\", count_start)\\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"dbrepo.row_count_end\\\\\\\",   count_end)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # change-event metrics\\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"dbrepo.num_inserts\\\\\\\", n_insert)\\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"dbrepo.num_deletes\\\\\\\", n_delete)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # 2) Capture raw metadata\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"data_source\\\\\\\", API_URL)\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"retrieval_time\\\\\\\", datetime.utcnow().isoformat())\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"n_records\\\\\\\", len(df))\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"columns_raw\\\\\\\", df.columns.tolist())\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"dropped_columns\\\\\\\", id_cols)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 4) Post\\u2010processing metadata\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"n_features_final\\\\\\\", X.shape[1])\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"feature_names\\\\\\\", X.columns.tolist())\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"target_name\\\\\\\", y)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"       # Label encoding\\\\n\\\",\\n+    \\\"    label_map = {int(idx): cls for idx, cls in enumerate(le.classes_)}\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # Save to an in-memory file\\\\n\\\",\\n+    \\\"    buffer = io.StringIO()\\\\n\\\",\\n+    \\\"    json.dump(label_map, buffer, indent=2)\\\\n\\\",\\n+    \\\"    buffer.seek(0)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # Log it to MLflow\\\\n\\\",\\n+    \\\"    mlflow.log_text(buffer.getvalue(), artifact_file=\\\\\\\"label_mapping.json\\\\\\\")\\\\n\\\",\\n+    \\\"   \\\\n\\\",\\n+    \\\"    ts = datetime.now().strftime(\\\\\\\"%Y%m%d_%H%M%S\\\\\\\")\\\\n\\\",\\n+    \\\"    model_name = f\\\\\\\"RandomForest_Iris_v{ts}\\\\\\\"\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"model_name\\\\\\\",model_name)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    train_start_ts = datetime.now().isoformat()\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"training_start_time\\\\\\\", train_start_ts)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    test_size    = 0.2\\\\n\\\",\\n+    \\\"    random_state = 42\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # \\ud83d\\udcc8 Model Training\\\\n\\\",\\n+    \\\"    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # \\u2500\\u2500 2) Log dataset split params \\u2500\\u2500\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"test_size\\\\\\\", test_size)\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"random_state\\\\\\\", random_state)\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"n_train_samples\\\\\\\", X_train.shape[0])\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"n_test_samples\\\\\\\",  X_test.shape[0])\\\\n\\\",\\n+    \\\"    mlflow.log_param(\\\\\\\"n_features\\\\\\\",      X_train.shape[1])\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"     # 1) Define a more complex hyperparameter dict\\\\n\\\",\\n+    \\\"    hyperparams = {\\\\n\\\",\\n+    \\\"        \\\\\\\"n_estimators\\\\\\\":       200,\\\\n\\\",\\n+    \\\"        \\\\\\\"criterion\\\\\\\":          \\\\\\\"entropy\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"max_depth\\\\\\\":          12,\\\\n\\\",\\n+    \\\"        \\\\\\\"min_samples_split\\\\\\\":  5,\\\\n\\\",\\n+    \\\"        \\\\\\\"min_samples_leaf\\\\\\\":   2,\\\\n\\\",\\n+    \\\"        \\\\\\\"max_features\\\\\\\":       \\\\\\\"sqrt\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"bootstrap\\\\\\\":          True,\\\\n\\\",\\n+    \\\"        \\\\\\\"oob_score\\\\\\\":          False,\\\\n\\\",\\n+    \\\"        \\\\\\\"class_weight\\\\\\\":       None,\\\\n\\\",\\n+    \\\"        \\\\\\\"random_state\\\\\\\":       42,\\\\n\\\",\\n+    \\\"        \\\\\\\"verbose\\\\\\\":            1,\\\\n\\\",\\n+    \\\"        \\\\\\\"n_jobs\\\\\\\":             -1\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # 2) Log them ALL at once\\\\n\\\",\\n+    \\\"    mlflow.log_params(hyperparams)\\\\n\\\",\\n+    \\\"    model = RandomForestClassifier(**hyperparams)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    for key, val in hyperparams.items():\\\\n\\\",\\n+    \\\"        log_with_justification(mlflow.log_param, key, val, context=\\\\\\\"Hyperparameter configuration\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # Prompt for and log justifications for high-level modeling decisions\\\\n\\\",\\n+    \\\"    log_justification(\\\\\\\"model_choice\\\\\\\", \\\\\\\"Why did you choose RandomForestClassifier for this task?\\\\\\\")\\\\n\\\",\\n+    \\\"    log_justification(\\\\\\\"target_variable\\\\\\\", \\\\\\\"Why did you choose this column as the prediction target?\\\\\\\")\\\\n\\\",\\n+    \\\"    log_justification(\\\\\\\"test_split\\\\\\\", \\\\\\\"Why this train/test ratio (e.g., 80/20)?\\\\\\\")\\\\n\\\",\\n+    \\\"    log_justification(\\\\\\\"metric_choice\\\\\\\", \\\\\\\"Why accuracy/f1/ROC-AUC as your evaluation metric?\\\\\\\")\\\\n\\\",\\n+    \\\"    log_justification(\\\\\\\"threshold_accuracy\\\\\\\", \\\\\\\"Why 0.95 as performance threshold?\\\\\\\")\\\\n\\\",\\n+    \\\"    log_justification(\\\\\\\"dataset_version\\\\\\\", \\\\\\\"Why use this specific dataset version?\\\\\\\")\\\\n\\\",\\n+    \\\"    log_justification(\\\\\\\"drop_column_X\\\\\\\", \\\\\\\"Why drop any specific columns from the dataset?\\\\\\\")\\\\n\\\",\\n+    \\\"    log_justification(\\\\\\\"experiment_name\\\\\\\", \\\\\\\"Any context behind this experiment name or setup?\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    model.fit(X_train, y_train)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    train_end_ts = datetime.now().isoformat()\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"training_end_time\\\\\\\", train_end_ts)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"     # \\u2500\\u2500 6) Predict & log metrics \\u2500\\u2500\\\\n\\\",\\n+    \\\"    y_pred = model.predict(X_test)\\\\n\\\",\\n+    \\\"    y_proba = model.predict_proba(X_test)\\\\n\\\",\\n+    \\\"    acc = accuracy_score(y_test, y_pred)\\\\n\\\",\\n+    \\\"    auc = roc_auc_score(y_test, y_proba, multi_class=\\\\\\\"ovr\\\\\\\")\\\\n\\\",\\n+    \\\"    prec = precision_score(y_test, y_pred, average=\\\\\\\"macro\\\\\\\")\\\\n\\\",\\n+    \\\"    rec  = recall_score(y_test,    y_pred, average=\\\\\\\"macro\\\\\\\")\\\\n\\\",\\n+    \\\"    f1   = f1_score(y_test,      y_pred, average=\\\\\\\"macro\\\\\\\")\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"precision_macro\\\\\\\", prec)\\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"recall_macro\\\\\\\",    rec)\\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"f1_macro\\\\\\\",        f1)\\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"accuracy\\\\\\\", acc)\\\\n\\\",\\n+    \\\"    mlflow.log_metric(\\\\\\\"roc_auc\\\\\\\",   auc)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # \\u2705 Log Environment Automatically\\\\n\\\",\\n+    \\\"    mlflow.log_params({\\\\n\\\",\\n+    \\\"        \\\\\\\"python_version\\\\\\\": platform.python_version(),\\\\n\\\",\\n+    \\\"        \\\\\\\"os_platform\\\\\\\": f\\\\\\\"{platform.system()} {platform.release()}\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"sklearn_version\\\\\\\": sklearn.__version__,\\\\n\\\",\\n+    \\\"        \\\\\\\"pandas_version\\\\\\\": pd.__version__,\\\\n\\\",\\n+    \\\"        \\\\\\\"numpy_version\\\\\\\": np.__version__,\\\\n\\\",\\n+    \\\"        \\\\\\\"matplotlib_version\\\\\\\": matplotlib.__version__,\\\\n\\\",\\n+    \\\"        \\\\\\\"seaborn_version\\\\\\\": sns.__version__,\\\\n\\\",\\n+    \\\"        \\\\\\\"shap_version\\\\\\\": shap.__version__,\\\\n\\\",\\n+    \\\"    })\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # \\u2705 Git and Notebook Metadata\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"notebook_name\\\\\\\", \\\\\\\"RQ1.ipynb\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # \\u2705 Dataset Metadata Tags\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dataset_name\\\\\\\", \\\\\\\"Iris\\\\\\\") #TODO\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dataset_version\\\\\\\", \\\\\\\"1.0.0\\\\\\\") #TODO\\\\n\\\",\\n+    \\\"    mlflow.set_tag(\\\\\\\"dataset_id\\\\\\\", \\\\\\\"iris_local\\\\\\\") #TODO\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # \\u2500\\u2500\\u2500 2) Create a folder for this run\\u2019s plots \\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"    plot_dir = os.path.join(\\\\\\\"plots\\\\\\\", model_name)\\\\n\\\",\\n+    \\\"    os.makedirs(plot_dir, exist_ok=True)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 1) Feature Importance Bar Chart\\\\n\\\",\\n+    \\\"    importances = model.feature_importances_\\\\n\\\",\\n+    \\\"    try:\\\\n\\\",\\n+    \\\"        feature_names = X_train.columns\\\\n\\\",\\n+    \\\"    except AttributeError:\\\\n\\\",\\n+    \\\"        feature_names = [f\\\\\\\"f{i}\\\\\\\" for i in range(X_train.shape[1])]\\\\n\\\",\\n+    \\\"    fi_path = os.path.join(plot_dir, \\\\\\\"feature_importances.png\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    plt.figure(figsize=(8, 6))\\\\n\\\",\\n+    \\\"    sns.barplot(x=importances, y=feature_names)\\\\n\\\",\\n+    \\\"    plt.title(\\\\\\\"Feature Importances\\\\\\\")\\\\n\\\",\\n+    \\\"    plt.xlabel(\\\\\\\"Importance\\\\\\\")\\\\n\\\",\\n+    \\\"    plt.ylabel(\\\\\\\"Feature\\\\\\\")\\\\n\\\",\\n+    \\\"    plt.tight_layout()\\\\n\\\",\\n+    \\\"    plt.savefig(fi_path)\\\\n\\\",\\n+    \\\"    mlflow.log_artifact(fi_path)\\\\n\\\",\\n+    \\\"    plt.close()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# 2) Multi-class ROC Curves\\\\n\\\",\\n+    \\\"# Binarize labels for one-vs-rest\\\\n\\\",\\n+    \\\"    classes = np.unique(y_test)\\\\n\\\",\\n+    \\\"    y_test_bin = label_binarize(y_test, classes=classes)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    for idx, cls in enumerate(classes):\\\\n\\\",\\n+    \\\"        disp = RocCurveDisplay.from_predictions(\\\\n\\\",\\n+    \\\"            y_test_bin[:, idx], \\\\n\\\",\\n+    \\\"            y_proba[:, idx],\\\\n\\\",\\n+    \\\"            name=f\\\\\\\"ROC for class {cls}\\\\\\\"\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        roc_path = os.path.join(plot_dir, f\\\\\\\"roc_curve_cls_{cls}.png\\\\\\\")\\\\n\\\",\\n+    \\\"        disp.figure_.savefig(roc_path)\\\\n\\\",\\n+    \\\"        mlflow.log_artifact(roc_path)\\\\n\\\",\\n+    \\\"        plt.close(disp.figure_)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 3) Multi-class Precision-Recall Curves\\\\n\\\",\\n+    \\\"    for idx, cls in enumerate(classes):\\\\n\\\",\\n+    \\\"        disp = PrecisionRecallDisplay.from_predictions(\\\\n\\\",\\n+    \\\"            y_test_bin[:, idx], \\\\n\\\",\\n+    \\\"            y_proba[:, idx],\\\\n\\\",\\n+    \\\"            name=f\\\\\\\"PR curve for class {cls}\\\\\\\"\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        pr_path = os.path.join(plot_dir, f\\\\\\\"pr_curve_cls_{cls}.png\\\\\\\")\\\\n\\\",\\n+    \\\"        disp.figure_.savefig(pr_path)\\\\n\\\",\\n+    \\\"        mlflow.log_artifact(pr_path)\\\\n\\\",\\n+    \\\"        plt.close(disp.figure_)\\\\n\\\",\\n+    \\\"        \\\\n\\\",\\n+    \\\"    # \\u2705 Confusion Matrix Plot\\\\n\\\",\\n+    \\\"    cm_path = os.path.join(plot_dir, \\\\\\\"confusion_matrix.png\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    cm = confusion_matrix(y_test, y_pred)\\\\n\\\",\\n+    \\\"    plt.figure(figsize=(6, 6))\\\\n\\\",\\n+    \\\"    sns.heatmap(cm, annot=True, fmt=\\\\\\\"d\\\\\\\", cmap=\\\\\\\"Blues\\\\\\\")\\\\n\\\",\\n+    \\\"    plt.title(\\\\\\\"Confusion Matrix\\\\\\\")\\\\n\\\",\\n+    \\\"    plt.xlabel(\\\\\\\"Predicted\\\\\\\")\\\\n\\\",\\n+    \\\"    plt.ylabel(\\\\\\\"Actual\\\\\\\")\\\\n\\\",\\n+    \\\"    plt.savefig(cm_path)\\\\n\\\",\\n+    \\\"    mlflow.log_artifact(cm_path)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # \\u2705 SHAP Summary\\\\n\\\",\\n+    \\\"    shap_path = os.path.join(plot_dir, \\\\\\\"shap_summary.png\\\\\\\")\\\\n\\\",\\n+    \\\"    explainer = shap.TreeExplainer(model)\\\\n\\\",\\n+    \\\"    shap_values = explainer.shap_values(X_test)\\\\n\\\",\\n+    \\\"    shap.summary_plot(shap_values, X_test, show=False)\\\\n\\\",\\n+    \\\"    plt.savefig(shap_path)\\\\n\\\",\\n+    \\\"    mlflow.log_artifact(shap_path)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # \\u2500\\u2500\\u2500 1) Build a .pkl filename (you can include your model_name for clarity)\\\\n\\\",\\n+    \\\"    pkl_path = f\\\\\\\"Trained_models/{model_name}.pkl\\\\\\\"\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # \\u2500\\u2500\\u2500 2) Serialize your trained model to disk\\\\n\\\",\\n+    \\\"    with open(pkl_path, \\\\\\\"wb\\\\\\\") as f:\\\\n\\\",\\n+    \\\"        pickle.dump(model, f)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # \\u2500\\u2500\\u2500 3) Log that pickle file as an MLflow artifact\\\\n\\\",\\n+    \\\"    #     It will appear under Artifacts \\u2192 models/RandomForest_Iris_vYYYYMMDD_HHMMSS.pkl\\\\n\\\",\\n+    \\\"    mlflow.log_artifact(pkl_path, artifact_path=model_name)\\\\n\\\",\\n+    \\\"        \\\\n\\\",\\n+    \\\"    def get_latest_commit_hash(repo_path=\\\\\\\".\\\\\\\"):\\\\n\\\",\\n+    \\\"        # returns the full SHA of HEAD\\\\n\\\",\\n+    \\\"        res = subprocess.run(\\\\n\\\",\\n+    \\\"            [\\\\\\\"git\\\\\\\", \\\\\\\"-C\\\\\\\", repo_path, \\\\\\\"rev-parse\\\\\\\", \\\\\\\"HEAD\\\\\\\"],\\\\n\\\",\\n+    \\\"            capture_output=True, text=True, check=True)\\\\n\\\",\\n+    \\\"        \\\\n\\\",\\n+    \\\"        return res.stdout.strip()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    def get_remote_url(repo_path=\\\\\\\".\\\\\\\", remote=\\\\\\\"origin\\\\\\\"):\\\\n\\\",\\n+    \\\"        # returns something like git@github.com:user/repo.git or https://...\\\\n\\\",\\n+    \\\"        res = subprocess.run(\\\\n\\\",\\n+    \\\"            [\\\\\\\"git\\\\\\\", \\\\\\\"-C\\\\\\\", repo_path, \\\\\\\"config\\\\\\\", \\\\\\\"--get\\\\\\\", f\\\\\\\"remote.{remote}.url\\\\\\\"],\\\\n\\\",\\n+    \\\"            capture_output=True, text=True, check=True\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        return res.stdout.strip()\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    def make_commit_link(remote_url, commit_hash):\\\\n\\\",\\n+    \\\"        # handle GitHub/GitLab convention; strip \\u201c.git\\u201d if present\\\\n\\\",\\n+    \\\"        base = remote_url.rstrip(\\\\\\\".git\\\\\\\")\\\\n\\\",\\n+    \\\"        # if SSH form (git@github.com:owner/repo), convert to https\\\\n\\\",\\n+    \\\"        if base.startswith(\\\\\\\"git@\\\\\\\"):\\\\n\\\",\\n+    \\\"            base = base.replace(\\\\\\\":\\\\\\\", \\\\\\\"/\\\\\\\").replace(\\\\\\\"git@\\\\\\\", \\\\\\\"https://\\\\\\\")\\\\n\\\",\\n+    \\\"        return f\\\\\\\"{base}/commit/{commit_hash}\\\\\\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    def simple_commit_and_push_and_log(repo_path=\\\\\\\".\\\\\\\", message=\\\\\\\"Auto commit\\\\\\\", remote=\\\\\\\"origin\\\\\\\", branch=\\\\\\\"main\\\\\\\"):\\\\n\\\",\\n+    \\\"    # 1) Check for changes\\\\n\\\",\\n+    \\\"        status = subprocess.run(\\\\n\\\",\\n+    \\\"            [\\\\\\\"git\\\\\\\", \\\\\\\"-C\\\\\\\", repo_path, \\\\\\\"status\\\\\\\", \\\\\\\"--porcelain\\\\\\\"],\\\\n\\\",\\n+    \\\"            capture_output=True, text=True\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        if not status.stdout.strip():\\\\n\\\",\\n+    \\\"            print(\\\\\\\"\\ud83d\\udfe1 No changes to commit.\\\\\\\")\\\\n\\\",\\n+    \\\"            return None, None\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"        # 2) Stage everything\\\\n\\\",\\n+    \\\"        add = subprocess.run(\\\\n\\\",\\n+    \\\"            [\\\\\\\"git\\\\\\\", \\\\\\\"-C\\\\\\\", repo_path, \\\\\\\"add\\\\\\\", \\\\\\\"--all\\\\\\\"],\\\\n\\\",\\n+    \\\"            capture_output=True, text=True\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        if add.returncode:\\\\n\\\",\\n+    \\\"            print(\\\\\\\"\\u274c git add failed:\\\\\\\\n\\\\\\\", add.stderr)\\\\n\\\",\\n+    \\\"            return None, None\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"        # 3) Commit\\\\n\\\",\\n+    \\\"        commit = subprocess.run(\\\\n\\\",\\n+    \\\"            [\\\\\\\"git\\\\\\\", \\\\\\\"-C\\\\\\\", repo_path, \\\\\\\"commit\\\\\\\", \\\\\\\"-m\\\\\\\", message],\\\\n\\\",\\n+    \\\"            capture_output=True, text=True\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        if commit.returncode:\\\\n\\\",\\n+    \\\"            print(\\\\\\\"\\u274c git commit failed:\\\\\\\\n\\\\\\\", commit.stderr)\\\\n\\\",\\n+    \\\"            return None, None\\\\n\\\",\\n+    \\\"        print(\\\\\\\"\\u2705 Commit successful.\\\\\\\")\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"        # 4) Push\\\\n\\\",\\n+    \\\"        push = subprocess.run(\\\\n\\\",\\n+    \\\"            [\\\\\\\"git\\\\\\\", \\\\\\\"-C\\\\\\\", repo_path, \\\\\\\"push\\\\\\\", \\\\\\\"-u\\\\\\\", remote, branch],\\\\n\\\",\\n+    \\\"            capture_output=True, text=True\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        if push.returncode:\\\\n\\\",\\n+    \\\"            print(\\\\\\\"\\u274c git push failed:\\\\\\\\n\\\\\\\", push.stderr)\\\\n\\\",\\n+    \\\"        else:\\\\n\\\",\\n+    \\\"            print(\\\\\\\"\\ud83d\\ude80 Push successful.\\\\\\\")\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"        # 5) Retrieve hash & remote URL\\\\n\\\",\\n+    \\\"        sha = get_latest_commit_hash(repo_path)\\\\n\\\",\\n+    \\\"        url = get_remote_url(repo_path, remote)\\\\n\\\",\\n+    \\\"        link = make_commit_link(url, sha)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"        return sha, link\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"      \\\\n\\\",\\n+    \\\"    sha, link = simple_commit_and_push_and_log(\\\\n\\\",\\n+    \\\"        repo_path=\\\\\\\".\\\\\\\",\\\\n\\\",\\n+    \\\"        message=\\\\\\\"Auto commit after successful training\\\\\\\"\\\\n\\\",\\n+    \\\"    )\\\\n\\\",\\n+    \\\"    if sha and link:\\\\n\\\",\\n+    \\\"        diff_text = subprocess.check_output(\\\\n\\\",\\n+    \\\"            [\\\\\\\"git\\\\\\\", \\\\\\\"-C\\\\\\\", \\\\\\\".\\\\\\\", \\\\\\\"diff\\\\\\\", previous_commit_hash, sha],\\\\n\\\",\\n+    \\\"            encoding=\\\\\\\"utf-8\\\\\\\",\\\\n\\\",\\n+    \\\"            errors=\\\\\\\"ignore\\\\\\\"    # or \\\\\\\"replace\\\\\\\"\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"                \\\\n\\\",\\n+    \\\"        # 1) Get your repo\\u2019s remote URL and normalize to HTTPS\\\\n\\\",\\n+    \\\"        remote_url = subprocess.check_output(\\\\n\\\",\\n+    \\\"            [\\\\\\\"git\\\\\\\", \\\\\\\"config\\\\\\\", \\\\\\\"--get\\\\\\\", \\\\\\\"remote.origin.url\\\\\\\"],\\\\n\\\",\\n+    \\\"            text=True\\\\n\\\",\\n+    \\\"        ).strip().rstrip(\\\\\\\".git\\\\\\\")\\\\n\\\",\\n+    \\\"        if remote_url.startswith(\\\\\\\"git@\\\\\\\"):\\\\n\\\",\\n+    \\\"            # git@github.com:owner/repo.git \\u2192 https://github.com/owner/repo\\\\n\\\",\\n+    \\\"            remote_url = remote_url.replace(\\\\\\\":\\\\\\\", \\\\\\\"/\\\\\\\").replace(\\\\\\\"git@\\\\\\\", \\\\\\\"https://\\\\\\\")\\\\n\\\",\\n+    \\\"        \\\\n\\\",\\n+    \\\"        # 2) Build commit URLs\\\\n\\\",\\n+    \\\"        previous_commit_url  = f\\\\\\\"{remote_url}/commit/{previous_commit_hash}\\\\\\\"\\\\n\\\",\\n+    \\\"        current_commit_url = f\\\\\\\"{remote_url}/commit/{sha}\\\\\\\"\\\\n\\\",\\n+    \\\"        diff_data = {\\\\n\\\",\\n+    \\\"            \\\\\\\"previous_commit\\\\\\\":  previous_commit_hash,\\\\n\\\",\\n+    \\\"            \\\\\\\"previous_commit_url\\\\\\\":previous_commit_url,\\\\n\\\",\\n+    \\\"            \\\\\\\"current_commit_url\\\\\\\":current_commit_url,\\\\n\\\",\\n+    \\\"            \\\\\\\"current_commit\\\\\\\": sha,\\\\n\\\",\\n+    \\\"            \\\\\\\"diff\\\\\\\": diff_text\\\\n\\\",\\n+    \\\"        }\\\\n\\\",\\n+    \\\"        mlflow.log_dict(\\\\n\\\",\\n+    \\\"            diff_data,\\\\n\\\",\\n+    \\\"            artifact_file=\\\\\\\"commit_diff.json\\\\\\\"\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        mlflow.set_tag(\\\\\\\"git_previous_commit_hash\\\\\\\", previous_commit_hash)\\\\n\\\",\\n+    \\\"        mlflow.set_tag(\\\\\\\"git_current_commit_hash\\\\\\\", sha)\\\\n\\\",\\n+    \\\"        mlflow.set_tag(\\\\\\\"git__current_commit_url\\\\\\\", link) \\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    client   = MlflowClient()\\\\n\\\",\\n+    \\\"    run_id    = run.info.run_id\\\\n\\\",\\n+    \\\"    run_info  = client.get_run(run_id).info\\\\n\\\",\\n+    \\\"    run_data  = client.get_run(run_id).data\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # 1) params, metrics, tags\\\\n\\\",\\n+    \\\"    params  = dict(run_data.params)\\\\n\\\",\\n+    \\\"    metrics = dict(run_data.metrics)\\\\n\\\",\\n+    \\\"    tags    = dict(run_data.tags)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # (4) List artifacts under a specific subfolder\\\\n\\\",\\n+    \\\"    run_meta     = client.get_run(run_id).info\\\\n\\\",\\n+    \\\"    artifact_uri = run_meta.artifact_uri  # base URI for all artifacts\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    artifact_meta = []\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    def _gather(path=\\\\\\\"\\\\\\\"):\\\\n\\\",\\n+    \\\"        for af in client.list_artifacts(run_id, path):\\\\n\\\",\\n+    \\\"            # If it\\u2019s a directory, recurse\\\\n\\\",\\n+    \\\"            if af.is_dir:\\\\n\\\",\\n+    \\\"                _gather(af.path)\\\\n\\\",\\n+    \\\"                continue\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"            rel_path = af.path\\\\n\\\",\\n+    \\\"            uri      = f\\\\\\\"{artifact_uri}/{rel_path}\\\\\\\"\\\\n\\\",\\n+    \\\"            lower    = rel_path.lower()\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"            # 1) Text files \\u2192 download & embed contents\\\\n\\\",\\n+    \\\"            if lower.endswith((\\\\\\\".json\\\\\\\", \\\\\\\".txt\\\\\\\", \\\\\\\".patch\\\\\\\")):\\\\n\\\",\\n+    \\\"                local = client.download_artifacts(run_id, rel_path)\\\\n\\\",\\n+    \\\"                with open(local, \\\\\\\"r\\\\\\\", encoding=\\\\\\\"utf-8\\\\\\\") as f:\\\\n\\\",\\n+    \\\"                    content = f.read()\\\\n\\\",\\n+    \\\"                artifact_meta.append({\\\\n\\\",\\n+    \\\"                    \\\\\\\"path\\\\\\\":    rel_path,\\\\n\\\",\\n+    \\\"                    \\\\\\\"type\\\\\\\":    \\\\\\\"text\\\\\\\",\\\\n\\\",\\n+    \\\"                    \\\\\\\"content\\\\\\\": content\\\\n\\\",\\n+    \\\"                })\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"            # 2) Images \\u2192 surface a clickable URI\\\\n\\\",\\n+    \\\"            elif lower.endswith((\\\\\\\".png\\\\\\\", \\\\\\\".jpg\\\\\\\", \\\\\\\".jpeg\\\\\\\", \\\\\\\".svg\\\\\\\")):\\\\n\\\",\\n+    \\\"                artifact_meta.append({\\\\n\\\",\\n+    \\\"                    \\\\\\\"path\\\\\\\": rel_path,\\\\n\\\",\\n+    \\\"                    \\\\\\\"type\\\\\\\": \\\\\\\"image\\\\\\\",\\\\n\\\",\\n+    \\\"                    \\\\\\\"uri\\\\\\\":  uri\\\\n\\\",\\n+    \\\"                })\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"            # 3) Everything else \\u2192 just link\\\\n\\\",\\n+    \\\"            else:\\\\n\\\",\\n+    \\\"                artifact_meta.append({\\\\n\\\",\\n+    \\\"                    \\\\\\\"path\\\\\\\": rel_path,\\\\n\\\",\\n+    \\\"                    \\\\\\\"type\\\\\\\": \\\\\\\"other\\\\\\\",\\\\n\\\",\\n+    \\\"                    \\\\\\\"uri\\\\\\\":  uri\\\\n\\\",\\n+    \\\"                })\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # Run the gather\\\\n\\\",\\n+    \\\"    _gather()\\\\n\\\",\\n+    \\\"     \\\\n\\\",\\n+    \\\"    summary = {\\\\n\\\",\\n+    \\\"        \\\\\\\"run_id\\\\\\\":         run_id,\\\\n\\\",\\n+    \\\"        \\\\\\\"run_name\\\\\\\": run_info.run_name,\\\\n\\\",\\n+    \\\"        \\\\\\\"experiment_id\\\\\\\":  run_info.experiment_id,\\\\n\\\",\\n+    \\\"        \\\\\\\"start_time\\\\\\\":     run_info.start_time,\\\\n\\\",\\n+    \\\"        \\\\\\\"end_time\\\\\\\":       run_info.end_time,\\\\n\\\",\\n+    \\\"        \\\\\\\"params\\\\\\\":         params,\\\\n\\\",\\n+    \\\"        \\\\\\\"metrics\\\\\\\":        metrics,\\\\n\\\",\\n+    \\\"        \\\\\\\"tags\\\\\\\":           tags,\\\\n\\\",\\n+    \\\"        \\\\\\\"artifacts\\\\\\\":      artifact_meta\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 1) Determine notebook directory (where your .ipynb lives)\\\\n\\\",\\n+    \\\"    notebook_dir = os.getcwd()\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # \\u2705 Create a subdirectory inside MODEL_PROVENANCE for the model\\\\n\\\",\\n+    \\\"    summary_dir = os.path.join(os.getcwd(), \\\\\\\"MODEL_PROVENANCE\\\\\\\", model_name)\\\\n\\\",\\n+    \\\"    os.makedirs(summary_dir, exist_ok=True)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"   # 2) Pick a filename based on your model_name\\\\n\\\",\\n+    \\\"    summary_filename   = f\\\\\\\"{model_name}_run_summary.json\\\\\\\"\\\\n\\\",\\n+    \\\"    summary_local_path = os.path.join(summary_dir, summary_filename)\\\\n\\\",\\n+    \\\"   # 3) Write the JSON locally\\\\n\\\",\\n+    \\\"    with open(summary_local_path, \\\\\\\"w\\\\\\\", encoding=\\\\\\\"utf-8\\\\\\\") as f:\\\\n\\\",\\n+    \\\"        json.dump(summary, f, indent=2)\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # 4) (Optional) Mirror it into MLflow artifacts under a single folder\\\\n\\\",\\n+    \\\"    mlflow.log_artifact(summary_local_path, artifact_path=\\\\\\\"run_summaries\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    mlflow.end_run()\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"d0c4e1b2-9fa9-4606-8128-6ac66b5c6e78\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"what does it create: \\\\n\\\",\\n+    \\\"lable_mapping in the current dir\\\\n\\\",\\n+    \\\"provenence file :REPO/notebooks/RQ_notebooks/MODEL_PROVENANCE/RandomForest_Iris_v20250425_120045_run_summary.json\\\\n\\\",\\n+    \\\"plots based on run:REPO/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_120045/shap_summary.png\\\\n\\\",\\n+    \\\"mlrun:REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/5d1fa0fc65af47128f3200628b1afaea\\\\n\\\",\\n+    \\\"trained model:REPO/notebooks/RQ_notebooks/Trained_models/RandomForest_Iris_v20250425_120852.pkl\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"7a5e3bbb-0288-47d0-9dc4-2855d7e4801a\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"1. Standards-compliant export (JSON-LD + Turtle)\\\\n\\\",\\n+    \\\"I already have your plain run_summary.json , wrap it in a JSON-LD context that maps your fields into PROV-O terms, then use rdflib to emit Turtle:\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"28ed1cfb-930a-4f17-a48f-30e4cffb7f3e\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"import json\\\\n\\\",\\n+    \\\"import pandas as pd\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Load the JSON file\\\\n\\\",\\n+    \\\"json_path = \\\\\\\"/mnt/data/REPO/notebooks/RQ_notebooks/MODEL_PROVENANCE/RandomForest_Iris_v20250425_125653/RandomForest_Iris_v20250425_125653_run_summary.json\\\\\\\"\\\\n\\\",\\n+    \\\"with open(json_path, \\\\\\\"r\\\\\\\", encoding=\\\\\\\"utf-8\\\\\\\") as file:\\\\n\\\",\\n+    \\\"    data = json.load(file)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Extract justification tags\\\\n\\\",\\n+    \\\"justifications = {\\\\n\\\",\\n+    \\\"    k: v for k, v in data.get(\\\\\\\"tags\\\\\\\", {}).items()\\\\n\\\",\\n+    \\\"    if k.startswith(\\\\\\\"justification_\\\\\\\")\\\\n\\\",\\n+    \\\"}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Create a DataFrame\\\\n\\\",\\n+    \\\"justification_df = pd.DataFrame([\\\\n\\\",\\n+    \\\"    {\\\\\\\"Decision\\\\\\\": k.replace(\\\\\\\"justification_\\\\\\\", \\\\\\\"\\\\\\\"), \\\\\\\"Justification\\\\\\\": v}\\\\n\\\",\\n+    \\\"    for k, v in justifications.items()\\\\n\\\",\\n+    \\\"])\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"import ace_tools as tools; tools.display_dataframe_to_user(name=\\\\\\\"Researcher Justifications\\\\\\\", dataframe=justification_df)\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 66,\\n+   \\\"id\\\": \\\"5cf88da4-69f8-4982-a594-28cf25e4f79a\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"Converted RandomForest_Iris_v20250425_121328_run_summary.json \\u2192 RandomForest_Iris_v20250425_121328.jsonld, RandomForest_Iris_v20250425_121328.ttl\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"def iso8601(ms):\\\\n\\\",\\n+    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"Convert milliseconds since epoch to ISO8601 UTC.\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    return datetime.fromtimestamp(ms / 1000, tz=timezone.utc).isoformat()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"for json_path in glob.glob(\\\\\\\"MODEL_PROVENANCE/*/*_run_summary.json\\\\\\\"):\\\\n\\\",\\n+    \\\"    basename   = os.path.basename(json_path)\\\\n\\\",\\n+    \\\"    model_name = basename.rsplit(\\\\\\\"_run_summary.json\\\\\\\", 1)[0]\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    with open(json_path, \\\\\\\"r\\\\\\\", encoding=\\\\\\\"utf-8\\\\\\\") as f:\\\\n\\\",\\n+    \\\"        summary = json.load(f)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    #\\u2013\\u2013 Minimal override context: keep all your flat fields as-is,\\\\n\\\",\\n+    \\\"    #\\u2013\\u2013 and only map the actual PROV terms to their IRIs.\\\\n\\\",\\n+    \\\"    ctx = {\\\\n\\\",\\n+    \\\"        # keep these flat\\\\n\\\",\\n+    \\\"        \\\\\\\"run_id\\\\\\\":       { \\\\\\\"@id\\\\\\\": \\\\\\\"run_id\\\\\\\" },\\\\n\\\",\\n+    \\\"        \\\\\\\"run_name\\\\\\\":     { \\\\\\\"@id\\\\\\\": \\\\\\\"run_name\\\\\\\" },\\\\n\\\",\\n+    \\\"        \\\\\\\"experiment_id\\\\\\\":{ \\\\\\\"@id\\\\\\\": \\\\\\\"experiment_id\\\\\\\" },\\\\n\\\",\\n+    \\\"        \\\\\\\"params\\\\\\\":       { \\\\\\\"@id\\\\\\\": \\\\\\\"params\\\\\\\" },\\\\n\\\",\\n+    \\\"        \\\\\\\"metrics\\\\\\\":      { \\\\\\\"@id\\\\\\\": \\\\\\\"metrics\\\\\\\" },\\\\n\\\",\\n+    \\\"        \\\\\\\"artifacts\\\\\\\":    { \\\\\\\"@id\\\\\\\": \\\\\\\"artifacts\\\\\\\" },\\\\n\\\",\\n+    \\\"        \\\\\\\"tags\\\\\\\":         { \\\\\\\"@id\\\\\\\": \\\\\\\"tags\\\\\\\" },\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        # provenance namespace\\\\n\\\",\\n+    \\\"        \\\\\\\"prov\\\\\\\": \\\\\\\"http://www.w3.org/ns/prov#\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"xsd\\\\\\\":  \\\\\\\"http://www.w3.org/2001/XMLSchema#\\\\\\\",\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        # map your timestamp fields into PROV\\\\n\\\",\\n+    \\\"        \\\\\\\"start_time\\\\\\\": { \\\\\\\"@id\\\\\\\": \\\\\\\"prov:startedAtTime\\\\\\\", \\\\\\\"@type\\\\\\\": \\\\\\\"xsd:dateTime\\\\\\\" },\\\\n\\\",\\n+    \\\"        \\\\\\\"end_time\\\\\\\":   { \\\\\\\"@id\\\\\\\": \\\\\\\"prov:endedAtTime\\\\\\\",   \\\\\\\"@type\\\\\\\": \\\\\\\"xsd:dateTime\\\\\\\" },\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        # PROV-used/generated\\\\n\\\",\\n+    \\\"        \\\\\\\"used\\\\\\\":      { \\\\\\\"@id\\\\\\\": \\\\\\\"prov:used\\\\\\\",      \\\\\\\"@type\\\\\\\": \\\\\\\"@id\\\\\\\" },\\\\n\\\",\\n+    \\\"        \\\\\\\"generated\\\\\\\": { \\\\\\\"@id\\\\\\\": \\\\\\\"prov:generated\\\\\\\", \\\\\\\"@type\\\\\\\": \\\\\\\"@id\\\\\\\" },\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        # JSON-LD boilerplate\\\\n\\\",\\n+    \\\"        \\\\\\\"@id\\\\\\\":   \\\\\\\"@id\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"@type\\\\\\\": \\\\\\\"@type\\\\\\\"\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    #\\u2013\\u2013 Build JSON-LD document, re-using your original keys verbatim\\\\n\\\",\\n+    \\\"    doc = {\\\\n\\\",\\n+    \\\"        \\\\\\\"@context\\\\\\\":      ctx,\\\\n\\\",\\n+    \\\"        \\\\\\\"run_id\\\\\\\":        summary[\\\\\\\"run_id\\\\\\\"],\\\\n\\\",\\n+    \\\"        \\\\\\\"run_name\\\\\\\":      summary.get(\\\\\\\"run_name\\\\\\\"),\\\\n\\\",\\n+    \\\"        \\\\\\\"experiment_id\\\\\\\": summary.get(\\\\\\\"experiment_id\\\\\\\"),\\\\n\\\",\\n+    \\\"        \\\\\\\"params\\\\\\\":        summary.get(\\\\\\\"params\\\\\\\", {}),\\\\n\\\",\\n+    \\\"        \\\\\\\"metrics\\\\\\\":       summary.get(\\\\\\\"metrics\\\\\\\", {}),\\\\n\\\",\\n+    \\\"        \\\\\\\"artifacts\\\\\\\":     summary.get(\\\\\\\"artifacts\\\\\\\", []),\\\\n\\\",\\n+    \\\"        \\\\\\\"tags\\\\\\\":          summary.get(\\\\\\\"tags\\\\\\\", {}),\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        # PROV fields:\\\\n\\\",\\n+    \\\"        \\\\\\\"start_time\\\\\\\": iso8601(summary[\\\\\\\"start_time\\\\\\\"])\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    if summary.get(\\\\\\\"end_time\\\\\\\") is not None:\\\\n\\\",\\n+    \\\"        doc[\\\\\\\"end_time\\\\\\\"] = iso8601(summary[\\\\\\\"end_time\\\\\\\"])\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # for used/generated, just point at your dataset/model URIs\\\\n\\\",\\n+    \\\"    # (or blank-node them if you prefer richer structure)\\\\n\\\",\\n+    \\\"    doc[\\\\\\\"used\\\\\\\"] = summary.get(\\\\\\\"tags\\\\\\\", {}).get(\\\\\\\"dataset_uri\\\\\\\") or []\\\\n\\\",\\n+    \\\"    doc[\\\\\\\"generated\\\\\\\"] = [\\\\n\\\",\\n+    \\\"        art.get(\\\\\\\"uri\\\\\\\") or art.get(\\\\\\\"path\\\\\\\")\\\\n\\\",\\n+    \\\"        for art in summary.get(\\\\\\\"artifacts\\\\\\\", [])\\\\n\\\",\\n+    \\\"    ]\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    #\\u2013\\u2013 write JSON-LD\\\\n\\\",\\n+    \\\"    out_jsonld = os.path.join(\\\\\\\"MODEL_PROVENANCE\\\\\\\", model_name, f\\\\\\\"{model_name}.jsonld\\\\\\\")\\\\n\\\",\\n+    \\\"    with open(out_jsonld, \\\\\\\"w\\\\\\\", encoding=\\\\\\\"utf-8\\\\\\\") as f:\\\\n\\\",\\n+    \\\"        json.dump(doc, f, indent=2)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    #\\u2013\\u2013 parse & serialize to Turtle\\\\n\\\",\\n+    \\\"    g = Graph().parse(data=json.dumps(doc), format=\\\\\\\"json-ld\\\\\\\")\\\\n\\\",\\n+    \\\"    out_ttl = os.path.join(\\\\\\\"MODEL_PROVENANCE\\\\\\\", model_name, f\\\\\\\"{model_name}.ttl\\\\\\\")\\\\n\\\",\\n+    \\\"    g.serialize(destination=out_ttl, format=\\\\\\\"turtle\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"Converted {basename} \\u2192 {os.path.basename(out_jsonld)}, {os.path.basename(out_ttl)}\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"83d6d524-01da-4f20-8131-0d4a3ac005e2\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"This code programatically, finds diff between generated Json file and created JsonLD and .TTL file to make it easier to understand if there is any discrepency\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 67,\\n+   \\\"id\\\": \\\"77a420c0-230d-41c0-9b63-f3dbbca1e670\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"== JSON-LD vs TTL ==\\\\n\\\",\\n+      \\\"Change summary:\\\\n\\\",\\n+      \\\"type\\\\n\\\",\\n+      \\\"changed    1 \\\\n\\\",\\n+      \\\"\\\\n\\\",\\n+      \\\"First 10 \\u2018changed\\u2019 entries:\\\\n\\\",\\n+      \\\"Top-level adds/removes:\\\\n\\\",\\n+      \\\"Empty DataFrame\\\\n\\\",\\n+      \\\"Columns: [path, type, a, b]\\\\n\\\",\\n+      \\\"Index: []\\\\n\\\",\\n+      \\\"\\\\n\\\",\\n+      \\\"== JSON vs JSON-LD ==\\\\n\\\",\\n+      \\\"Change summary:\\\\n\\\",\\n+      \\\"type\\\\n\\\",\\n+      \\\"added      3\\\\n\\\",\\n+      \\\"removed    1\\\\n\\\",\\n+      \\\"changed    1 \\\\n\\\",\\n+      \\\"\\\\n\\\",\\n+      \\\"First 10 \\u2018changed\\u2019 entries:\\\\n\\\",\\n+      \\\"Top-level adds/removes:\\\\n\\\",\\n+      \\\"Empty DataFrame\\\\n\\\",\\n+      \\\"Columns: [path, type, a, b]\\\\n\\\",\\n+      \\\"Index: []\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def load_as_dict(path):\\\\n\\\",\\n+    \\\"    if path.endswith((\\\\\\\".ttl\\\\\\\", \\\\\\\".turtle\\\\\\\")):\\\\n\\\",\\n+    \\\"        g = Graph()\\\\n\\\",\\n+    \\\"        g.parse(path, format=\\\\\\\"turtle\\\\\\\")\\\\n\\\",\\n+    \\\"        # normalize to JSON-LD dict\\\\n\\\",\\n+    \\\"        return json.loads(g.serialize(format=\\\\\\\"json-ld\\\\\\\", indent=2))\\\\n\\\",\\n+    \\\"    else:\\\\n\\\",\\n+    \\\"        with open(path, encoding=\\\\\\\"utf-8\\\\\\\") as f:\\\\n\\\",\\n+    \\\"            return json.load(f)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def compare_json(a, b, path=\\\\\\\"\\\\\\\"):\\\\n\\\",\\n+    \\\"    diffs = []\\\\n\\\",\\n+    \\\"    if isinstance(a, dict) and isinstance(b, dict):\\\\n\\\",\\n+    \\\"        all_keys = set(a) | set(b)\\\\n\\\",\\n+    \\\"        for k in all_keys:\\\\n\\\",\\n+    \\\"            new_path = f\\\\\\\"{path}/{k}\\\\\\\" if path else k\\\\n\\\",\\n+    \\\"            if k not in a:\\\\n\\\",\\n+    \\\"                diffs.append({\\\\\\\"path\\\\\\\": new_path, \\\\\\\"type\\\\\\\": \\\\\\\"added\\\\\\\",   \\\\\\\"a\\\\\\\": None,    \\\\\\\"b\\\\\\\": b[k]})\\\\n\\\",\\n+    \\\"            elif k not in b:\\\\n\\\",\\n+    \\\"                diffs.append({\\\\\\\"path\\\\\\\": new_path, \\\\\\\"type\\\\\\\": \\\\\\\"removed\\\\\\\", \\\\\\\"a\\\\\\\": a[k],   \\\\\\\"b\\\\\\\": None})\\\\n\\\",\\n+    \\\"            else:\\\\n\\\",\\n+    \\\"                diffs.extend(compare_json(a[k], b[k], new_path))\\\\n\\\",\\n+    \\\"    elif isinstance(a, list) and isinstance(b, list):\\\\n\\\",\\n+    \\\"        for i, (ia, ib) in enumerate(zip(a, b)):\\\\n\\\",\\n+    \\\"            diffs.extend(compare_json(ia, ib, f\\\\\\\"{path}[{i}]\\\\\\\"))\\\\n\\\",\\n+    \\\"        # handle length mismatches\\\\n\\\",\\n+    \\\"        if len(a) < len(b):\\\\n\\\",\\n+    \\\"            for i in range(len(a), len(b)):\\\\n\\\",\\n+    \\\"                diffs.append({\\\\\\\"path\\\\\\\": f\\\\\\\"{path}[{i}]\\\\\\\", \\\\\\\"type\\\\\\\": \\\\\\\"added\\\\\\\",   \\\\\\\"a\\\\\\\": None,  \\\\\\\"b\\\\\\\": b[i]})\\\\n\\\",\\n+    \\\"        elif len(a) > len(b):\\\\n\\\",\\n+    \\\"            for i in range(len(b), len(a)):\\\\n\\\",\\n+    \\\"                diffs.append({\\\\\\\"path\\\\\\\": f\\\\\\\"{path}[{i}]\\\\\\\", \\\\\\\"type\\\\\\\": \\\\\\\"removed\\\\\\\", \\\\\\\"a\\\\\\\": a[i],  \\\\\\\"b\\\\\\\": None})\\\\n\\\",\\n+    \\\"    else:\\\\n\\\",\\n+    \\\"        if a != b:\\\\n\\\",\\n+    \\\"            diffs.append({\\\\\\\"path\\\\\\\": path, \\\\\\\"type\\\\\\\": \\\\\\\"changed\\\\\\\", \\\\\\\"a\\\\\\\": a, \\\\\\\"b\\\\\\\": b})\\\\n\\\",\\n+    \\\"    return diffs\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# --- Usage example -----------------------------------------------\\\\n\\\",\\n+    \\\"# REPO/notebooks/RQ_notebooks/MODEL_PROVENANCE/RandomForest_Iris_v20250425_121328/RandomForest_Iris_v20250425_121328_run_summary.json\\\\n\\\",\\n+    \\\"# # Compare JSON-LD vs Turtle:\\\\n\\\",\\n+    \\\"# a = load_as_dict(\\\\\\\"MODEL_PROVENANCE/RandomForest_Iris_v20250423_230422_run_summary.json\\\\\\\")\\\\n\\\",\\n+    \\\"# b = load_as_dict(\\\\\\\"MODEL_PROVENANCE/RandomForest_Iris_v20250423_230422.ttl\\\\\\\")\\\\n\\\",\\n+    \\\"# diffs_jsonld_vs_ttl = compare_json(a, b)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# # Compare JSON vs JSON-LD:\\\\n\\\",\\n+    \\\"# c = load_as_dict(\\\\\\\"MODEL_PROVENANCE/RandomForest_Iris_v20250423_230422_run_summary.json\\\\\\\")\\\\n\\\",\\n+    \\\"# d = load_as_dict(\\\\\\\"MODEL_PROVENANCE/RandomForest_Iris_v20250423_230422.jsonld\\\\\\\")\\\\n\\\",\\n+    \\\"# diffs_json_vs_jsonld = compare_json(c, d)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Define base directory\\\\n\\\",\\n+    \\\"base_dir = os.path.join(\\\\\\\"MODEL_PROVENANCE\\\\\\\", model_name)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Build full paths for the files to compare\\\\n\\\",\\n+    \\\"summary_json    = os.path.join(base_dir, f\\\\\\\"{model_name}_run_summary.json\\\\\\\")\\\\n\\\",\\n+    \\\"turtle_file     = os.path.join(base_dir, f\\\\\\\"{model_name}.ttl\\\\\\\")\\\\n\\\",\\n+    \\\"jsonld_file     = os.path.join(base_dir, f\\\\\\\"{model_name}.jsonld\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Load files\\\\n\\\",\\n+    \\\"a = load_as_dict(summary_json)\\\\n\\\",\\n+    \\\"b = load_as_dict(turtle_file)\\\\n\\\",\\n+    \\\"c = load_as_dict(summary_json)\\\\n\\\",\\n+    \\\"d = load_as_dict(jsonld_file)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Perform comparisons\\\\n\\\",\\n+    \\\"diffs_jsonld_vs_ttl = compare_json(a, b)\\\\n\\\",\\n+    \\\"diffs_json_vs_jsonld = compare_json(c, d)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Build DataFrames for interactive inspection\\\\n\\\",\\n+    \\\"df1 = pd.DataFrame(diffs_jsonld_vs_ttl)\\\\n\\\",\\n+    \\\"df2 = pd.DataFrame(diffs_json_vs_jsonld)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# --- Summaries & Filtering ---------------------------------------\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def summarize_and_preview(df, preview_n=10):\\\\n\\\",\\n+    \\\"    print(\\\\\\\"Change summary:\\\\\\\")\\\\n\\\",\\n+    \\\"    print(df['type'].value_counts().to_string(), \\\\\\\"\\\\\\\\n\\\\\\\")\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    print(f\\\\\\\"First {preview_n} \\u2018changed\\u2019 entries:\\\\\\\")\\\\n\\\",\\n+    \\\"    # print(df[df['type']==\\\\\\\"changed\\\\\\\"].head(preview_n).to_string(index=False), \\\\\\\"\\\\\\\\n\\\\\\\")\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # Top\\u2010level (one slash) adds/removes\\\\n\\\",\\n+    \\\"    top = df[df['path'].str.count(\\\\\\\"/\\\\\\\") == 1]\\\\n\\\",\\n+    \\\"    print(\\\\\\\"Top-level adds/removes:\\\\\\\")\\\\n\\\",\\n+    \\\"    print(top[top['type'].isin(['added','removed'])].to_string(index=False))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"print(\\\\\\\"== JSON-LD vs TTL ==\\\\\\\")\\\\n\\\",\\n+    \\\"summarize_and_preview(df1)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"print(\\\\\\\"\\\\\\\\n== JSON vs JSON-LD ==\\\\\\\")\\\\n\\\",\\n+    \\\"summarize_and_preview(df2)\\\\n\\\",\\n+    \\\"\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 68,\\n+   \\\"id\\\": \\\"41af9d6e-c683-45f9-bac1-296611b4d0b9\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"Removed in JSON-LD comparison:\\\\n\\\",\\n+      \\\"    path\\\\n\\\",\\n+      \\\"end_time\\\\n\\\",\\n+      \\\"\\\\n\\\",\\n+      \\\"Added in JSON-LD comparison:\\\\n\\\",\\n+      \\\"     path\\\\n\\\",\\n+      \\\" @context\\\\n\\\",\\n+      \\\"     used\\\\n\\\",\\n+      \\\"generated\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"# show all the removed paths (in JSON but not in JSON-LD)\\\\n\\\",\\n+    \\\"print(\\\\\\\"Removed in JSON-LD comparison:\\\\\\\")\\\\n\\\",\\n+    \\\"print(df2[df2['type']==\\\\\\\"removed\\\\\\\"][['path']].to_string(index=False))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# show all the added paths (in JSON-LD but not in JSON)\\\\n\\\",\\n+    \\\"print(\\\\\\\"\\\\\\\\nAdded in JSON-LD comparison:\\\\\\\")\\\\n\\\",\\n+    \\\"print(df2[df2['type']==\\\\\\\"added\\\\\\\"][['path']].to_string(index=False))\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 69,\\n+   \\\"id\\\": \\\"f0d6f92a-5cd9-4c78-9c2a-0cd3247137c6\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"Removed in .ttl comparison:\\\\n\\\",\\n+      \\\"Empty DataFrame\\\\n\\\",\\n+      \\\"Columns: [path]\\\\n\\\",\\n+      \\\"Index: []\\\\n\\\",\\n+      \\\"\\\\n\\\",\\n+      \\\"Added in .ttl comparison:\\\\n\\\",\\n+      \\\"Empty DataFrame\\\\n\\\",\\n+      \\\"Columns: [path]\\\\n\\\",\\n+      \\\"Index: []\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"# show all the removed paths (in JSON but not in JSON-LD)\\\\n\\\",\\n+    \\\"print(\\\\\\\"Removed in .ttl comparison:\\\\\\\")\\\\n\\\",\\n+    \\\"print(df1[df1['type']==\\\\\\\"removed\\\\\\\"][['path']].to_string(index=False))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# show all the added paths (in JSON-LD but not in JSON)\\\\n\\\",\\n+    \\\"print(\\\\\\\"\\\\\\\\nAdded in .ttl comparison:\\\\\\\")\\\\n\\\",\\n+    \\\"print(df1[df1['type']==\\\\\\\"added\\\\\\\"][['path']].to_string(index=False))\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"69efd0d0-9277-4efa-88cf-d2fd1b90d74c\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"Checks for completeness and mapping and time taken, needs work #TODO\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 70,\\n+   \\\"id\\\": \\\"165a13eb-7679-4f4c-b346-24f25da72cce\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"\\\\n\\\",\\n+      \\\"0/0 runs passed completeness checks (0.0%).\\\\n\\\",\\n+      \\\"\\\\n\\\",\\n+      \\\"Mapping integrity: 0/0 runs have zero diffs \\u2014 0.0%\\\\n\\\",\\n+      \\\"Overall quality score: 0.0%\\\\n\\\",\\n+      \\\"\\\\n\\\",\\n+      \\\"Benchmarking train_and_log() overhead:\\\\n\\\",\\n+      \\\"  \\u2022 No MLflow : 0.502s\\\\n\\\",\\n+      \\\"  \\u2022 With MLflow: 0.601s\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"# \\u2500\\u2500 User configuration \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Which keys must appear in every run_summary.json?\\\\n\\\",\\n+    \\\"REQUIRED_TOPLEVEL = {\\\\n\\\",\\n+    \\\"    \\\\\\\"run_id\\\\\\\", \\\\\\\"start_time\\\\\\\", \\\\\\\"end_time\\\\\\\",\\\\n\\\",\\n+    \\\"    \\\\\\\"params\\\\\\\", \\\\\\\"metrics\\\\\\\", \\\\\\\"tags\\\\\\\", \\\\\\\"artifacts\\\\\\\"\\\\n\\\",\\n+    \\\"}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# A couple of sub-fields we also want to spot-check:\\\\n\\\",\\n+    \\\"REQUIRED_PARAMS  = {\\\\\\\"random_state\\\\\\\"}\\\\n\\\",\\n+    \\\"REQUIRED_METRICS = {\\\\\\\"accuracy\\\\\\\"}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"JSON_SUMMARIES = glob.glob(\\\\\\\"MODEL_PROVENANCE/*_run_summary.json\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# \\u2500\\u2500 Helpers \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def iso8601(ms):\\\\n\\\",\\n+    \\\"    return datetime.fromtimestamp(ms/1000, tz=timezone.utc).isoformat()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def load_json(path):\\\\n\\\",\\n+    \\\"    with open(path, \\\\\\\"r\\\\\\\", encoding=\\\\\\\"utf-8\\\\\\\") as f:\\\\n\\\",\\n+    \\\"        return json.load(f)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def write_json(path, obj):\\\\n\\\",\\n+    \\\"    with open(path, \\\\\\\"w\\\\\\\", encoding=\\\\\\\"utf-8\\\\\\\") as f:\\\\n\\\",\\n+    \\\"        json.dump(obj, f, indent=2)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def convert_to_jsonld_and_ttl(summary, basename):\\\\n\\\",\\n+    \\\"    # build @context\\\\n\\\",\\n+    \\\"    ctx = {\\\\n\\\",\\n+    \\\"        \\\\\\\"prov\\\\\\\":    \\\\\\\"http://www.w3.org/ns/prov#\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"xsd\\\\\\\":     \\\\\\\"http://www.w3.org/2001/XMLSchema#\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"run\\\\\\\":     \\\\\\\"prov:Activity\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"start\\\\\\\":   \\\\\\\"prov:startedAtTime\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"end\\\\\\\":     \\\\\\\"prov:endedAtTime\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"used\\\\\\\":    \\\\\\\"prov:used\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"gen\\\\\\\":     \\\\\\\"prov:generated\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"param\\\\\\\":   \\\\\\\"prov:hadParameter\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"metric\\\\\\\":  \\\\\\\"prov:hadQuality\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"entity\\\\\\\":  \\\\\\\"prov:Entity\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"label\\\\\\\":   \\\\\\\"prov:label\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"value\\\\\\\":   \\\\\\\"prov:value\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"version\\\\\\\": \\\\\\\"prov:hadRevision\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"id\\\\\\\":      \\\\\\\"@id\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"type\\\\\\\":    \\\\\\\"@type\\\\\\\"\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    jsonld = {\\\\n\\\",\\n+    \\\"        \\\\\\\"@context\\\\\\\": ctx,\\\\n\\\",\\n+    \\\"        \\\\\\\"@id\\\\\\\":      f\\\\\\\"urn:run:{summary['run_id']}\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"@type\\\\\\\":    \\\\\\\"run\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"start\\\\\\\": {\\\\n\\\",\\n+    \\\"            \\\\\\\"@type\\\\\\\":  \\\\\\\"xsd:dateTime\\\\\\\",\\\\n\\\",\\n+    \\\"            \\\\\\\"@value\\\\\\\": iso8601(summary[\\\\\\\"start_time\\\\\\\"])\\\\n\\\",\\n+    \\\"        }\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"    if summary.get(\\\\\\\"end_time\\\\\\\") is not None:\\\\n\\\",\\n+    \\\"        jsonld[\\\\\\\"end\\\\\\\"] = {\\\\n\\\",\\n+    \\\"            \\\\\\\"@type\\\\\\\":  \\\\\\\"xsd:dateTime\\\\\\\",\\\\n\\\",\\n+    \\\"            \\\\\\\"@value\\\\\\\": iso8601(summary[\\\\\\\"end_time\\\\\\\"])\\\\n\\\",\\n+    \\\"        }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # params\\\\n\\\",\\n+    \\\"    jsonld[\\\\\\\"param\\\\\\\"] = [\\\\n\\\",\\n+    \\\"        {\\\\\\\"@type\\\\\\\":\\\\\\\"entity\\\\\\\",\\\\\\\"label\\\\\\\":k,\\\\\\\"value\\\\\\\":str(v)}\\\\n\\\",\\n+    \\\"        for k,v in summary.get(\\\\\\\"params\\\\\\\",{}).items()\\\\n\\\",\\n+    \\\"    ]\\\\n\\\",\\n+    \\\"    # metrics\\\\n\\\",\\n+    \\\"    jsonld[\\\\\\\"metric\\\\\\\"] = [\\\\n\\\",\\n+    \\\"        {\\\\\\\"@type\\\\\\\":\\\\\\\"entity\\\\\\\",\\\\\\\"label\\\\\\\":k,\\\\n\\\",\\n+    \\\"         \\\\\\\"value\\\\\\\":{\\\\\\\"@type\\\\\\\":\\\\\\\"xsd:decimal\\\\\\\",\\\\\\\"@value\\\\\\\":v}}\\\\n\\\",\\n+    \\\"        for k,v in summary.get(\\\\\\\"metrics\\\\\\\",{}).items()\\\\n\\\",\\n+    \\\"    ]\\\\n\\\",\\n+    \\\"    # artifacts\\\\n\\\",\\n+    \\\"    jsonld[\\\\\\\"gen\\\\\\\"] = [\\\\n\\\",\\n+    \\\"        {\\\\n\\\",\\n+    \\\"            \\\\\\\"@type\\\\\\\":\\\\\\\"entity\\\\\\\",\\\\n\\\",\\n+    \\\"            \\\\\\\"label\\\\\\\": art.get(\\\\\\\"path\\\\\\\") or art.get(\\\\\\\"label\\\\\\\"),\\\\n\\\",\\n+    \\\"            \\\\\\\"prov:location\\\\\\\": (\\\\n\\\",\\n+    \\\"                art.get(\\\\\\\"uri\\\\\\\")\\\\n\\\",\\n+    \\\"                or (art.get(\\\\\\\"content\\\\\\\",\\\\\\\"\\\\\\\")[:30]+\\\\\\\"\\u2026\\\\\\\")\\\\n\\\",\\n+    \\\"                if isinstance(art.get(\\\\\\\"content\\\\\\\"),str)\\\\n\\\",\\n+    \\\"                else \\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"            )\\\\n\\\",\\n+    \\\"        }\\\\n\\\",\\n+    \\\"        for art in summary.get(\\\\\\\"artifacts\\\\\\\",[])\\\\n\\\",\\n+    \\\"    ]\\\\n\\\",\\n+    \\\"    # dataset used\\\\n\\\",\\n+    \\\"    jsonld[\\\\\\\"used\\\\\\\"] = {\\\\n\\\",\\n+    \\\"        \\\\\\\"@type\\\\\\\":\\\\\\\"entity\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"label\\\\\\\": summary[\\\\\\\"tags\\\\\\\"].get(\\\\\\\"dataset_name\\\\\\\"),\\\\n\\\",\\n+    \\\"        \\\\\\\"version\\\\\\\": summary[\\\\\\\"tags\\\\\\\"].get(\\\\\\\"dataset_version\\\\\\\")\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # write JSON-LD\\\\n\\\",\\n+    \\\"    out_jsonld = f\\\\\\\"MODEL_PROVENANCE/{basename}.jsonld\\\\\\\"\\\\n\\\",\\n+    \\\"    write_json(out_jsonld, jsonld)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # serialize TTL\\\\n\\\",\\n+    \\\"    g = Graph().parse(data=json.dumps(jsonld), format=\\\\\\\"json-ld\\\\\\\")\\\\n\\\",\\n+    \\\"    out_ttl = f\\\\\\\"MODEL_PROVENANCE/{basename}.ttl\\\\\\\"\\\\n\\\",\\n+    \\\"    g.serialize(destination=out_ttl, format=\\\\\\\"turtle\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    return out_jsonld, out_ttl\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def normalize_jsonld(js):\\\\n\\\",\\n+    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"Simple deep-sort so compare_json doesn\\u2019t trip over ordering.\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    if isinstance(js, dict):\\\\n\\\",\\n+    \\\"        return {k: normalize_jsonld(js[k]) for k in sorted(js)}\\\\n\\\",\\n+    \\\"    if isinstance(js, list):\\\\n\\\",\\n+    \\\"        return sorted((normalize_jsonld(el) for el in js),\\\\n\\\",\\n+    \\\"                      key=lambda x: json.dumps(x, sort_keys=True))\\\\n\\\",\\n+    \\\"    return js\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def diff_roundtrip(orig_json, jsonld_path, ttl_path):\\\\n\\\",\\n+    \\\"    orig = load_json(orig_json)\\\\n\\\",\\n+    \\\"    ld   = load_json(jsonld_path)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # parse TTL back to JSON-LD\\\\n\\\",\\n+    \\\"    g = Graph().parse(ttl_path, format=\\\\\\\"turtle\\\\\\\")\\\\n\\\",\\n+    \\\"    ttl_as_ld = json.loads(g.serialize(format=\\\\\\\"json-ld\\\\\\\"))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # normalize\\\\n\\\",\\n+    \\\"    nl = normalize_jsonld(ld)\\\\n\\\",\\n+    \\\"    nt = normalize_jsonld(ttl_as_ld)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    return {\\\\n\\\",\\n+    \\\"        \\\\\\\"orig_vs_jsonld\\\\\\\":   compare_json(orig, ld),\\\\n\\\",\\n+    \\\"        \\\\\\\"jsonld_vs_ttl_ld\\\\\\\": compare_json(nl, nt)\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# \\u2500\\u2500 Main flow \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def main():\\\\n\\\",\\n+    \\\"    ok = 0\\\\n\\\",\\n+    \\\"    total = len(JSON_SUMMARIES)\\\\n\\\",\\n+    \\\"    missing_reports = []\\\\n\\\",\\n+    \\\"    cases = {}  # store diff results per run\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    for js_path in JSON_SUMMARIES:\\\\n\\\",\\n+    \\\"        summary = load_json(js_path)\\\\n\\\",\\n+    \\\"        base    = os.path.basename(js_path).split(\\\\\\\"_run_summary.json\\\\\\\")[0]\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        # 1) completeness check\\\\n\\\",\\n+    \\\"        if not REQUIRED_TOPLEVEL.issubset(summary):\\\\n\\\",\\n+    \\\"            missing = REQUIRED_TOPLEVEL - set(summary)\\\\n\\\",\\n+    \\\"            missing_reports.append((js_path, f\\\\\\\"missing fields {missing}\\\\\\\"))\\\\n\\\",\\n+    \\\"            continue\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        if not (REQUIRED_PARAMS <= summary[\\\\\\\"params\\\\\\\"].keys()):\\\\n\\\",\\n+    \\\"            missing_reports.append((js_path, f\\\\\\\"params missing {REQUIRED_PARAMS - summary['params'].keys()}\\\\\\\"))\\\\n\\\",\\n+    \\\"            continue\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        if not (REQUIRED_METRICS <= summary[\\\\\\\"metrics\\\\\\\"].keys()):\\\\n\\\",\\n+    \\\"            missing_reports.append((js_path, f\\\\\\\"metrics missing {REQUIRED_METRICS - summary['metrics'].keys()}\\\\\\\"))\\\\n\\\",\\n+    \\\"            continue\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        ok += 1\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        # 2) convert\\\\n\\\",\\n+    \\\"        jsonld_path, ttl_path = convert_to_jsonld_and_ttl(summary, base)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        # 3) diff\\\\n\\\",\\n+    \\\"        diffs = diff_roundtrip(js_path, jsonld_path, ttl_path)\\\\n\\\",\\n+    \\\"        cases[base] = diffs\\\\n\\\",\\n+    \\\"        print(f\\\\\\\"\\\\\\\\n\\u2500\\u2500 {base} diffs \\u2500\\u2500\\\\\\\")\\\\n\\\",\\n+    \\\"        print(\\\\\\\"  \\u2022 JSON \\u2192 JSON-LD:\\\\\\\", len(diffs[\\\\\\\"orig_vs_jsonld\\\\\\\"]), \\\\\\\"differences\\\\\\\")\\\\n\\\",\\n+    \\\"        print(\\\\\\\"  \\u2022 JSON-LD \\u2192 TTL \\u2192 JSON-LD:\\\\\\\", len(diffs[\\\\\\\"jsonld_vs_ttl_ld\\\\\\\"]), \\\\\\\"differences\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 4) completeness summary\\\\n\\\",\\n+    \\\"    completeness_pct = (100 * ok / total) if total else 0\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"\\\\\\\\n{ok}/{total} runs passed completeness checks ({completeness_pct:.1f}%).\\\\\\\")\\\\n\\\",\\n+    \\\"    if missing_reports:\\\\n\\\",\\n+    \\\"        print(\\\\\\\"\\\\\\\\nFailures:\\\\\\\")\\\\n\\\",\\n+    \\\"        for path, reason in missing_reports:\\\\n\\\",\\n+    \\\"            print(f\\\\\\\" \\u2022 {path}: {reason}\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 5) integrity check\\\\n\\\",\\n+    \\\"    total_runs = len(cases)\\\\n\\\",\\n+    \\\"    zero_diff_runs = sum(\\\\n\\\",\\n+    \\\"        1\\\\n\\\",\\n+    \\\"        for diffs in cases.values()\\\\n\\\",\\n+    \\\"        if not diffs[\\\\\\\"orig_vs_jsonld\\\\\\\"] and not diffs[\\\\\\\"jsonld_vs_ttl_ld\\\\\\\"]\\\\n\\\",\\n+    \\\"    )\\\\n\\\",\\n+    \\\"    integrity_pct = (100 * zero_diff_runs / total_runs) if total_runs else 0\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"\\\\\\\\nMapping integrity: {zero_diff_runs}/{total_runs} runs have zero diffs \\u2014 {integrity_pct:.1f}%\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 6) overall quality score\\\\n\\\",\\n+    \\\"    overall_score = (completeness_pct + integrity_pct) / 2\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"Overall quality score: {overall_score:.1f}%\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 7) Benchmark your training fn\\\\n\\\",\\n+    \\\"    print(\\\\\\\"\\\\\\\\nBenchmarking train_and_log() overhead:\\\\\\\")\\\\n\\\",\\n+    \\\"    def train_and_log(use_mlflow=False):\\\\n\\\",\\n+    \\\"        # \\u2190 your real instrumentation + fit logic here\\\\n\\\",\\n+    \\\"        time.sleep(0.5 + (0.1 if use_mlflow else 0))  # stub\\\\n\\\",\\n+    \\\"        return\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    for flag in (False, True):\\\\n\\\",\\n+    \\\"        start = time.time()\\\\n\\\",\\n+    \\\"        train_and_log(use_mlflow=flag)\\\\n\\\",\\n+    \\\"        elapsed = time.time() - start\\\\n\\\",\\n+    \\\"        label = \\\\\\\"With MLflow\\\\\\\" if flag else \\\\\\\"No MLflow\\\\\\\"\\\\n\\\",\\n+    \\\"        print(f\\\\\\\"  \\u2022 {label:10s}: {elapsed:.3f}s\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"if __name__ == \\\\\\\"__main__\\\\\\\":\\\\n\\\",\\n+    \\\"    main()\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"5883f673-371e-415e-a73e-5c9c88b56fb1\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"RQ2  implementation\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 72,\\n+   \\\"id\\\": \\\"6d07ac1c-ea80-4787-bcb9-da047d12167d\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"text/plain\\\": [\\n+       \\\"Index(['run_id', 'param_bootstrap', 'param_ccp_alpha', 'param_class_weight',\\\\n\\\",\\n+       \\\"       'param_columns_raw', 'param_criterion', 'param_database.description',\\\\n\\\",\\n+       \\\"       'param_database.id', 'param_database.name', 'param_database.owner',\\\\n\\\",\\n+       \\\"       'param_dataset.authors', 'param_dataset.doi', 'param_dataset.published',\\\\n\\\",\\n+       \\\"       'param_dataset.publisher', 'param_dataset.title',\\\\n\\\",\\n+       \\\"       'param_dropped_columns', 'param_feature_names',\\\\n\\\",\\n+       \\\"       'param_matplotlib_version', 'param_max_depth', 'param_max_features',\\\\n\\\",\\n+       \\\"       'param_max_leaf_nodes', 'param_max_samples',\\\\n\\\",\\n+       \\\"       'param_min_impurity_decrease', 'param_min_samples_leaf',\\\\n\\\",\\n+       \\\"       'param_min_samples_split', 'param_min_weight_fraction_leaf',\\\\n\\\",\\n+       \\\"       'param_numpy_version', 'param_n_estimators', 'param_n_features',\\\\n\\\",\\n+       \\\"       'param_n_features_final', 'param_n_jobs', 'param_n_records',\\\\n\\\",\\n+       \\\"       'param_n_test_samples', 'param_n_train_samples', 'param_oob_score',\\\\n\\\",\\n+       \\\"       'param_os_platform', 'param_pandas_version', 'param_python_version',\\\\n\\\",\\n+       \\\"       'param_random_state', 'param_retrieval_time', 'param_seaborn_version',\\\\n\\\",\\n+       \\\"       'param_shap_version', 'param_sklearn_version', 'param_test_size',\\\\n\\\",\\n+       \\\"       'param_verbose', 'param_warm_start', 'metric_accuracy',\\\\n\\\",\\n+       \\\"       'metric_accuracy_score_X_test', 'metric_dbrepo.num_deletes',\\\\n\\\",\\n+       \\\"       'metric_dbrepo.num_inserts', 'metric_dbrepo.row_count_end',\\\\n\\\",\\n+       \\\"       'metric_dbrepo.row_count_start', 'metric_f1_macro',\\\\n\\\",\\n+       \\\"       'metric_f1_score_X_test', 'metric_precision_macro',\\\\n\\\",\\n+       \\\"       'metric_precision_score_X_test', 'metric_recall_macro',\\\\n\\\",\\n+       \\\"       'metric_recall_score_X_test', 'metric_roc_auc',\\\\n\\\",\\n+       \\\"       'metric_roc_auc_score_X_test', 'metric_training_accuracy_score',\\\\n\\\",\\n+       \\\"       'metric_training_f1_score', 'metric_training_log_loss',\\\\n\\\",\\n+       \\\"       'metric_training_precision_score', 'metric_training_recall_score',\\\\n\\\",\\n+       \\\"       'metric_training_roc_auc', 'metric_training_score', 'tag_dataset_id',\\\\n\\\",\\n+       \\\"       'tag_dataset_name', 'tag_dataset_version', 'tag_data_source',\\\\n\\\",\\n+       \\\"       'tag_dbrepo.admin_email', 'tag_dbrepo.base_url',\\\\n\\\",\\n+       \\\"       'tag_dbrepo.granularity', 'tag_dbrepo.protocol_version',\\\\n\\\",\\n+       \\\"       'tag_dbrepo.repository_name', 'tag_dbrepo.table_last_modified',\\\\n\\\",\\n+       \\\"       'tag_estimator_class', 'tag_estimator_name',\\\\n\\\",\\n+       \\\"       'tag_git_current_commit_hash', 'tag_git_previous_commit_hash',\\\\n\\\",\\n+       \\\"       'tag_git__current_commit_url', 'tag_mlflow.log-model.history',\\\\n\\\",\\n+       \\\"       'tag_mlflow.runName', 'tag_mlflow.source.name',\\\\n\\\",\\n+       \\\"       'tag_mlflow.source.type', 'tag_mlflow.user', 'tag_model_name',\\\\n\\\",\\n+       \\\"       'tag_notebook_name', 'tag_target_name', 'tag_training_end_time',\\\\n\\\",\\n+       \\\"       'tag_training_start_time'],\\\\n\\\",\\n+       \\\"      dtype='object')\\\"\\n+      ]\\n+     },\\n+     \\\"execution_count\\\": 72,\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"execute_result\\\"\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"# Load all run summary JSON files\\\\n\\\",\\n+    \\\"files = glob.glob(\\\\\\\"MODEL_PROVENANCE/*/*_run_summary.json\\\\\\\")\\\\n\\\",\\n+    \\\"rows = []\\\\n\\\",\\n+    \\\"for f in files:\\\\n\\\",\\n+    \\\"    with open(f) as fh:\\\\n\\\",\\n+    \\\"        summary = json.load(fh)\\\\n\\\",\\n+    \\\"    # Flatten parameters and metrics\\\\n\\\",\\n+    \\\"    row = {\\\\\\\"run_id\\\\\\\": summary[\\\\\\\"run_id\\\\\\\"]}\\\\n\\\",\\n+    \\\"    row.update({f\\\\\\\"param_{k}\\\\\\\": v for k, v in summary.get(\\\\\\\"params\\\\\\\", {}).items()})\\\\n\\\",\\n+    \\\"    row.update({f\\\\\\\"metric_{k}\\\\\\\": v for k, v in summary.get(\\\\\\\"metrics\\\\\\\", {}).items()})\\\\n\\\",\\n+    \\\"    row.update({f\\\\\\\"tag_{k}\\\\\\\": v for k, v in summary.get(\\\\\\\"tags\\\\\\\", {}).items()})\\\\n\\\",\\n+    \\\"    rows.append(row)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Create DataFrame\\\\n\\\",\\n+    \\\"df = pd.DataFrame(rows)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Display the DataFrame\\\\n\\\",\\n+    \\\"df.columns\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"ba148da6-6ce5-45cf-a985-f164a53c969b\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"1) Tracing preprocessing steps\\\\n\\\",\\n+    \\\":\\\\n\\\",\\n+    \\\"Here are the top 4 Iris\\u2010focused preprocessing\\u2010tracing use cases I\\u2019d tackle first:\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"Reconstruct a run\\u2019s exact preprocessing\\\\n\\\",\\n+    \\\"Fetch a run\\u2019s run_id, columns_raw, dropped_columns, feature_names and test_size so you can replay the exact data pull & split.\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"Feature\\u2010drop impact analysis\\\\n\\\",\\n+    \\\"Identify runs where one or more measurements (e.g. petalwidthcm) were dropped and compare their test accuracies.\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"Best feature subset discovery\\\\n\\\",\\n+    \\\"Group runs by which features they used (sepals only vs petals only vs both) and rank them by test F1 or accuracy.\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"Common steps in high-accuracy runs\\\\n\\\",\\n+    \\\"Filter for runs with accuracy_score_X_test \\u2265 0.95 and list the shared preprocessing settings (dropped columns, test_size, etc.).\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 73,\\n+   \\\"id\\\": \\\"6e147555-afbf-4bba-b6da-7e90ff391920\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 12:23:44 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'df84c36b36cc4ebd90a999db3ebc4ad4', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"[{'run_id': '28f01e38b7f04d2f948fe21f57f41d0c', 'param_dataset.title': 'Scikit-Learn Iris', 'param_columns_raw': \\\\\\\"['id', 'sepallengthcm', 'sepalwidthcm', 'petallengthcm', 'petalwidthcm', 'species']\\\\\\\", 'param_dropped_columns': \\\\\\\"['id']\\\\\\\", 'param_feature_names': \\\\\\\"['sepallengthcm', 'sepalwidthcm', 'petallengthcm', 'petalwidthcm']\\\\\\\", 'param_dataset.authors': '[\\\\\\\"Marshall Michael\\\\\\\"]', 'param_dataset.doi': '10.5281/ZENODO.1404173', 'param_dataset.published': '2018-8-27', 'param_test_size': '0.2', 'param_criterion': 'entropy', 'param_max_depth': '12', 'param_max_leaf_nodes': 'None', 'param_max_samples': 'None', 'metric_accuracy': 1.0, 'metric_f1_macro': 1.0, 'metric_roc_auc': 1.0}]\\\\n\\\",\\n+      \\\"[]\\\\n\\\",\\n+      \\\"[{'param_dropped_columns': \\\\\\\"['id']\\\\\\\", 'param_test_size': '0.2', 'param_feature_names': \\\\\\\"['sepallengthcm', 'sepalwidthcm', 'petallengthcm', 'petalwidthcm']\\\\\\\"}]\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"d931b602947d4db8872f254d48e22027\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 12:23:52 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '41261519e1a643c5b1335701aee1bf95', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"{'features': ['sepallengthcm', 'sepalwidthcm'], 'accuracy': 0.7666666666666667}\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"83ff1224205a4a8eb0c351a7f299dd93\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 12:23:58 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'e0955d231fa6488e9339086b5845064c', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"1eaa6c141e064593b73b6c72ce0b00cf\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 12:24:04 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '21d299b426ac42a0ad799604e9e7ff88', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"{'dropped_feature': 'petallengthcm', 'baseline_acc': 1.0, 'dropped_acc': 0.9666666666666667, 'impact': 0.033333333333333326}\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"4c04ce12f62f49a29f48509b1483f16b\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 12:24:10 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '8ca4591a1b53402f854187104d1e7ee0', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"66bf06c45648410daa144c12f85658c6\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 12:24:16 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '0c8a66e5e4b244f9a6a8e9fa02d26828', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"0d71b6d9b58d4e5a9db241baeaa79d53\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 12:24:22 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '53994143a51e481abd23e988be2466b1', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"3fa13db4a66940d59cf37a30cb7a3cbc\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stderr\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"2025/04/25 12:24:28 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '35588f1cd8c34ce28770848de714d3c4', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"data\\\": {\\n+      \\\"application/vnd.jupyter.widget-view+json\\\": {\\n+       \\\"model_id\\\": \\\"273c026f2e0b464f98090472792b3a87\\\",\\n+       \\\"version_major\\\": 2,\\n+       \\\"version_minor\\\": 0\\n+      },\\n+      \\\"text/plain\\\": [\\n+       \\\"Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]\\\"\\n+      ]\\n+     },\\n+     \\\"metadata\\\": {},\\n+     \\\"output_type\\\": \\\"display_data\\\"\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"[{'dropped_feature': 'sepallengthcm', 'baseline_acc': 1.0, 'dropped_acc': 1.0, 'impact': 0.0}, {'dropped_feature': 'sepalwidthcm', 'baseline_acc': 1.0, 'dropped_acc': 1.0, 'impact': 0.0}, {'dropped_feature': 'petallengthcm', 'baseline_acc': 1.0, 'dropped_acc': 0.9666666666666667, 'impact': 0.0333}, {'dropped_feature': 'petalwidthcm', 'baseline_acc': 1.0, 'dropped_acc': 0.9666666666666667, 'impact': 0.0333}]\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Helper to get the \\u201cofficial\\u201d feature_names from your summary DF\\\\n\\\",\\n+    \\\"def _get_all_features(df):\\\\n\\\",\\n+    \\\"    # assumes every row has the same param_feature_names\\\\n\\\",\\n+    \\\"    raw = df.loc[0, 'param_feature_names']\\\\n\\\",\\n+    \\\"    return ast.literal_eval(raw)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Train & eval RF on just these columns of Iris\\\\n\\\",\\n+    \\\"def evaluate_subset(features, test_size=0.2, random_state=42, n_estimators=200):\\\\n\\\",\\n+    \\\"    iris = load_iris()\\\\n\\\",\\n+    \\\"    X = pd.DataFrame(iris.data, columns=iris.feature_names)\\\\n\\\",\\n+    \\\"    # map sklearn\\u2019s names to your param names, e.g. \\\\\\\"sepal length (cm)\\\\\\\" \\u2192 \\\\\\\"sepallengthcm\\\\\\\"\\\\n\\\",\\n+    \\\"    canon = _get_all_features(df)\\\\n\\\",\\n+    \\\"    mapping = dict(zip(iris.feature_names, canon))\\\\n\\\",\\n+    \\\"    X = X.rename(columns=mapping)\\\\n\\\",\\n+    \\\"    X_sub = X[features]\\\\n\\\",\\n+    \\\"    y = iris.target\\\\n\\\",\\n+    \\\"    Xtr, Xte, ytr, yte = train_test_split(X_sub, y, test_size=test_size, random_state=random_state)\\\\n\\\",\\n+    \\\"    m = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\\\\n\\\",\\n+    \\\"    m.fit(Xtr, ytr)\\\\n\\\",\\n+    \\\"    return accuracy_score(yte, m.predict(Xte))\\\\n\\\",\\n+    \\\"def trace_preprocessing(df, run_id=None):\\\\n\\\",\\n+    \\\"    cols = ['run_id',\\\\n\\\",\\n+    \\\"            'param_dataset.title',\\\\n\\\",\\n+    \\\"            'param_columns_raw',\\\\n\\\",\\n+    \\\"            'param_dropped_columns',\\\\n\\\",\\n+    \\\"            'param_feature_names',\\\\n\\\",\\n+    \\\"            'param_dataset.authors', 'param_dataset.doi', 'param_dataset.published',\\\\n\\\",\\n+    \\\"            'param_test_size',\\\\n\\\",\\n+    \\\"            'param_criterion',\\\\n\\\",\\n+    \\\"            'param_max_depth','param_max_leaf_nodes', 'param_max_samples',\\\\n\\\",\\n+    \\\"           'metric_accuracy','metric_f1_macro','metric_roc_auc']\\\\n\\\",\\n+    \\\"    if run_id is None:\\\\n\\\",\\n+    \\\"        subset = df.loc[:, cols]\\\\n\\\",\\n+    \\\"    else:\\\\n\\\",\\n+    \\\"        subset = df.loc[df['run_id'] == run_id, cols]\\\\n\\\",\\n+    \\\"    return subset.to_dict(orient='records')\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def drop_impact(df, feature, **_):\\\\n\\\",\\n+    \\\"    all_feats = _get_all_features(df)\\\\n\\\",\\n+    \\\"    baseline = evaluate_subset(all_feats)\\\\n\\\",\\n+    \\\"    without = [f for f in all_feats if f!=feature]\\\\n\\\",\\n+    \\\"    dropped = evaluate_subset(without)\\\\n\\\",\\n+    \\\"    return {\\\\n\\\",\\n+    \\\"      'dropped_feature': feature,\\\\n\\\",\\n+    \\\"      'baseline_acc': baseline,\\\\n\\\",\\n+    \\\"      'dropped_acc': dropped,\\\\n\\\",\\n+    \\\"      'impact': baseline - dropped\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def drop_impact_all(df: pd.DataFrame) -> List[Dict[str, Any]]:\\\\n\\\",\\n+    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    Compute drop-impact for every feature in the dataset.\\\\n\\\",\\n+    \\\"    Returns list of dicts with dropped_feature, baseline_acc, dropped_acc, impact.\\\\n\\\",\\n+    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    feats = _get_all_features(df)\\\\n\\\",\\n+    \\\"    baseline = evaluate_subset(feats)\\\\n\\\",\\n+    \\\"    summary = []\\\\n\\\",\\n+    \\\"    for feat in feats:\\\\n\\\",\\n+    \\\"        without = [f for f in feats if f != feat]\\\\n\\\",\\n+    \\\"        acc = evaluate_subset(without)\\\\n\\\",\\n+    \\\"        summary.append({\\\\n\\\",\\n+    \\\"            'dropped_feature': feat,\\\\n\\\",\\n+    \\\"            'baseline_acc': baseline,\\\\n\\\",\\n+    \\\"            'dropped_acc': acc,\\\\n\\\",\\n+    \\\"            'impact': round(baseline - acc, 4)\\\\n\\\",\\n+    \\\"        })\\\\n\\\",\\n+    \\\"    return summary\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def best_feature_subset(df, features, **_):\\\\n\\\",\\n+    \\\"    acc = evaluate_subset(features)\\\\n\\\",\\n+    \\\"    return {'features': features, 'accuracy': acc}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def common_high_accuracy(df: pd.DataFrame, threshold: float = 0.95) -> List[Dict[str, Any]]:\\\\n\\\",\\n+    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    Filter runs with test accuracy >= threshold and list unique shared preprocessing settings.\\\\n\\\",\\n+    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    high = df[df['metric_accuracy_score_X_test'] >= threshold]\\\\n\\\",\\n+    \\\"    cols = ['param_dropped_columns', 'param_test_size', 'param_feature_names']\\\\n\\\",\\n+    \\\"    return high[cols].drop_duplicates().to_dict(orient='records')\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# --------------------------------------------\\\\n\\\",\\n+    \\\"# Use Case Registry with parameter order for minimal input\\\\n\\\",\\n+    \\\"# --------------------------------------------\\\\n\\\",\\n+    \\\"USE_CASES = {\\\\n\\\",\\n+    \\\"    'trace_preprocessing': {\\\\n\\\",\\n+    \\\"        'func': trace_preprocessing,\\\\n\\\",\\n+    \\\"        'required_params': [],            # none strictly required\\\\n\\\",\\n+    \\\"        'optional_params': ['run_id'],    # run_id can be supplied or not\\\\n\\\",\\n+    \\\"    },\\\\n\\\",\\n+    \\\"    'drop_impact': {\\\\n\\\",\\n+    \\\"        'func': drop_impact,\\\\n\\\",\\n+    \\\"        'required_params': ['feature'],\\\\n\\\",\\n+    \\\"        'optional_params': [],\\\\n\\\",\\n+    \\\"    },\\\\n\\\",\\n+    \\\"     'drop_impact_all': {\\\\n\\\",\\n+    \\\"        'func': drop_impact_all,\\\\n\\\",\\n+    \\\"        'required_params': [],\\\\n\\\",\\n+    \\\"        'optional_params': [],\\\\n\\\",\\n+    \\\"    },\\\\n\\\",\\n+    \\\"    'best_feature_subset': {\\\\n\\\",\\n+    \\\"        'func': best_feature_subset,\\\\n\\\",\\n+    \\\"        'required_params': ['features'],\\\\n\\\",\\n+    \\\"        'optional_params': [],\\\\n\\\",\\n+    \\\"    },\\\\n\\\",\\n+    \\\"    'common_high_accuracy': {\\\\n\\\",\\n+    \\\"        'func': common_high_accuracy,\\\\n\\\",\\n+    \\\"        'required_params': ['threshold'],\\\\n\\\",\\n+    \\\"        'optional_params': [],\\\\n\\\",\\n+    \\\"    },\\\\n\\\",\\n+    \\\"}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def call_use_case(df, use_case_name, **kwargs):\\\\n\\\",\\n+    \\\"    if use_case_name not in USE_CASES:\\\\n\\\",\\n+    \\\"        raise ValueError(f\\\\\\\"Unknown use case: {use_case_name}\\\\\\\")\\\\n\\\",\\n+    \\\"    case = USE_CASES[use_case_name]\\\\n\\\",\\n+    \\\"    func = case['func']\\\\n\\\",\\n+    \\\"    # check required\\\\n\\\",\\n+    \\\"    missing = [p for p in case['required_params'] if p not in kwargs]\\\\n\\\",\\n+    \\\"    if missing:\\\\n\\\",\\n+    \\\"        raise ValueError(f\\\\\\\"{use_case_name} missing required params: {missing}\\\\\\\")\\\\n\\\",\\n+    \\\"    # build args\\\\n\\\",\\n+    \\\"    args = {p: kwargs[p] for p in case['required_params']}\\\\n\\\",\\n+    \\\"    for p in case['optional_params']:\\\\n\\\",\\n+    \\\"        args[p] = kwargs.get(p)\\\\n\\\",\\n+    \\\"    return func(df, **args)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# --------------------------------------------\\\\n\\\",\\n+    \\\"# Example Usage\\\\n\\\",\\n+    \\\"# --------------------------------------------\\\\n\\\",\\n+    \\\"if __name__ == '__main__':\\\\n\\\",\\n+    \\\"   # # 1) trace_preprocessing for all runs\\\\n\\\",\\n+    \\\"    print(call_use_case(df, 'trace_preprocessing'))\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # 2) trace_preprocessing for a single run_id\\\\n\\\",\\n+    \\\"    print(call_use_case(df, 'trace_preprocessing', run_id='361daa12f99f4129a06cd20b78dd6fa7'))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 5) common_high_accuracy\\\\n\\\",\\n+    \\\"    print(call_use_case(df, 'common_high_accuracy', threshold=0.99))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 4) Best\\u2010subset on just sepals:\\\\n\\\",\\n+    \\\"    print(call_use_case(df, 'best_feature_subset', features=['sepallengthcm','sepalwidthcm']))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 3) Drop\\u2010impact for \\u201cpetallengthcm\\u201d:\\\\n\\\",\\n+    \\\"    print(call_use_case(df, 'drop_impact', feature='petallengthcm'))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    print(call_use_case(df, 'drop_impact_all'))  # summary for all features\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"96f912d6-0e84-4155-858a-9668bef63f6e\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\" \\u2022 Detecting models trained with deprecated code versions\\\\n\\\",\\n+    \\\" \\u2022 Mapping models to specific datasets used during training\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 74,\\n+   \\\"id\\\": \\\"34a02c9a-5459-478f-a3c5-7f7a58ff22b0\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"[{'param_dataset.doi': '10.5281/ZENODO.1404173',\\\\n\\\",\\n+      \\\"  'param_dataset.published': '2018-8-27',\\\\n\\\",\\n+      \\\"  'param_dataset.publisher': 'Zenodo',\\\\n\\\",\\n+      \\\"  'param_dataset.title': 'Scikit-Learn Iris',\\\\n\\\",\\n+      \\\"  'run_id': '28f01e38b7f04d2f948fe21f57f41d0c',\\\\n\\\",\\n+      \\\"  'tag_model_name': 'RandomForest_Iris_v20250425_121328'}]\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"def detect_deprecated_code(df: pd.DataFrame, deprecated_commits: List[str], **_) -> List[Dict[str, Any]]:\\\\n\\\",\\n+    \\\"    # we know the column is called tag_git_current_commit_hash\\\\n\\\",\\n+    \\\"    commit_col = 'tag_git_current_commit_hash'\\\\n\\\",\\n+    \\\"    if commit_col not in df.columns:\\\\n\\\",\\n+    \\\"        raise KeyError(f\\\\\\\"Missing {commit_col} in DataFrame\\\\\\\")\\\\n\\\",\\n+    \\\"    out = df[df[commit_col].isin(deprecated_commits)]\\\\n\\\",\\n+    \\\"    # include run_id and notebook/runName for context\\\\n\\\",\\n+    \\\"    cols = ['run_id', commit_col, 'tag_notebook_name', 'tag_mlflow.runName']\\\\n\\\",\\n+    \\\"    # drop any that don\\u2019t exist\\\\n\\\",\\n+    \\\"    cols = [c for c in cols if c in df.columns]\\\\n\\\",\\n+    \\\"    return out[cols].to_dict(orient='records')\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"def map_model_dataset(df: pd.DataFrame, **_) -> List[Dict[str, Any]]:\\\\n\\\",\\n+    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    For each run, return its model name (or run_id) alongside the dataset\\\\n\\\",\\n+    \\\"    title, DOI, published date and publisher.\\\\n\\\",\\n+    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n+    \\\"    # pick whichever model-name column you have\\\\n\\\",\\n+    \\\"    model_col = 'tag_model_name' if 'tag_model_name' in df.columns else 'param_model_name'\\\\n\\\",\\n+    \\\"    cols = [\\\\n\\\",\\n+    \\\"        'run_id',\\\\n\\\",\\n+    \\\"        model_col,\\\\n\\\",\\n+    \\\"        'param_dataset.title',\\\\n\\\",\\n+    \\\"        'param_dataset.doi',\\\\n\\\",\\n+    \\\"        'param_dataset.published',\\\\n\\\",\\n+    \\\"        'param_dataset.publisher'\\\\n\\\",\\n+    \\\"    ]\\\\n\\\",\\n+    \\\"    # filter out any columns that don\\u2019t actually exist\\\\n\\\",\\n+    \\\"    cols = [c for c in cols if c in df.columns]\\\\n\\\",\\n+    \\\"    return df[cols].to_dict(orient='records')\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# --------------------------------------------\\\\n\\\",\\n+    \\\"# Extend Use-Case Registry\\\\n\\\",\\n+    \\\"# --------------------------------------------\\\\n\\\",\\n+    \\\"USE_CASES.update({\\\\n\\\",\\n+    \\\"    'detect_deprecated_code': {\\\\n\\\",\\n+    \\\"        'func': detect_deprecated_code,\\\\n\\\",\\n+    \\\"        'required_params': ['deprecated_commits'],\\\\n\\\",\\n+    \\\"        'optional_params': []\\\\n\\\",\\n+    \\\"    },\\\\n\\\",\\n+    \\\"    'map_model_dataset': {\\\\n\\\",\\n+    \\\"        'func': map_model_dataset,\\\\n\\\",\\n+    \\\"        'required_params': [],\\\\n\\\",\\n+    \\\"        'optional_params': []\\\\n\\\",\\n+    \\\"    },\\\\n\\\",\\n+    \\\"})\\\\n\\\",\\n+    \\\"# 1) Detect runs on deprecated commits:\\\\n\\\",\\n+    \\\"deprecated = [\\\\n\\\",\\n+    \\\"    \\\\\\\"a07434af4f547af2daab044d6873eb7081162293\\\\\\\",\\\\n\\\",\\n+    \\\"    \\\\\\\"d329c92495e196ec0f39fbb19dfdd367131a77d9\\\\\\\"\\\\n\\\",\\n+    \\\"]\\\\n\\\",\\n+    \\\"# print(call_use_case(df, \\\\\\\"detect_deprecated_code\\\\\\\", deprecated_commits=deprecated))\\\\n\\\",\\n+    \\\"pprint(call_use_case(df, 'map_model_dataset'))\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"c52607ad-5849-4a2d-97ef-e8fc1ca16dc7\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"Goal: Notify collaborators who have forked the GitHub repo if their fork is outdated (i.e., behind the current commit used to train a model).\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"f29c8ad9-00bb-4c1e-ac3b-ee6861991acd\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"\\ud83e\\udde0 What We Need\\\\n\\\",\\n+    \\\"Current training run\\u2019s Git commit hash\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"GitHub API to fetch all forks of your repo\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"Compare each fork\\u2019s main or master branch head commit\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"Create an issue on their fork or on your repo tagging them if they\\u2019re behind\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"c72bed50-fb56-442d-a21e-bb7991892d07\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\": Notify via issues on your own repo\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": 75,\\n+   \\\"id\\\": \\\"852f147c-9d0a-4d7f-a4ab-545d1e2375fb\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [\\n+    {\\n+     \\\"name\\\": \\\"stdin\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"Do you want to notify collaborators whose forks are behind? (y/N):  N\\\\n\\\"\\n+     ]\\n+    },\\n+    {\\n+     \\\"name\\\": \\\"stdout\\\",\\n+     \\\"output_type\\\": \\\"stream\\\",\\n+     \\\"text\\\": [\\n+      \\\"No action taken.\\\\n\\\"\\n+     ]\\n+    }\\n+   ],\\n+   \\\"source\\\": [\\n+    \\\"def notify_outdated_forks():\\\\n\\\",\\n+    \\\"    load_dotenv()\\\\n\\\",\\n+    \\\"    token     = os.getenv(\\\\\\\"THESIS_TOKEN\\\\\\\")\\\\n\\\",\\n+    \\\"    owner     = \\\\\\\"reema-dass26\\\\\\\"\\\\n\\\",\\n+    \\\"    repo      = \\\\\\\"REPO\\\\\\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    if not token:\\\\n\\\",\\n+    \\\"        print(\\\\\\\"\\u26a0\\ufe0f GITHUB_TOKEN not set.\\\\\\\")\\\\n\\\",\\n+    \\\"        return\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    headers = {\\\\n\\\",\\n+    \\\"        \\\\\\\"Authorization\\\\\\\": f\\\\\\\"token {token}\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"Accept\\\\\\\":        \\\\\\\"application/vnd.github.v3+json\\\\\\\"\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 1) Get latest upstream commit\\\\n\\\",\\n+    \\\"    main_commits = requests.get(\\\\n\\\",\\n+    \\\"        f\\\\\\\"https://api.github.com/repos/{owner}/{repo}/commits\\\\\\\",\\\\n\\\",\\n+    \\\"        headers=headers,\\\\n\\\",\\n+    \\\"        params={\\\\\\\"per_page\\\\\\\": 1}\\\\n\\\",\\n+    \\\"    )\\\\n\\\",\\n+    \\\"    main_commits.raise_for_status()\\\\n\\\",\\n+    \\\"    new_commit_hash = main_commits.json()[0][\\\\\\\"sha\\\\\\\"]\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"Latest upstream commit: {new_commit_hash}\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 2) List forks\\\\n\\\",\\n+    \\\"    forks_resp = requests.get(f\\\\\\\"https://api.github.com/repos/{owner}/{repo}/forks\\\\\\\", headers=headers)\\\\n\\\",\\n+    \\\"    forks_resp.raise_for_status()\\\\n\\\",\\n+    \\\"    forks = forks_resp.json()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 3) Compare each fork\\\\n\\\",\\n+    \\\"    outdated = []\\\\n\\\",\\n+    \\\"    for fork in forks:\\\\n\\\",\\n+    \\\"        fork_owner = fork[\\\\\\\"owner\\\\\\\"][\\\\\\\"login\\\\\\\"]\\\\n\\\",\\n+    \\\"        fork_comm = requests.get(\\\\n\\\",\\n+    \\\"            fork[\\\\\\\"url\\\\\\\"] + \\\\\\\"/commits\\\\\\\",\\\\n\\\",\\n+    \\\"            headers=headers,\\\\n\\\",\\n+    \\\"            params={\\\\\\\"per_page\\\\\\\": 1}\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        if fork_comm.status_code != 200:\\\\n\\\",\\n+    \\\"            print(f\\\\\\\"\\u00a0\\u00a0\\u2013 could not fetch commits for {fork_owner}, skipping.\\\\\\\")\\\\n\\\",\\n+    \\\"            continue\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"        fork_sha = fork_comm.json()[0][\\\\\\\"sha\\\\\\\"]\\\\n\\\",\\n+    \\\"        if fork_sha != new_commit_hash:\\\\n\\\",\\n+    \\\"            outdated.append(f\\\\\\\"@{fork_owner}\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 4) Open an issue if any are behind\\\\n\\\",\\n+    \\\"    if outdated:\\\\n\\\",\\n+    \\\"        title = \\\\\\\"\\ud83d\\udd14 Notification: Your fork is behind the latest commit\\\\\\\"\\\\n\\\",\\n+    \\\"        body  = (\\\\n\\\",\\n+    \\\"            f\\\\\\\"Hi {' '.join(outdated)},\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n\\\",\\n+    \\\"            f\\\\\\\"The main repository has been updated to commit `{new_commit_hash}`.\\\\\\\\n\\\\\\\"\\\\n\\\",\\n+    \\\"            \\\\\\\"Please consider pulling the latest changes to stay in sync.\\\\\\\\n\\\\\\\\n\\\\\\\"\\\\n\\\",\\n+    \\\"            \\\\\\\"Thanks!\\\\\\\"\\\\n\\\",\\n+    \\\"        )\\\\n\\\",\\n+    \\\"        issues_url = f\\\\\\\"https://api.github.com/repos/{owner}/{repo}/issues\\\\\\\"\\\\n\\\",\\n+    \\\"        resp = requests.post(\\\\n\\\",\\n+    \\\"        issues_url,\\\\n\\\",\\n+    \\\"        headers=headers,\\\\n\\\",\\n+    \\\"        json={\\\\\\\"title\\\\\\\": title, \\\\\\\"body\\\\\\\": body}\\\\n\\\",\\n+    \\\"    )\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # DEBUGGING OUTPUT\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"\\u2192 POST {issues_url}\\\\\\\")\\\\n\\\",\\n+    \\\"    print(\\\\\\\"\\u2192 Status code:\\\\\\\", resp.status_code)\\\\n\\\",\\n+    \\\"    print(\\\\\\\"\\u2192 Response headers:\\\\\\\", resp.headers)\\\\n\\\",\\n+    \\\"    try:\\\\n\\\",\\n+    \\\"        data = resp.json()\\\\n\\\",\\n+    \\\"        print(\\\\\\\"\\u2192 Response JSON:\\\\\\\", data)\\\\n\\\",\\n+    \\\"        print(\\\\\\\"\\u2192 html_url field:\\\\\\\", data.get(\\\\\\\"html_url\\\\\\\"))\\\\n\\\",\\n+    \\\"    except ValueError:\\\\n\\\",\\n+    \\\"        print(\\\\\\\"\\u2192 No JSON response body; raw text:\\\\\\\", resp.text)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"if __name__ == \\\\\\\"__main__\\\\\\\":\\\\n\\\",\\n+    \\\"    answer = input(\\\\\\\"Do you want to notify collaborators whose forks are behind? (y/N): \\\\\\\").strip().lower()\\\\n\\\",\\n+    \\\"    if answer in (\\\\\\\"y\\\\\\\", \\\\\\\"yes\\\\\\\"):\\\\n\\\",\\n+    \\\"        notify_outdated_forks()\\\\n\\\",\\n+    \\\"    else:\\\\n\\\",\\n+    \\\"        print(\\\\\\\"No action taken.\\\\\\\")\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"cda31f16-fbe9-40ce-ac1b-9ebc898c8820\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"INVENIO INTEGRETION to upload the necessary files and publish\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"c2e5a7fc-3b03-45c8-bc90-817ea5ba7352\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"############################################################################################\\\\n\\\",\\n+    \\\"# TEST CODE FOR INVENIO INTEGRETION\\\\n\\\",\\n+    \\\"#############################################################################################\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# API_BASE = \\\\\\\"https://127.0.0.1:5000\\\\\\\"\\\\n\\\",\\n+    \\\"# TOKEN    = \\\\\\\"8LnqJuz3TsBHffnDJ3isPLHYHtRbWrC0M667Nb5haEbnXpWqGbFRyfDApymr\\\\\\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# # 1) Test read\\u2010scope by listing records (no size param or size=1)\\\\n\\\",\\n+    \\\"# resp = requests.get(\\\\n\\\",\\n+    \\\"#     f\\\\\\\"{API_BASE}/api/records\\\\\\\",\\\\n\\\",\\n+    \\\"#     headers={\\\\\\\"Authorization\\\\\\\": f\\\\\\\"Bearer {TOKEN}\\\\\\\"},\\\\n\\\",\\n+    \\\"#     verify=False\\\\n\\\",\\n+    \\\"# )\\\\n\\\",\\n+    \\\"# print(resp.status_code)\\\\n\\\",\\n+    \\\"# # should be 200 and a JSON page of records\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# # or explicitly:\\\\n\\\",\\n+    \\\"# resp = requests.get(\\\\n\\\",\\n+    \\\"#     f\\\\\\\"{API_BASE}/api/records?size=1\\\\\\\",\\\\n\\\",\\n+    \\\"#     headers={\\\\\\\"Authorization\\\\\\\": f\\\\\\\"Bearer {TOKEN}\\\\\\\"},\\\\n\\\",\\n+    \\\"#     verify=False\\\\n\\\",\\n+    \\\"# )\\\\n\\\",\\n+    \\\"# print(resp.status_code, resp.json())\\\\n\\\",\\n+    \\\"# #################################################################################################\\\\n\\\",\\n+    \\\"# API_BASE = \\\\\\\"https://127.0.0.1:5000\\\\\\\"\\\\n\\\",\\n+    \\\"# TOKEN    = \\\\\\\"8LnqJuz3TsBHffnDJ3isPLHYHtRbWrC0M667Nb5haEbnXpWqGbFRyfDApymr\\\\\\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# resp = requests.options(\\\\n\\\",\\n+    \\\"#     f\\\\\\\"{API_BASE}/api/records\\\\\\\",\\\\n\\\",\\n+    \\\"#     headers={\\\\\\\"Authorization\\\\\\\": f\\\\\\\"Bearer {TOKEN}\\\\\\\"},\\\\n\\\",\\n+    \\\"#     verify=False\\\\n\\\",\\n+    \\\"# )\\\\n\\\",\\n+    \\\"# print(\\\\\\\"Allowed methods:\\\\\\\", resp.headers.get(\\\\\\\"Allow\\\\\\\"))\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"7e5b2cc5-ecf3-4e13-8cac-47f57f12cbdd\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"# Configuration\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"API_BASE   = \\\\\\\"https://127.0.0.1:5000\\\\\\\"\\\\n\\\",\\n+    \\\"TOKEN      = \\\\\\\"8LnqJuz3TsBHffnDJ3isPLHYHtRbWrC0M667Nb5haEbnXpWqGbFRyfDApymr\\\\\\\"\\\\n\\\",\\n+    \\\"VERIFY_SSL = False  # only for self\\u2010signed dev\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"HEADERS_JSON = {\\\\n\\\",\\n+    \\\"    \\\\\\\"Accept\\\\\\\":        \\\\\\\"application/json\\\\\\\",\\\\n\\\",\\n+    \\\"    \\\\\\\"Content-Type\\\\\\\":  \\\\\\\"application/json\\\\\\\",\\\\n\\\",\\n+    \\\"    \\\\\\\"Authorization\\\\\\\": f\\\\\\\"Bearer {TOKEN}\\\\\\\",\\\\n\\\",\\n+    \\\"}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"HEADERS_OCTET = {\\\\n\\\",\\n+    \\\"    \\\\\\\"Content-Type\\\\\\\":  \\\\\\\"application/octet-stream\\\\\\\",\\\\n\\\",\\n+    \\\"    \\\\\\\"Authorization\\\\\\\": f\\\\\\\"Bearer {TOKEN}\\\\\\\",\\\\n\\\",\\n+    \\\"}\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# The folders you want to walk & upload:\\\\n\\\",\\n+    \\\"TO_UPLOAD = [\\\\\\\"Trained_models\\\\\\\", \\\\\\\"plots\\\\\\\", \\\\\\\"MODEL_PROVENANCE\\\\\\\"]\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"# 1) Create draft with ALL required metadata\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"def create_draft():\\\\n\\\",\\n+    \\\"    payload = {\\\\n\\\",\\n+    \\\"  \\\\\\\"metadata\\\\\\\": {\\\\n\\\",\\n+    \\\"    \\\\\\\"title\\\\\\\":            \\\\\\\"RandomForest Iris Model Artifacts\\\\\\\",\\\\n\\\",\\n+    \\\"    \\\\\\\"creators\\\\\\\": [ {\\\\n\\\",\\n+    \\\"      \\\\\\\"person_or_org\\\\\\\": {\\\\n\\\",\\n+    \\\"        \\\\\\\"type\\\\\\\":        \\\\\\\"personal\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"given_name\\\\\\\":  \\\\\\\"Reema\\\\\\\",\\\\n\\\",\\n+    \\\"        \\\\\\\"family_name\\\\\\\": \\\\\\\"Dass\\\\\\\"\\\\n\\\",\\n+    \\\"      }\\\\n\\\",\\n+    \\\"    } ],\\\\n\\\",\\n+    \\\"    \\\\\\\"publication_date\\\\\\\": \\\\\\\"2025-04-24\\\\\\\",\\\\n\\\",\\n+    \\\"    \\\\\\\"resource_type\\\\\\\":    { \\\\\\\"id\\\\\\\": \\\\\\\"software\\\\\\\" },\\\\n\\\",\\n+    \\\"    \\\\\\\"access\\\\\\\": {\\\\n\\\",\\n+    \\\"      \\\\\\\"record\\\\\\\": \\\\\\\"public\\\\\\\",\\\\n\\\",\\n+    \\\"      \\\\\\\"files\\\\\\\":  \\\\\\\"public\\\\\\\"\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"  }\\\\n\\\",\\n+    \\\"}\\\\n\\\",\\n+    \\\"    r = requests.post(f\\\\\\\"{API_BASE}/api/records\\\\\\\",\\\\n\\\",\\n+    \\\"                      headers=HEADERS_JSON,\\\\n\\\",\\n+    \\\"                      json=payload,\\\\n\\\",\\n+    \\\"                      verify=VERIFY_SSL)\\\\n\\\",\\n+    \\\"    r.raise_for_status()\\\\n\\\",\\n+    \\\"    draft = r.json()\\\\n\\\",\\n+    \\\"    print(\\\\\\\"\\u2705 Draft created:\\\\\\\", draft[\\\\\\\"id\\\\\\\"])\\\\n\\\",\\n+    \\\"    return draft[\\\\\\\"id\\\\\\\"], draft[\\\\\\\"links\\\\\\\"]\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"# 2) Register, upload and commit a single file\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"def upload_and_commit(links, key, path):\\\\n\\\",\\n+    \\\"    # 2a) register the filename in the draft\\\\n\\\",\\n+    \\\"    r1 = requests.post(links[\\\\\\\"files\\\\\\\"],\\\\n\\\",\\n+    \\\"                       headers=HEADERS_JSON,\\\\n\\\",\\n+    \\\"                       json=[{\\\\\\\"key\\\\\\\": key}],\\\\n\\\",\\n+    \\\"                       verify=VERIFY_SSL)\\\\n\\\",\\n+    \\\"    r1.raise_for_status()\\\\n\\\",\\n+    \\\"    entry = next(e for e in r1.json()[\\\\\\\"entries\\\\\\\"] if e[\\\\\\\"key\\\\\\\"] == key)\\\\n\\\",\\n+    \\\"    file_links = entry[\\\\\\\"links\\\\\\\"]\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 2b) upload the bytes\\\\n\\\",\\n+    \\\"    with open(path, \\\\\\\"rb\\\\\\\") as fp:\\\\n\\\",\\n+    \\\"        r2 = requests.put(file_links[\\\\\\\"content\\\\\\\"],\\\\n\\\",\\n+    \\\"                          headers=HEADERS_OCTET,\\\\n\\\",\\n+    \\\"                          data=fp,\\\\n\\\",\\n+    \\\"                          verify=VERIFY_SSL)\\\\n\\\",\\n+    \\\"    r2.raise_for_status()\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # 2c) commit the upload\\\\n\\\",\\n+    \\\"    r3 = requests.post(file_links[\\\\\\\"commit\\\\\\\"],\\\\n\\\",\\n+    \\\"                       headers=HEADERS_JSON,\\\\n\\\",\\n+    \\\"                       verify=VERIFY_SSL)\\\\n\\\",\\n+    \\\"    r3.raise_for_status()\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"  \\u2022 Uploaded {key}\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"# 3) Walk each folder and upload every file\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"def upload_folder(links):\\\\n\\\",\\n+    \\\"    for folder in TO_UPLOAD:\\\\n\\\",\\n+    \\\"        if not os.path.isdir(folder):\\\\n\\\",\\n+    \\\"            print(f\\\\\\\"\\u26a0\\ufe0f Skipping missing folder {folder}\\\\\\\")\\\\n\\\",\\n+    \\\"            continue\\\\n\\\",\\n+    \\\"        base = os.path.dirname(folder) or folder\\\\n\\\",\\n+    \\\"        for root, _, files in os.walk(folder):\\\\n\\\",\\n+    \\\"            for fn in files:\\\\n\\\",\\n+    \\\"                local = os.path.join(root, fn)\\\\n\\\",\\n+    \\\"                # create a POSIX\\u2010style key preserving subfolders\\\\n\\\",\\n+    \\\"                key = os.path.relpath(local, start=base).replace(os.sep, \\\\\\\"/\\\\\\\")\\\\n\\\",\\n+    \\\"                upload_and_commit(links, key, local)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"# 4) Publish the draft\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"def publish(links):\\\\n\\\",\\n+    \\\"    r = requests.post(links[\\\\\\\"publish\\\\\\\"],\\\\n\\\",\\n+    \\\"                      headers=HEADERS_JSON,\\\\n\\\",\\n+    \\\"                      verify=VERIFY_SSL)\\\\n\\\",\\n+    \\\"    if not r.ok:\\\\n\\\",\\n+    \\\"        print(\\\\\\\"\\u274c Publish failed:\\\\\\\", r.status_code, r.text)\\\\n\\\",\\n+    \\\"        try: print(r.json())\\\\n\\\",\\n+    \\\"        except: pass\\\\n\\\",\\n+    \\\"        r.raise_for_status()\\\\n\\\",\\n+    \\\"    print(\\\\\\\"\\u2705 Published:\\\\\\\", r.json()[\\\\\\\"id\\\\\\\"])\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"# 5) Fetch metadata and save to a file\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"def fetch_metadata(record_id):\\\\n\\\",\\n+    \\\"    r = requests.get(f\\\\\\\"{API_BASE}/api/records/{record_id}\\\\\\\",\\\\n\\\",\\n+    \\\"                     headers=HEADERS_JSON,\\\\n\\\",\\n+    \\\"                     verify=VERIFY_SSL)\\\\n\\\",\\n+    \\\"    r.raise_for_status()\\\\n\\\",\\n+    \\\"    metadata = r.json()\\\\n\\\",\\n+    \\\"    print(\\\\\\\"\\u2705 Metadata fetched successfully\\\\\\\")\\\\n\\\",\\n+    \\\"    \\\\n\\\",\\n+    \\\"    # Save the metadata to a file\\\\n\\\",\\n+    \\\"    with open(f\\\\\\\"metadata_{record_id}.json\\\\\\\", \\\\\\\"w\\\\\\\") as f:\\\\n\\\",\\n+    \\\"        json.dump(metadata, f, indent=4)\\\\n\\\",\\n+    \\\"    print(f\\\\\\\"\\u2705 Metadata saved as metadata_{record_id}.json\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"# Main\\\\n\\\",\\n+    \\\"# -----------------------------------------------------------------------------\\\\n\\\",\\n+    \\\"if __name__ == \\\\\\\"__main__\\\\\\\":\\\\n\\\",\\n+    \\\"    recid, links = create_draft()\\\\n\\\",\\n+    \\\"    upload_folder(links)\\\\n\\\",\\n+    \\\"    publish(links)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # Fetch and save metadata after publishing\\\\n\\\",\\n+    \\\"    print(fetch_metadata(recid))\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"markdown\\\",\\n+   \\\"id\\\": \\\"2f7423f2-0ff3-4104-913e-50eeb32d9d0f\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"source\\\": [\\n+    \\\"METADATA EXTRACTION FROM INVENIO:\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"0013878b-37da-4a22-9586-3773531bfd01\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"\\\\n\\\",\\n+    \\\"# Function to dynamically extract and structure metadata from the original JSON\\\\n\\\",\\n+    \\\"def extract_metadata(metadata):\\\\n\\\",\\n+    \\\"    # Debug: Check if metadata is loaded correctly\\\\n\\\",\\n+    \\\"    print(\\\\\\\"Debug: Metadata loaded successfully\\\\\\\")\\\\n\\\",\\n+    \\\"    print(metadata.get(\\\\\\\"id\\\\\\\", \\\\\\\"\\\\\\\"))  # Check if 'id' is being fetched\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # Check if the required fields are in the metadata\\\\n\\\",\\n+    \\\"    print(\\\\\\\"Debug: Extracting fields from metadata...\\\\\\\")\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    extracted_data = {\\\\n\\\",\\n+    \\\"        \\\\\\\"invenio_metadata\\\\\\\": {\\\\n\\\",\\n+    \\\"            \\\\\\\"id\\\\\\\": metadata.get(\\\\\\\"id\\\\\\\", \\\\\\\"\\\\\\\"),\\\\n\\\",\\n+    \\\"            \\\\\\\"title\\\\\\\": metadata.get(\\\\\\\"metadata\\\\\\\", {}).get(\\\\\\\"title\\\\\\\", \\\\\\\"\\\\\\\"),\\\\n\\\",\\n+    \\\"            \\\\\\\"creator\\\\\\\": \\\\\\\", \\\\\\\".join([creator[\\\\\\\"person_or_org\\\\\\\"].get(\\\\\\\"name\\\\\\\", \\\\\\\"\\\\\\\") for creator in metadata.get(\\\\\\\"metadata\\\\\\\", {}).get(\\\\\\\"creators\\\\\\\", [])]),\\\\n\\\",\\n+    \\\"            \\\\\\\"publication_date\\\\\\\": metadata.get(\\\\\\\"metadata\\\\\\\", {}).get(\\\\\\\"publication_date\\\\\\\", \\\\\\\"\\\\\\\"),\\\\n\\\",\\n+    \\\"            \\\\\\\"files\\\\\\\": [],  # Initialize 'files' as a list\\\\n\\\",\\n+    \\\"            \\\\\\\"pids\\\\\\\": metadata.get(\\\\\\\"pids\\\\\\\", {}),\\\\n\\\",\\n+    \\\"            \\\\\\\"version_info\\\\\\\": metadata.get(\\\\\\\"versions\\\\\\\", {}),\\\\n\\\",\\n+    \\\"            \\\\\\\"status\\\\\\\": metadata.get(\\\\\\\"status\\\\\\\", \\\\\\\"\\\\\\\"),\\\\n\\\",\\n+    \\\"            \\\\\\\"views\\\\\\\": metadata.get(\\\\\\\"stats\\\\\\\", {}).get(\\\\\\\"this_version\\\\\\\", {}).get(\\\\\\\"views\\\\\\\", 0),\\\\n\\\",\\n+    \\\"            \\\\\\\"downloads\\\\\\\": metadata.get(\\\\\\\"stats\\\\\\\", {}).get(\\\\\\\"this_version\\\\\\\", {}).get(\\\\\\\"downloads\\\\\\\", 0),\\\\n\\\",\\n+    \\\"        }\\\\n\\\",\\n+    \\\"    }\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    # Extract file details from the metadata\\\\n\\\",\\n+    \\\"    for key, file_info in metadata.get(\\\\\\\"files\\\\\\\", {}).get(\\\\\\\"entries\\\\\\\", {}).items():\\\\n\\\",\\n+    \\\"        file_detail = {\\\\n\\\",\\n+    \\\"            \\\\\\\"key\\\\\\\": key,\\\\n\\\",\\n+    \\\"            \\\\\\\"url\\\\\\\": file_info[\\\\\\\"links\\\\\\\"].get(\\\\\\\"content\\\\\\\", \\\\\\\"\\\\\\\"),\\\\n\\\",\\n+    \\\"            \\\\\\\"size\\\\\\\": file_info.get(\\\\\\\"size\\\\\\\", 0),\\\\n\\\",\\n+    \\\"            \\\\\\\"mimetype\\\\\\\": file_info.get(\\\\\\\"mimetype\\\\\\\", \\\\\\\"\\\\\\\"),\\\\n\\\",\\n+    \\\"            \\\\\\\"checksum\\\\\\\": file_info.get(\\\\\\\"checksum\\\\\\\", \\\\\\\"\\\\\\\"),\\\\n\\\",\\n+    \\\"            \\\\\\\"metadata\\\\\\\": file_info.get(\\\\\\\"metadata\\\\\\\", {}),\\\\n\\\",\\n+    \\\"        }\\\\n\\\",\\n+    \\\"        extracted_data[\\\\\\\"invenio_metadata\\\\\\\"][\\\\\\\"files\\\\\\\"].append(file_detail)  # Append to the 'files' list\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"    return extracted_data\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Load the original metadata from the JSON file (replace with your actual file path)\\\\n\\\",\\n+    \\\"with open('metadata_p8a8y-1bn93.json', 'r') as f: \\\\n\\\",\\n+    \\\"    original_metadata = json.load(f)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Debugging: print out the first part of the original metadata to verify its structure\\\\n\\\",\\n+    \\\"print(\\\\\\\"Debug: Original Metadata (start):\\\\\\\", json.dumps(original_metadata, indent=4)[:1000])  # Print only the start for review\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Extract relevant details dynamically\\\\n\\\",\\n+    \\\"extracted_metadata = extract_metadata(original_metadata)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Debugging: print the extracted metadata to verify it's correct\\\\n\\\",\\n+    \\\"print(\\\\\\\"Debug: Extracted Metadata:\\\\\\\", json.dumps(extracted_metadata, indent=4))\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Load the existing JSON file (replace with your actual file path)\\\\n\\\",\\n+    \\\"with open('MODEL_PROVENANCE/RandomForest_Iris_v20250424_111946_run_summary.json', 'r') as f:\\\\n\\\",\\n+    \\\"    existing_metadata = json.load(f)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Add the extracted metadata as a new node\\\\n\\\",\\n+    \\\"existing_metadata.update(extracted_metadata)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# Save the updated metadata back to the file\\\\n\\\",\\n+    \\\"with open('updated_metadata.json', 'w') as f:\\\\n\\\",\\n+    \\\"    json.dump(existing_metadata, f, indent=4)\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"print(\\\\\\\"\\u2705 New dynamic metadata added successfully!\\\\\\\")\\\\n\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"38a807a7-6ecd-4ea7-93ac-78c0f853825c\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": [\\n+    \\\"# import mlflow\\\\n\\\",\\n+    \\\"# import mlflow.sklearn\\\\n\\\",\\n+    \\\"# from sklearn.datasets import load_iris\\\\n\\\",\\n+    \\\"# from sklearn.ensemble import RandomForestClassifier\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"# X, y = load_iris(return_X_y=True)\\\\n\\\",\\n+    \\\"# mlflow.sklearn.autolog()\\\\n\\\",\\n+    \\\"# with mlflow.start_run() as run:\\\\n\\\",\\n+    \\\"\\\\n\\\",\\n+    \\\"#     model = RandomForestClassifier(**hyperparams)\\\\n\\\",\\n+    \\\"#     model.fit(X_train, y_train)\\\"\\n+   ]\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"570fa169-a5e2-47b3-b7f5-44f9577f22ad\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": []\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"f67b7a46-a70d-44ea-976c-322a1a795311\\\",\\n+   \\\"metadata\\\": {\\n+    \\\"scrolled\\\": true\\n+   },\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": []\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"4211bdef-5785-472d-8ea5-0bc24a3faf3c\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": []\\n+  },\\n+  {\\n+   \\\"cell_type\\\": \\\"code\\\",\\n+   \\\"execution_count\\\": null,\\n+   \\\"id\\\": \\\"9d4d71f2-ef66-4e04-9d9b-c4b381d45590\\\",\\n+   \\\"metadata\\\": {},\\n+   \\\"outputs\\\": [],\\n+   \\\"source\\\": []\\n+  }\\n+ ],\\n+ \\\"metadata\\\": {\\n+  \\\"kernelspec\\\": {\\n+   \\\"display_name\\\": \\\"Python 3 (ipykernel)\\\",\\n+   \\\"language\\\": \\\"python\\\",\\n+   \\\"name\\\": \\\"python3\\\"\\n+  },\\n+  \\\"language_info\\\": {\\n+   \\\"codemirror_mode\\\": {\\n+    \\\"name\\\": \\\"ipython\\\",\\n+    \\\"version\\\": 3\\n+   },\\n+   \\\"file_extension\\\": \\\".py\\\",\\n+   \\\"mimetype\\\": \\\"text/x-python\\\",\\n+   \\\"name\\\": \\\"python\\\",\\n+   \\\"nbconvert_exporter\\\": \\\"python\\\",\\n+   \\\"pygments_lexer\\\": \\\"ipython3\\\",\\n+   \\\"version\\\": \\\"3.11.5\\\"\\n+  }\\n+ },\\n+ \\\"nbformat\\\": 4,\\n+ \\\"nbformat_minor\\\": 5\\n+}\\ndiff --git a/notebooks/RQ_notebooks/RQ3.ipynb b/notebooks/RQ_notebooks/RQ3_draft.ipynb\\nsimilarity index 100%\\nrename from notebooks/RQ_notebooks/RQ3.ipynb\\nrename to notebooks/RQ_notebooks/RQ3_draft.ipynb\\ndiff --git a/notebooks/RQ_notebooks/Trained_models/RandomForest_Iris_v20250425_131407.pkl b/notebooks/RQ_notebooks/Trained_models/RandomForest_Iris_v20250425_131407.pkl\\nnew file mode 100644\\nindex 0000000..648e779\\nBinary files /dev/null and b/notebooks/RQ_notebooks/Trained_models/RandomForest_Iris_v20250425_131407.pkl differ\\ndiff --git a/notebooks/RQ_notebooks/all_runs_provenance.csv b/notebooks/RQ_notebooks/all_runs_provenance.csv\\ndeleted file mode 100644\\nindex 2784e45..0000000\\n--- a/notebooks/RQ_notebooks/all_runs_provenance.csv\\n+++ /dev/null\\n@@ -1,6 +0,0 @@\\n-run_id,run_name,experiment_id,start_time,end_time,n_artifacts,params__bootstrap,params__ccp_alpha,params__class_weight,params__columns_raw,params__criterion,params__database.description,params__database.id,params__database.name,params__database.owner,params__dataset.authors,params__dataset.doi,params__dataset.published,params__dataset.publisher,params__dataset.title,params__dropped_columns,params__feature_names,params__matplotlib_version,params__max_depth,params__max_features,params__max_leaf_nodes,params__max_samples,params__min_impurity_decrease,params__min_samples_leaf,params__min_samples_split,params__min_weight_fraction_leaf,params__numpy_version,params__n_estimators,params__n_features,params__n_features_final,params__n_jobs,params__n_records,params__n_test_samples,params__n_train_samples,params__oob_score,params__os_platform,params__pandas_version,params__python_version,params__random_state,params__retrieval_time,params__seaborn_version,params__shap_version,params__sklearn_version,params__test_size,params__verbose,params__warm_start,metrics__accuracy,metrics__accuracy_score_X_test,metrics__dbrepo.num_deletes,metrics__dbrepo.num_inserts,metrics__dbrepo.row_count_end,metrics__dbrepo.row_count_start,metrics__f1_macro,metrics__f1_score_X_test,metrics__precision_macro,metrics__precision_score_X_test,metrics__recall_macro,metrics__recall_score_X_test,metrics__roc_auc,metrics__roc_auc_score_X_test,metrics__training_accuracy_score,metrics__training_f1_score,metrics__training_log_loss,metrics__training_precision_score,metrics__training_recall_score,metrics__training_roc_auc,metrics__training_score,tags__dataset_id,tags__dataset_name,tags__dataset_version,tags__data_source,tags__dbrepo.admin_email,tags__dbrepo.base_url,tags__dbrepo.granularity,tags__dbrepo.protocol_version,tags__dbrepo.repository_name,tags__dbrepo.table_last_modified,tags__estimator_class,tags__estimator_name,tags__git_current_commit_hash,tags__git_previous_commit_hash,tags__git__current_commit_url,tags__mlflow.log-model.history,tags__mlflow.runName,tags__mlflow.source.name,tags__mlflow.source.type,tags__mlflow.user,tags__model_name,tags__notebook_name,tags__target_name,tags__training_end_time,tags__training_start_time\\n-361daa12f99f4129a06cd20b78dd6fa7,flawless-kit-371,615223710259862608,2025-04-23 21:04:21.262000+00:00,,23,True,0.0,None,\\\"['id', 'sepallengthcm', 'sepalwidthcm', 'petallengthcm', 'petalwidthcm', 'species']\\\",entropy,None,c3a42d17-42b7-43c9-a504-2363fb4c9c8d,Iris,reema,\\\"[\\\"\\\"Marshall Michael\\\"\\\"]\\\",10.5281/ZENODO.1404173,2018-8-27,Zenodo,Scikit-Learn Iris,[],\\\"['sepallengthcm', 'sepalwidthcm', 'petallengthcm', 'petalwidthcm']\\\",3.7.2,12,sqrt,None,None,0.0,2,5,0.0,1.24.4,200,4,4,-1,150,30,120,False,Windows 10,2.2.3,3.11.5,42,2025-04-23T21:04:22.410093,0.12.2,0.47.1,1.3.0,0.2,1,False,1.0,1.0,0.0,1.0,150.0,150.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.9666666666666667,0.9666666666666667,0.0653522301195834,0.9674588284344383,0.9666666666666667,0.9987492182614135,0.9666666666666667,iris_local,Iris,1.0.0,http://localhost/api/database/c3a42d17-42b7-43c9-a504-2363fb4c9c8d/table/5315e7da-64fb-4fdb-b493-95b4138c765f/data?size=100000&page=0,noreply@localhost,http://localhost,YYYY-MM-DDThh:mm:ssZ,2.0,Database Repository,2025-04-23T20:42:29.501Z,sklearn.ensemble._forest.RandomForestClassifier,RandomForestClassifier,d329c92495e196ec0f39fbb19dfdd367131a77d9,a07434af4f547af2daab044d6873eb7081162293,https://github.com/reema-dass26/REPO/commit/d329c92495e196ec0f39fbb19dfdd367131a77d9,\\\"[{\\\"\\\"run_id\\\"\\\": \\\"\\\"361daa12f99f4129a06cd20b78dd6fa7\\\"\\\", \\\"\\\"artifact_path\\\"\\\": \\\"\\\"model\\\"\\\", \\\"\\\"utc_time_created\\\"\\\": \\\"\\\"2025-04-23 21:04:23.264719\\\"\\\", \\\"\\\"model_uuid\\\"\\\": \\\"\\\"5788ebda55b2492fb25d01738dae4022\\\"\\\", \\\"\\\"flavors\\\"\\\": {\\\"\\\"python_function\\\"\\\": {\\\"\\\"model_path\\\"\\\": \\\"\\\"model.pkl\\\"\\\", \\\"\\\"predict_fn\\\"\\\": \\\"\\\"predict\\\"\\\", \\\"\\\"loader_module\\\"\\\": \\\"\\\"mlflow.sklearn\\\"\\\", \\\"\\\"python_version\\\"\\\": \\\"\\\"3.11.5\\\"\\\", \\\"\\\"env\\\"\\\": {\\\"\\\"conda\\\"\\\": \\\"\\\"conda.yaml\\\"\\\", \\\"\\\"virtualenv\\\"\\\": \\\"\\\"python_env.yaml\\\"\\\"}}, \\\"\\\"sklearn\\\"\\\": {\\\"\\\"pickled_model\\\"\\\": \\\"\\\"model.pkl\\\"\\\", \\\"\\\"sklearn_version\\\"\\\": \\\"\\\"1.3.0\\\"\\\", \\\"\\\"serialization_format\\\"\\\": \\\"\\\"cloudpickle\\\"\\\", \\\"\\\"code\\\"\\\": null}}}]\\\",flawless-kit-371,C:\\\\Users\\\\reema\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\ipykernel_launcher.py,LOCAL,reema,RandomForest_Iris_v20250423_230422,RQ1.ipynb,\\\"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\\n- 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\\n- 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\\n- 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\\n- 2 2]\\\",2025-04-23T23:04:30.472556,2025-04-23T23:04:22.449390\\ndiff --git a/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/confusion_matrix.png b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/confusion_matrix.png\\nnew file mode 100644\\nindex 0000000..5bf98dd\\nBinary files /dev/null and b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/confusion_matrix.png differ\\ndiff --git a/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/feature_importances.png b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/feature_importances.png\\nnew file mode 100644\\nindex 0000000..65481a2\\nBinary files /dev/null and b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/feature_importances.png differ\\ndiff --git a/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/pr_curve_cls_0.png b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/pr_curve_cls_0.png\\nnew file mode 100644\\nindex 0000000..a8d230a\\nBinary files /dev/null and b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/pr_curve_cls_0.png differ\\ndiff --git a/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/pr_curve_cls_1.png b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/pr_curve_cls_1.png\\nnew file mode 100644\\nindex 0000000..b36796e\\nBinary files /dev/null and b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/pr_curve_cls_1.png differ\\ndiff --git a/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/pr_curve_cls_2.png b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/pr_curve_cls_2.png\\nnew file mode 100644\\nindex 0000000..86bc90f\\nBinary files /dev/null and b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/pr_curve_cls_2.png differ\\ndiff --git a/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/roc_curve_cls_0.png b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/roc_curve_cls_0.png\\nnew file mode 100644\\nindex 0000000..5ae6a09\\nBinary files /dev/null and b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/roc_curve_cls_0.png differ\\ndiff --git a/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/roc_curve_cls_1.png b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/roc_curve_cls_1.png\\nnew file mode 100644\\nindex 0000000..7d3f039\\nBinary files /dev/null and b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/roc_curve_cls_1.png differ\\ndiff --git a/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/roc_curve_cls_2.png b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/roc_curve_cls_2.png\\nnew file mode 100644\\nindex 0000000..24b4468\\nBinary files /dev/null and b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/roc_curve_cls_2.png differ\\ndiff --git a/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/shap_summary.png b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/shap_summary.png\\nnew file mode 100644\\nindex 0000000..4ed1b76\\nBinary files /dev/null and b/notebooks/RQ_notebooks/plots/RandomForest_Iris_v20250425_131407/shap_summary.png differ\\ndiff --git a/notebooks/RQ_notebooks/vizualization.py b/notebooks/RQ_notebooks/vizualization.py\\nindex a858d33..358973f 100644\\n--- a/notebooks/RQ_notebooks/vizualization.py\\n+++ b/notebooks/RQ_notebooks/vizualization.py\\n@@ -19,7 +19,8 @@ import streamlit.components.v1 as components\\n import networkx as nx\\n from streamlit_agraph import agraph, Node, Edge, Config\\n import time\\n-\\n+from datetime import datetime\\n+import re\\n \\n st.set_page_config(\\n     page_title=\\\"Building Bridges in Research: Integrating Provenance and Data Management in Virtual Research Environments\\\",\\n@@ -213,7 +214,42 @@ USE_CASES = {\\n         'optional_params': [],\\n     },\\n }\\n-\\n+# \\u2014\\u2014 Find latest run summary with justification data \\u2014\\u2014\\n+\\n+def get_latest_justification_summary(base_dir=\\\"MODEL_PROVENANCE\\\"):\\n+    folders = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\\n+    timestamped_folders = []\\n+    for folder in folders:\\n+        match = re.search(r'_(v\\\\d{8}_\\\\d{6})', folder)\\n+        if match:\\n+            try:\\n+                timestamp = datetime.strptime(match.group(1), \\\"v%Y%m%d_%H%M%S\\\")\\n+                timestamped_folders.append((timestamp, folder))\\n+            except ValueError:\\n+                continue\\n+\\n+    if not timestamped_folders:\\n+        raise FileNotFoundError(\\\"No timestamped folders found in MODEL_PROVENANCE\\\")\\n+\\n+    latest_folder = max(timestamped_folders)[1]\\n+    file_path = os.path.join(base_dir, latest_folder, f\\\"{latest_folder}_run_summary.json\\\")\\n+    return file_path\\n+\\n+# \\u2014\\u2014 Load justifications and return as DataFrame \\u2014\\u2014\\n+\\n+def load_justification_table(path):\\n+    with open(path, \\\"r\\\") as f:\\n+        js = json.load(f)\\n+\\n+    justifications = {\\n+        k: v for k, v in js.get(\\\"tags\\\", {}).items()\\n+        if k.startswith(\\\"justification_\\\")\\n+    }\\n+    rows = [\\n+        {\\\"Decision\\\": k.replace(\\\"justification_\\\", \\\"\\\"), \\\"Justification\\\": v}\\n+        for k, v in justifications.items()\\n+    ]\\n+    return pd.DataFrame(rows)\\n # -------- Load the metadata (flattened like before) ----------\\n @st.cache_data\\n def load_data():\\n@@ -278,7 +314,8 @@ with st.sidebar:\\n             \\\"\\ud83d\\udef0\\ufe0f Provenance Trace\\\",\\n             \\\"\\u26a0\\ufe0f Deprecated Code Check\\\",\\n             \\\"\\ud83e\\udded Model-Dataset Mapping\\\",\\n-            \\\"\\ud83d\\udce3 Notify Outdated Forks\\\"\\n+            \\\"\\ud83d\\udce3 Notify Outdated Forks\\\",\\n+            \\\"\\ud83d\\udcd8 Researcher Justifications\\\"\\n         ],\\n         icons=[\\n             \\\"house\\\", \\\"database\\\", \\\"gear\\\", \\\"bar-chart\\\", \\\"globe\\\", \\\"link\\\", \\\"exclamation-triangle\\\",\\\"map\\\", \\\"megaphone\\\" \\n@@ -874,3 +911,21 @@ Detect whether collaborators' forks of your GitHub repository are out-of-date wi\\n \\n                 except Exception as e:\\n                     st.error(f\\\"An error occurred: {e}\\\")\\n+elif selected == \\\"\\ud83d\\udcd8 Researcher Justifications\\\":\\n+    st.title(\\\"\\ud83d\\udcd8 Researcher Justifications\\\")\\n+    st.markdown(\\\"\\\"\\\"\\n+    This section displays all recorded **justifications** provided by the researcher \\n+    for specific modeling decisions, such as hyperparameter choices, dataset version, and evaluation criteria.\\n+    \\n+    \\ud83e\\udde0 These justifications help ensure **transparency**, **explainability**, and support for reproducibility.\\n+    \\\"\\\"\\\")\\n+\\n+    try:\\n+        latest_path = get_latest_justification_summary()\\n+        st.success(f\\\"Loaded: `{latest_path}`\\\")\\n+\\n+        df_just = load_justification_table(latest_path)\\n+        st.write(\\\"### Justification Table\\\")\\n+        st.dataframe(df_just, use_container_width=True)\\n+    except Exception as e:\\n+        st.error(f\\\"Failed to load justification data: {e}\\\")\\n\"\n}"
    },
    {
      "path": "confusion_matrix.png",
      "type": "image",
      "uri": "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/confusion_matrix.png"
    },
    {
      "path": "estimator.html",
      "type": "other",
      "uri": "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/estimator.html"
    },
    {
      "path": "feature_importances.png",
      "type": "image",
      "uri": "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/feature_importances.png"
    },
    {
      "path": "label_mapping.json",
      "type": "text",
      "content": "{\n  \"0\": \"Iris-setosa\",\n  \"1\": \"Iris-versicolor\",\n  \"2\": \"Iris-virginica\"\n}"
    },
    {
      "path": "model/MLmodel",
      "type": "other",
      "uri": "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/model/MLmodel"
    },
    {
      "path": "model/conda.yaml",
      "type": "other",
      "uri": "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/model/conda.yaml"
    },
    {
      "path": "model/model.pkl",
      "type": "other",
      "uri": "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/model/model.pkl"
    },
    {
      "path": "model/python_env.yaml",
      "type": "other",
      "uri": "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/model/python_env.yaml"
    },
    {
      "path": "model/requirements.txt",
      "type": "text",
      "content": "mlflow==2.21.2\nbackports-functools-lru-cache==1.6.4\nbackports-tempfile==1.0\ncloudpickle==2.2.1\njaraco-classes==3.2.1\njaraco-collections==5.1.0\nlz4==4.3.2\nnumpy==1.24.4\npathlib==1.0.1\npsutil==5.9.5\nscikit-learn==1.3.0\nscipy==1.11.1"
    },
    {
      "path": "pr_curve_cls_0.png",
      "type": "image",
      "uri": "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/pr_curve_cls_0.png"
    },
    {
      "path": "pr_curve_cls_1.png",
      "type": "image",
      "uri": "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/pr_curve_cls_1.png"
    },
    {
      "path": "pr_curve_cls_2.png",
      "type": "image",
      "uri": "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/pr_curve_cls_2.png"
    },
    {
      "path": "public_datasetRepository_metadata.json",
      "type": "text",
      "content": "{\n  \"zenodo\": {\n    \"title\": \"Scikit-Learn Iris\",\n    \"doi\": \"10.5281/ZENODO.1404173\",\n    \"authors\": [\n      \"Marshall Michael\"\n    ],\n    \"published\": \"2018-8-27\",\n    \"publisher\": \"Zenodo\"\n  }\n}"
    },
    {
      "path": "roc_curve_cls_0.png",
      "type": "image",
      "uri": "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/roc_curve_cls_0.png"
    },
    {
      "path": "roc_curve_cls_1.png",
      "type": "image",
      "uri": "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/roc_curve_cls_1.png"
    },
    {
      "path": "roc_curve_cls_2.png",
      "type": "image",
      "uri": "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/roc_curve_cls_2.png"
    },
    {
      "path": "shap_summary.png",
      "type": "image",
      "uri": "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/shap_summary.png"
    },
    {
      "path": "training_confusion_matrix.png",
      "type": "image",
      "uri": "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/training_confusion_matrix.png"
    }
  ],
  "tags": {
    "dataset_id": "iris_local",
    "dataset_name": "Iris",
    "dataset_version": "1.0.0",
    "data_source": "http://localhost/api/database/c3a42d17-42b7-43c9-a504-2363fb4c9c8d/table/5315e7da-64fb-4fdb-b493-95b4138c765f/data?size=100000&page=0",
    "dbrepo.admin_email": "noreply@localhost",
    "dbrepo.base_url": "http://localhost",
    "dbrepo.granularity": "YYYY-MM-DDThh:mm:ssZ",
    "dbrepo.protocol_version": "2.0",
    "dbrepo.repository_name": "Database Repository",
    "dbrepo.table_last_modified": "2025-04-23T20:42:29.501Z",
    "estimator_class": "sklearn.ensemble._forest.RandomForestClassifier",
    "estimator_name": "RandomForestClassifier",
    "git_current_commit_hash": "ac1670d33efd1522e7eb68bb44b935d9574f8bf0",
    "git_previous_commit_hash": "e67d756afe694e01d42bbac8ab69de73007f473a",
    "git__current_commit_url": "https://github.com/reema-dass26/REPO/commit/ac1670d33efd1522e7eb68bb44b935d9574f8bf0",
    "justification_bootstrap": "test",
    "justification_class_weight": "test",
    "justification_criterion": "test",
    "justification_dataset_version": "test",
    "justification_drop_column_X": "test",
    "justification_experiment_name": "test",
    "justification_max_depth": "test",
    "justification_max_features": "test",
    "justification_metric_choice": "test",
    "justification_min_samples_leaf": "test",
    "justification_min_samples_split": "test",
    "justification_model_choice": "test",
    "justification_n_estimators": "test",
    "justification_n_jobs": "test",
    "justification_oob_score": "test",
    "justification_random_state": "test",
    "justification_target_variable": "test",
    "justification_test_split": "test",
    "justification_threshold_accuracy": "test",
    "justification_verbose": "test",
    "mlflow.log-model.history": "[{\"run_id\": \"8f7521eaa562415d9a450f4167a127ab\", \"artifact_path\": \"model\", \"utc_time_created\": \"2025-04-25 11:14:30.293566\", \"model_uuid\": \"24744c62104b4234b9e779827115b18e\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.11.5\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.3.0\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]",
    "mlflow.runName": "defiant-bat-29",
    "mlflow.source.name": "C:\\Users\\reema\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py",
    "mlflow.source.type": "LOCAL",
    "mlflow.user": "reema",
    "model_name": "RandomForest_Iris_v20250425_131407",
    "notebook_name": "RQ1.ipynb",
    "target_name": "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]",
    "training_end_time": "2025-04-25T13:14:37.806516",
    "training_start_time": "2025-04-25T13:14:07.465803"
  },
  "start_time": "2025-04-25T11:14:06.269000+00:00",
  "used": [],
  "generated": [
    "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/RandomForest_Iris_v20250425_131407/RandomForest_Iris_v20250425_131407.pkl",
    "commit_diff.json",
    "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/confusion_matrix.png",
    "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/estimator.html",
    "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/feature_importances.png",
    "label_mapping.json",
    "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/model/MLmodel",
    "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/model/conda.yaml",
    "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/model/model.pkl",
    "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/model/python_env.yaml",
    "model/requirements.txt",
    "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/pr_curve_cls_0.png",
    "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/pr_curve_cls_1.png",
    "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/pr_curve_cls_2.png",
    "public_datasetRepository_metadata.json",
    "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/roc_curve_cls_0.png",
    "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/roc_curve_cls_1.png",
    "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/roc_curve_cls_2.png",
    "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/shap_summary.png",
    "file:///C:/Users/reema/REPO/notebooks/RQ_notebooks/mlrunlogs/mlflow.db/615223710259862608/8f7521eaa562415d9a450f4167a127ab/artifacts/training_confusion_matrix.png"
  ]
}