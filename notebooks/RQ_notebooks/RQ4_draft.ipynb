{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b379083d-146e-4d73-845d-2e11287fc469",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Full dynamic mapping file created: mappings/full_mapping.json with 149 fields!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def fetch_all_keys(json_path):\n",
    "    \"\"\"Recursively fetch all keys from a JSON file.\"\"\"\n",
    "    keys = set()\n",
    "\n",
    "    def _recursive_extract(obj, prefix=\"\"):\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in obj.items():\n",
    "                full_key = f\"{prefix}.{k}\" if prefix else k\n",
    "                keys.add(full_key)\n",
    "                _recursive_extract(v, prefix=full_key)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                _recursive_extract(item, prefix=prefix)\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        _recursive_extract(data)\n",
    "\n",
    "    return keys\n",
    "\n",
    "# 🔥 Dynamically fetch keys from all MODEL_PROVENANCE run summaries\n",
    "all_json_files = glob.glob(\"MODEL_PROVENANCE/*/*_run_summary.json\")\n",
    "\n",
    "collected_keys = set()\n",
    "for json_file in all_json_files:\n",
    "    keys = fetch_all_keys(json_file)\n",
    "    collected_keys.update(keys)\n",
    "\n",
    "# ✅ Now build the mapping\n",
    "mapping = {key: {\"@id\": key} for key in collected_keys}\n",
    "\n",
    "# Special case for timestamps\n",
    "if \"start_time\" in mapping:\n",
    "    mapping[\"start_time\"][\"@id\"] = \"prov:startedAtTime\"\n",
    "    mapping[\"start_time\"][\"@type\"] = \"xsd:dateTime\"\n",
    "if \"end_time\" in mapping:\n",
    "    mapping[\"end_time\"][\"@id\"] = \"prov:endedAtTime\"\n",
    "    mapping[\"end_time\"][\"@type\"] = \"xsd:dateTime\"\n",
    "\n",
    "# 🔥 Save mapping dynamically\n",
    "os.makedirs(\"mappings\", exist_ok=True)\n",
    "with open(\"mappings/full_mapping.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mapping, f, indent=2)\n",
    "\n",
    "print(f\"✅ Full dynamic mapping file created: mappings/full_mapping.json with {len(mapping)} fields!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae3bbfbe-cf32-4436-8b68-8c30b5588cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Full semantic JSON-LD created for RandomForest_Iris_v20250425_121328_run_summary.json -> RandomForest_Iris_v20250425_121328.jsonld\n",
      "✅ Full semantic JSON-LD created for RandomForest_Iris_v20250425_125653_run_summary.json -> RandomForest_Iris_v20250425_125653.jsonld\n",
      "✅ Full semantic JSON-LD created for RandomForest_Iris_v20250425_131407_run_summary.json -> RandomForest_Iris_v20250425_131407.jsonld\n",
      "✅ Full semantic JSON-LD created for RandomForest_Iris_v20250425_132526_run_summary.json -> RandomForest_Iris_v20250425_132526.jsonld\n",
      "✅ Full semantic JSON-LD created for RandomForest_Iris_v20250425_135553_run_summary.json -> RandomForest_Iris_v20250425_135553.jsonld\n",
      "✅ Full semantic JSON-LD created for RandomForest_Iris_v20250425_135900_run_summary.json -> RandomForest_Iris_v20250425_135900.jsonld\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import json\n",
    "# from datetime import datetime, timezone\n",
    "# from rdflib import Graph\n",
    "\n",
    "# import os\n",
    "# def iso8601(ms):\n",
    "#     \"\"\"Convert milliseconds since epoch to ISO8601 UTC.\"\"\"\n",
    "#     return datetime.fromtimestamp(ms / 1000, tz=timezone.utc).isoformat()\n",
    "# # Load the context mapping\n",
    "# with open(\"mappings/full_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     ctx = json.load(f)\n",
    "\n",
    "# # Loop through your run_summary files\n",
    "# for json_path in glob.glob(\"MODEL_PROVENANCE/*/*_run_summary.json\"):\n",
    "#     basename   = os.path.basename(json_path)\n",
    "#     model_name = basename.rsplit(\"_run_summary.json\", 1)[0]\n",
    "\n",
    "#     with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         summary = json.load(f)\n",
    "\n",
    "#     doc = {\n",
    "#         \"@context\": ctx,\n",
    "#         \"run_id\": summary.get(\"run_id\", \"\"),\n",
    "#         \"run_name\": summary.get(\"run_name\", \"\"),\n",
    "#         \"experiment_id\": summary.get(\"experiment_id\", \"\"),\n",
    "#         \"params\": summary.get(\"params\", {}),\n",
    "#         \"metrics\": summary.get(\"metrics\", {}),\n",
    "#         \"artifacts\": summary.get(\"artifacts\", []),\n",
    "#         \"tags\": summary.get(\"tags\", {}),\n",
    "#         \"start_time\": iso8601(summary[\"start_time\"])\n",
    "#     }\n",
    "\n",
    "#     if summary.get(\"end_time\") is not None:\n",
    "#         doc[\"end_time\"] = iso8601(summary[\"end_time\"])\n",
    "\n",
    "#     doc[\"used\"] = summary.get(\"tags\", {}).get(\"dataset_uri\") or []\n",
    "#     doc[\"generated\"] = [\n",
    "#         art.get(\"uri\") or art.get(\"path\")\n",
    "#         for art in summary.get(\"artifacts\", [])\n",
    "#     ]\n",
    "\n",
    "#     # Save .jsonld\n",
    "#     out_jsonld = os.path.join(\"MODEL_PROVENANCE\", model_name, f\"{model_name}.jsonld\")\n",
    "#     with open(out_jsonld, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(doc, f, indent=2)\n",
    "\n",
    "#     # # Save .ttl\n",
    "#     # g = Graph().parse(data=json.dumps(doc), format=\"json-ld\")\n",
    "#     # out_ttl = os.path.join(\"MODEL_PROVENANCE\", model_name, f\"{model_name}.ttl\")\n",
    "#     # g.serialize(destination=out_ttl, format=\"turtle\")\n",
    "\n",
    "#     print(f\"✅ Converted {basename} → {os.path.basename(out_jsonld)}\")\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Helper to convert milliseconds to ISO8601\n",
    "def iso8601(ms):\n",
    "    \"\"\"Convert milliseconds since epoch to ISO8601 UTC.\"\"\"\n",
    "    return datetime.fromtimestamp(ms / 1000, tz=timezone.utc).isoformat()\n",
    "\n",
    "# Helper to clean and build safe IDs\n",
    "def safe_id(prefix, key):\n",
    "    key = key.lower().replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\"+\", \"_\")\n",
    "    return f\"ex:{prefix}_{key}\"\n",
    "\n",
    "# Set up the base context\n",
    "context = {\n",
    "    \"prov\": \"http://www.w3.org/ns/prov#\",\n",
    "    \"ex\": \"http://example.org/mlprovenance#\",\n",
    "    \"xsd\": \"http://www.w3.org/2001/XMLSchema#\",\n",
    "    \"prov:value\": {\n",
    "        \"@id\": \"prov:value\"\n",
    "    },\n",
    "    \"prov:location\": {\n",
    "        \"@id\": \"prov:location\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Loop through your run_summary files\n",
    "for json_path in glob.glob(\"MODEL_PROVENANCE/*/*_run_summary.json\"):\n",
    "    basename   = os.path.basename(json_path)\n",
    "    model_name = basename.rsplit(\"_run_summary.json\", 1)[0]\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        summary = json.load(f)\n",
    "\n",
    "    graph = []\n",
    "\n",
    "    # Create the main Run node (Activity)\n",
    "    run_id = f\"ex:run_{summary.get('run_id', model_name)}\"\n",
    "    run_node = {\n",
    "        \"@id\": run_id,\n",
    "        \"@type\": \"prov:Activity\",\n",
    "        \"prov:startedAtTime\": iso8601(summary[\"start_time\"]),\n",
    "    }\n",
    "\n",
    "    if summary.get(\"end_time\") is not None:\n",
    "        run_node[\"prov:endedAtTime\"] = iso8601(summary[\"end_time\"])\n",
    "\n",
    "    # Collect linked nodes\n",
    "    had_parameters = []\n",
    "    had_quality = []\n",
    "    used_entities = []\n",
    "    generated_entities = []\n",
    "\n",
    "    # Params -> prov:Entity ex:Parameter\n",
    "    for param_name, param_value in summary.get(\"params\", {}).items():\n",
    "        param_id = safe_id(\"param\", param_name)\n",
    "        param_node = {\n",
    "            \"@id\": param_id,\n",
    "            \"@type\": [\"prov:Entity\", \"ex:Parameter\"],\n",
    "            \"prov:value\": param_value\n",
    "        }\n",
    "        graph.append(param_node)\n",
    "        had_parameters.append({\"@id\": param_id})\n",
    "\n",
    "    # Metrics -> prov:Entity ex:Metric\n",
    "    for metric_name, metric_value in summary.get(\"metrics\", {}).items():\n",
    "        metric_id = safe_id(\"metric\", metric_name)\n",
    "        metric_node = {\n",
    "            \"@id\": metric_id,\n",
    "            \"@type\": [\"prov:Entity\", \"ex:Metric\"],\n",
    "            \"prov:value\": metric_value\n",
    "        }\n",
    "        graph.append(metric_node)\n",
    "        had_quality.append({\"@id\": metric_id})\n",
    "\n",
    "    # Artifacts -> prov:Entity ex:Artifact\n",
    "    for artifact in summary.get(\"artifacts\", []):\n",
    "        art_path = artifact.get(\"path\", \"artifact_unknown\")\n",
    "        artifact_id = safe_id(\"artifact\", art_path)\n",
    "        artifact_node = {\n",
    "            \"@id\": artifact_id,\n",
    "            \"@type\": [\"prov:Entity\", \"ex:Artifact\"],\n",
    "        }\n",
    "        if \"uri\" in artifact:\n",
    "            artifact_node[\"prov:location\"] = artifact[\"uri\"]\n",
    "        graph.append(artifact_node)\n",
    "        generated_entities.append({\"@id\": artifact_id})\n",
    "\n",
    "    # Used dataset (optional)\n",
    "    dataset_uri = summary.get(\"tags\", {}).get(\"dataset_uri\")\n",
    "    if dataset_uri:\n",
    "        dataset_id = safe_id(\"dataset\", dataset_uri)\n",
    "        dataset_node = {\n",
    "            \"@id\": dataset_id,\n",
    "            \"@type\": [\"prov:Entity\", \"ex:Dataset\"],\n",
    "            \"prov:location\": dataset_uri\n",
    "        }\n",
    "        graph.append(dataset_node)\n",
    "        used_entities.append({\"@id\": dataset_id})\n",
    "\n",
    "    # Attach linked entities to run node\n",
    "    if had_parameters:\n",
    "        run_node[\"prov:hadParameter\"] = had_parameters\n",
    "    if had_quality:\n",
    "        run_node[\"prov:hadQuality\"] = had_quality\n",
    "    if used_entities:\n",
    "        run_node[\"prov:used\"] = used_entities\n",
    "    if generated_entities:\n",
    "        run_node[\"prov:generated\"] = generated_entities\n",
    "\n",
    "    # Add run to the graph\n",
    "    graph.insert(0, run_node)\n",
    "\n",
    "    # Final JSON-LD document\n",
    "    doc = {\n",
    "        \"@context\": context,\n",
    "        \"@graph\": graph\n",
    "    }\n",
    "\n",
    "    # Save .jsonld\n",
    "    out_dir = os.path.join(\"MODEL_PROVENANCE\", model_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_jsonld = os.path.join(out_dir, f\"{model_name}.jsonld\")\n",
    "    with open(out_jsonld, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(doc, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Full semantic JSON-LD created for {basename} -> {os.path.basename(out_jsonld)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c868f2bf-51a6-40dc-8071-f60a573463f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 6 JSON-LD files.\n",
      "\n",
      "📄 Visualizing and Saving for: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_121328\\RandomForest_Iris_v20250425_121328.jsonld\n",
      "✅ PNG graph saved at: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_121328\\RandomForest_Iris_v20250425_121328JSONLD_viz.png\n",
      "\n",
      "📄 Visualizing and Saving for: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_125653\\RandomForest_Iris_v20250425_125653.jsonld\n",
      "✅ PNG graph saved at: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_125653\\RandomForest_Iris_v20250425_125653JSONLD_viz.png\n",
      "\n",
      "📄 Visualizing and Saving for: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_131407\\RandomForest_Iris_v20250425_131407.jsonld\n",
      "✅ PNG graph saved at: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_131407\\RandomForest_Iris_v20250425_131407JSONLD_viz.png\n",
      "\n",
      "📄 Visualizing and Saving for: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_132526\\RandomForest_Iris_v20250425_132526.jsonld\n",
      "✅ PNG graph saved at: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_132526\\RandomForest_Iris_v20250425_132526JSONLD_viz.png\n",
      "\n",
      "📄 Visualizing and Saving for: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_135553\\RandomForest_Iris_v20250425_135553.jsonld\n",
      "✅ PNG graph saved at: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_135553\\RandomForest_Iris_v20250425_135553JSONLD_viz.png\n",
      "\n",
      "📄 Visualizing and Saving for: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_135900\\RandomForest_Iris_v20250425_135900.jsonld\n",
      "✅ PNG graph saved at: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_135900\\RandomForest_Iris_v20250425_135900JSONLD_viz.png\n",
      "\n",
      "🏁 Finished generating all graph visualizations!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from rdflib import Graph\n",
    "from graphviz import Digraph\n",
    "from IPython.display import display\n",
    "import glob\n",
    "import hashlib\n",
    "import os\n",
    "\n",
    "# Helper functions\n",
    "def safe_id(text):\n",
    "    return hashlib.md5(str(text).encode('utf-8')).hexdigest()\n",
    "\n",
    "def pretty_label(uri):\n",
    "    uri = str(uri)\n",
    "    if \"#\" in uri:\n",
    "        return uri.split(\"#\")[-1]\n",
    "    elif \"/\" in uri:\n",
    "        return uri.split(\"/\")[-1]\n",
    "    return uri\n",
    "\n",
    "# Step 1: Pick up all JSON-LD files\n",
    "jsonld_files = glob.glob('MODEL_PROVENANCE/*/*.jsonld')\n",
    "\n",
    "print(f\"✅ Found {len(jsonld_files)} JSON-LD files.\")\n",
    "\n",
    "# Step 2: Loop through all JSON-LD files (NO break anymore!)\n",
    "for file_path in jsonld_files:\n",
    "    print(f\"\\n📄 Visualizing and Saving for: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        # Step 3: Parse the RDF Graph\n",
    "        g = Graph()\n",
    "        g.parse(file_path, format=\"json-ld\")\n",
    "\n",
    "        dot = Digraph(comment=f'Graph for {os.path.basename(file_path)}')\n",
    "        dot.attr(rankdir='LR')  # left to right\n",
    "\n",
    "        nodes = set()\n",
    "\n",
    "        # Step 4: Build Nodes and Edges\n",
    "        for subj, pred, obj in g:\n",
    "            subj_id = safe_id(subj)\n",
    "            obj_id = safe_id(obj)\n",
    "\n",
    "            if subj_id not in nodes:\n",
    "                dot.node(subj_id, label=pretty_label(subj))\n",
    "                nodes.add(subj_id)\n",
    "            if obj_id not in nodes:\n",
    "                dot.node(obj_id, label=pretty_label(obj))\n",
    "                nodes.add(obj_id)\n",
    "\n",
    "            dot.edge(subj_id, obj_id, label=pretty_label(pred))\n",
    "\n",
    "        # Step 5: Save PNG next to JSON-LD\n",
    "        file_base = os.path.splitext(file_path)[0]  # removes \".jsonld\"\n",
    "        output_png_path = file_base + \"JSONLD_viz.png\"\n",
    "\n",
    "        dot.render(file_base + \"JSONLD_viz\", format='png', cleanup=True)\n",
    "        print(f\"✅ PNG graph saved at: {output_png_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path}: {e}\")\n",
    "\n",
    "print(\"\\n🏁 Finished generating all graph visualizations!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfca20f-13d2-4524-a65b-177f20f51ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3bb4a4-f8ef-4ede-919b-ce80659fb95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c3576c1-6df9-469f-8b72-302e9738bbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Differences for RandomForest_Iris_v20250425_121328: 4 differences found\n",
      "\n",
      "🔎 Differences for RandomForest_Iris_v20250425_125653: 5 differences found\n",
      "\n",
      "🔎 Differences for RandomForest_Iris_v20250425_131407: 4 differences found\n",
      "\n",
      "🔎 Differences for RandomForest_Iris_v20250425_132526: 4 differences found\n",
      "\n",
      "🔎 Differences for RandomForest_Iris_v20250425_135553: 5 differences found\n",
      "\n",
      "🔎 Differences for RandomForest_Iris_v20250425_135900: 5 differences found\n",
      "\n",
      "Summary of all differences:\n",
      "type\n",
      "added      12\n",
      "removed     9\n",
      "changed     6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import glob\n",
    "# import pandas as pd\n",
    "# from rdflib import Graph\n",
    "\n",
    "# # ---------- Helper functions -------------\n",
    "\n",
    "# def load_as_dict(path):\n",
    "#     \"\"\"Load a JSON or JSON-LD/Turtle file as dictionary.\"\"\"\n",
    "#     if path.endswith((\".ttl\", \".turtle\")):\n",
    "#         g = Graph()\n",
    "#         g.parse(path, format=\"turtle\")\n",
    "#         return json.loads(g.serialize(format=\"json-ld\", indent=2))\n",
    "#     else:\n",
    "#         with open(path, encoding=\"utf-8\") as f:\n",
    "#             return json.load(f)\n",
    "\n",
    "# def compare_json(a, b, path=\"\"):\n",
    "#     \"\"\"Recursively compare two JSON structures.\"\"\"\n",
    "#     diffs = []\n",
    "#     if isinstance(a, dict) and isinstance(b, dict):\n",
    "#         a = {k: v for k, v in a.items() if k != \"@context\"}\n",
    "#         b = {k: v for k, v in b.items() if k != \"@context\"}\n",
    "#         all_keys = set(a) | set(b)\n",
    "#         for k in all_keys:\n",
    "#             new_path = f\"{path}/{k}\" if path else k\n",
    "#             if k not in a:\n",
    "#                 diffs.append({\"path\": new_path, \"type\": \"added\", \"a\": None, \"b\": b[k]})\n",
    "#             elif k not in b:\n",
    "#                 diffs.append({\"path\": new_path, \"type\": \"removed\", \"a\": a[k], \"b\": None})\n",
    "#             else:\n",
    "#                 diffs.extend(compare_json(a[k], b[k], new_path))\n",
    "#     elif isinstance(a, list) and isinstance(b, list):\n",
    "#         for i, (ia, ib) in enumerate(zip(a, b)):\n",
    "#             diffs.extend(compare_json(ia, ib, f\"{path}[{i}]\"))\n",
    "#         if len(a) < len(b):\n",
    "#             for i in range(len(a), len(b)):\n",
    "#                 diffs.append({\"path\": f\"{path}[{i}]\", \"type\": \"added\", \"a\": None, \"b\": b[i]})\n",
    "#         elif len(a) > len(b):\n",
    "#             for i in range(len(b), len(a)):\n",
    "#                 diffs.append({\"path\": f\"{path}[{i}]\", \"type\": \"removed\", \"a\": a[i], \"b\": None})\n",
    "#     else:\n",
    "#         if a != b:\n",
    "#             diffs.append({\"path\": path, \"type\": \"changed\", \"a\": a, \"b\": b})\n",
    "#     return diffs\n",
    "\n",
    "# # ---------- Main comparison -------------\n",
    "\n",
    "# # 1. Scan for all run folders\n",
    "# base_dir = \"MODEL_PROVENANCE\"\n",
    "# runs = [d for d in glob.glob(os.path.join(base_dir, \"*\")) if os.path.isdir(d)]\n",
    "\n",
    "# # 2. Compare only JSON vs JSON-LD\n",
    "# all_diffs = []\n",
    "\n",
    "# for run_dir in runs:\n",
    "#     model_name = os.path.basename(run_dir)\n",
    "    \n",
    "#     json_path = os.path.join(run_dir, f\"{model_name}_run_summary.json\")\n",
    "#     jsonld_path = os.path.join(run_dir, f\"{model_name}.jsonld\")\n",
    "\n",
    "#     if os.path.exists(json_path) and os.path.exists(jsonld_path):\n",
    "#         try:\n",
    "#             json_obj = load_as_dict(json_path)\n",
    "#             jsonld_obj = load_as_dict(jsonld_path)\n",
    "\n",
    "#             diffs = compare_json(json_obj, jsonld_obj)\n",
    "#             if diffs:\n",
    "#                 print(f\"\\n🔎 Differences for {model_name}: {len(diffs)} differences found\")\n",
    "#                 all_diffs.extend(diffs)\n",
    "#             else:\n",
    "#                 print(f\"✅ {model_name}: No differences detected\")\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Error comparing {model_name}: {e}\")\n",
    "\n",
    "#     else:\n",
    "#         print(f\"⚠️ Missing files in {model_name}: Skipping.\")\n",
    "\n",
    "# # 3. Summarize if needed\n",
    "# if all_diffs:\n",
    "#     df_diffs = pd.DataFrame(all_diffs)\n",
    "#     print(\"\\nSummary of all differences:\")\n",
    "#     print(df_diffs['type'].value_counts())\n",
    "# else:\n",
    "#     print(\"\\n🎉 All JSON and JSON-LD files match perfectly!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c62d5-79bd-4141-9c68-b6e7361d3a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1882902a-7acd-4efe-b53f-4c4463cbb3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mapping file created: mapping_files\\dynamic_mapping.json\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import glob\n",
    "# import os\n",
    "\n",
    "# def fetch_all_keys(json_path):\n",
    "#     \"\"\"Recursively fetch all keys from a JSON file.\"\"\"\n",
    "#     keys = set()\n",
    "\n",
    "#     def _recursive_extract(obj, prefix=\"\"):\n",
    "#         if isinstance(obj, dict):\n",
    "#             for k, v in obj.items():\n",
    "#                 full_key = f\"{prefix}.{k}\" if prefix else k\n",
    "#                 keys.add(full_key)\n",
    "#                 _recursive_extract(v, prefix=full_key)\n",
    "#         elif isinstance(obj, list):\n",
    "#             for item in obj:\n",
    "#                 _recursive_extract(item, prefix=prefix)\n",
    "\n",
    "#     with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         data = json.load(f)\n",
    "#         _recursive_extract(data)\n",
    "\n",
    "#     return keys\n",
    "\n",
    "# def create_mapping_from_keys(keys):\n",
    "#     \"\"\"Create a simple mapping where each key maps to itself, with special rules for timestamps.\"\"\"\n",
    "#     mapping = {}\n",
    "\n",
    "#     for key in sorted(keys):\n",
    "#         if \".\" not in key:\n",
    "#             # Top-level fields\n",
    "#             if key in [\"start_time\", \"end_time\"]:\n",
    "#                 mapping[key] = {\n",
    "#                     \"@id\": f\"prov:{'startedAtTime' if key == 'start_time' else 'endedAtTime'}\",\n",
    "#                     \"@type\": \"xsd:dateTime\"\n",
    "#                 }\n",
    "#             elif key in [\"run_id\", \"run_name\", \"experiment_id\"]:\n",
    "#                 mapping[key] = {\"@id\": key}\n",
    "#             else:\n",
    "#                 mapping[key] = {\"@id\": key}\n",
    "#         else:\n",
    "#             # Nested fields\n",
    "#             mapping[key] = {\"@id\": key}\n",
    "\n",
    "#     # Attach namespaces\n",
    "#     mapping[\"@context\"] = {\n",
    "#         \"prov\": \"http://www.w3.org/ns/prov#\",\n",
    "#         \"xsd\":  \"http://www.w3.org/2001/XMLSchema#\"\n",
    "#     }\n",
    "\n",
    "#     return mapping\n",
    "\n",
    "# # --- Main execution ---\n",
    "\n",
    "# # 1. Fetch keys from all JSONs\n",
    "# all_keys = set()\n",
    "# for json_path in glob.glob(\"MODEL_PROVENANCE/*/*_run_summary.json\"):\n",
    "#     keys = fetch_all_keys(json_path)\n",
    "#     all_keys.update(keys)\n",
    "\n",
    "# # 2. Create mapping\n",
    "# mapping_dict = create_mapping_from_keys(all_keys)\n",
    "\n",
    "# # 3. Save mapping\n",
    "# output_dir = \"mapping_files\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# output_path = os.path.join(output_dir, \"dynamic_mapping.json\")\n",
    "\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(mapping_dict, f, indent=2)\n",
    "\n",
    "# print(f\"✅ Mapping file created: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e36288ee-3033-473a-ba9e-4233d10869a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RandomForest_Iris_v20250425_121328: JSON-LD and TTL generated\n",
      "✅ RandomForest_Iris_v20250425_125653: JSON-LD and TTL generated\n",
      "✅ RandomForest_Iris_v20250425_131407: JSON-LD and TTL generated\n",
      "✅ RandomForest_Iris_v20250425_132526: JSON-LD and TTL generated\n",
      "✅ RandomForest_Iris_v20250425_135553: JSON-LD and TTL generated\n",
      "✅ RandomForest_Iris_v20250425_135900: JSON-LD and TTL generated\n",
      "🚀 All model runs converted successfully!\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import os\n",
    "# import glob\n",
    "# from rdflib import Graph\n",
    "# from datetime import datetime, timezone\n",
    "\n",
    "# # === Utility functions ===\n",
    "\n",
    "# def iso8601(ms):\n",
    "#     \"\"\"Convert milliseconds since epoch to ISO8601 UTC.\"\"\"\n",
    "#     return datetime.fromtimestamp(ms / 1000, tz=timezone.utc).isoformat()\n",
    "\n",
    "# def load_mapping(mapping_path=\"mapping_files/dynamic_mapping.json\"):\n",
    "#     \"\"\"Load dynamic mapping file.\"\"\"\n",
    "#     with open(mapping_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         return json.load(f)\n",
    "\n",
    "# def map_json_fields(summary, mapping):\n",
    "#     \"\"\"Apply dynamic field mapping to summary dict.\"\"\"\n",
    "#     doc = {\"@context\": mapping[\"@context\"]}\n",
    "\n",
    "#     for key, map_info in mapping.items():\n",
    "#         if key == \"@context\":\n",
    "#             continue\n",
    "\n",
    "#         value = get_nested(summary, key)\n",
    "#         if value is not None:\n",
    "#             mapped_key = map_info[\"@id\"]\n",
    "#             # Apply ISO8601 if type is datetime\n",
    "#             if map_info.get(\"@type\") == \"xsd:dateTime\":\n",
    "#                 value = iso8601(value)\n",
    "#             doc[mapped_key] = value\n",
    "\n",
    "#     return doc\n",
    "\n",
    "# def get_nested(data, dotted_key):\n",
    "#     \"\"\"Safely get nested keys like artifacts.uri.\"\"\"\n",
    "#     parts = dotted_key.split(\".\")\n",
    "#     for part in parts:\n",
    "#         if isinstance(data, dict):\n",
    "#             data = data.get(part)\n",
    "#         else:\n",
    "#             return None\n",
    "#     return data\n",
    "\n",
    "# # === Main execution ===\n",
    "\n",
    "# # Load the dynamic mapping\n",
    "# mapping = load_mapping()\n",
    "\n",
    "# # Process all summaries\n",
    "# all_json_files = glob.glob(\"MODEL_PROVENANCE/*/*_run_summary.json\")\n",
    "\n",
    "# for json_path in all_json_files:\n",
    "#     base_dir = os.path.dirname(json_path)\n",
    "#     basename = os.path.basename(json_path)\n",
    "#     model_name = basename.replace(\"_run_summary.json\", \"\")\n",
    "\n",
    "#     # Load JSON\n",
    "#     with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         summary = json.load(f)\n",
    "\n",
    "#     # Map using dynamic mapping\n",
    "#     jsonld_doc = map_json_fields(summary, mapping)\n",
    "\n",
    "#     # Save as .jsonld\n",
    "#     jsonld_path = os.path.join(base_dir, f\"{model_name}.jsonld\")\n",
    "#     with open(jsonld_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(jsonld_doc, f, indent=2)\n",
    "\n",
    "#     # Convert to .ttl\n",
    "#     g = Graph()\n",
    "#     g.parse(data=json.dumps(jsonld_doc), format=\"json-ld\")\n",
    "#     ttl_path = os.path.join(base_dir, f\"{model_name}.ttl\")\n",
    "#     g.serialize(destination=ttl_path, format=\"turtle\")\n",
    "\n",
    "#     print(f\"✅ {model_name}: JSON-LD and TTL generated\")\n",
    "\n",
    "# print(\"🚀 All model runs converted successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb0175dd-db95-444e-b762-323162bf5c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RDF/XML written to MODEL_PROVENANCE/RandomForest_Iris_v20250425_135900/RandomForest_Iris_v20250425_135900.rdf\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# from rdflib import Graph\n",
    "\n",
    "# def convert_jsonld_to_rdfxml(jsonld_path, rdfxml_out_path):\n",
    "#     # 1. Load your JSON-LD\n",
    "#     with open(jsonld_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         jsonld_data = json.load(f)\n",
    "\n",
    "#     # 2. Parse it into an RDF Graph\n",
    "#     g = Graph()\n",
    "#     g.parse(data=json.dumps(jsonld_data), format=\"json-ld\")\n",
    "\n",
    "#     # 3. Serialize it into RDF/XML\n",
    "#     g.serialize(destination=rdfxml_out_path, format=\"xml\")\n",
    "\n",
    "#     print(f\"✅ RDF/XML written to {rdfxml_out_path}\")\n",
    "\n",
    "# # Example usage\n",
    "# convert_jsonld_to_rdfxml(\n",
    "#     \"MODEL_PROVENANCE/RandomForest_Iris_v20250425_135900/RandomForest_Iris_v20250425_135900.jsonld\",\n",
    "#     \"MODEL_PROVENANCE/RandomForest_Iris_v20250425_135900/RandomForest_Iris_v20250425_135900.rdf\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9773f38-8440-46ed-a330-8f00405d2368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Found 149 unique fields across 6 run summaries.\n",
      "✅ Mapping created and saved at: mappings\\json_to_rdf_mapping.json\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import json\n",
    "\n",
    "# def fetch_all_keys(json_path):\n",
    "#     \"\"\"Recursively fetch all JSON keys in dot notation.\"\"\"\n",
    "#     keys = set()\n",
    "\n",
    "#     def _recursive_extract(obj, prefix=\"\"):\n",
    "#         if isinstance(obj, dict):\n",
    "#             for k, v in obj.items():\n",
    "#                 full_key = f\"{prefix}.{k}\" if prefix else k\n",
    "#                 keys.add(full_key)\n",
    "#                 _recursive_extract(v, prefix=full_key)\n",
    "#         elif isinstance(obj, list):\n",
    "#             for item in obj:\n",
    "#                 _recursive_extract(item, prefix=prefix)\n",
    "\n",
    "#     with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         data = json.load(f)\n",
    "#         _recursive_extract(data)\n",
    "\n",
    "#     return keys\n",
    "\n",
    "# # 🔥 Fetch all keys from MODEL_PROVENANCE\n",
    "# all_json_files = glob.glob(\"MODEL_PROVENANCE/*/*_run_summary.json\")\n",
    "# collected_keys = set()\n",
    "\n",
    "# for json_file in all_json_files:\n",
    "#     keys = fetch_all_keys(json_file)\n",
    "#     collected_keys.update(keys)\n",
    "\n",
    "# print(f\"🔎 Found {len(collected_keys)} unique fields across {len(all_json_files)} run summaries.\")\n",
    "\n",
    "# # ✨ Auto-build the mapping: JSON key ➔ RDF property\n",
    "# mapping = {}\n",
    "\n",
    "# for key in sorted(collected_keys):\n",
    "#     rdf_key = key.replace(\".\", \"_\")  # replace dot with underscore\n",
    "#     mapping[key] = f\"prov:{rdf_key}\"\n",
    "\n",
    "# # 📂 Save to file\n",
    "# os.makedirs(\"mappings\", exist_ok=True)\n",
    "# mapping_file = os.path.join(\"mappings\", \"json_to_rdf_mapping.json\")\n",
    "\n",
    "# with open(mapping_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(mapping, f, indent=2)\n",
    "\n",
    "# print(f\"✅ Mapping created and saved at: {mapping_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dd52c15-e6a2-4b89-b26f-03bb64936beb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RDF/XML created: rdf_exports\\RandomForest_Iris_v20250425_121328.rdf\n",
      "✅ RDF/XML created: rdf_exports\\RandomForest_Iris_v20250425_125653.rdf\n",
      "✅ RDF/XML created: rdf_exports\\RandomForest_Iris_v20250425_131407.rdf\n",
      "✅ RDF/XML created: rdf_exports\\RandomForest_Iris_v20250425_132526.rdf\n",
      "✅ RDF/XML created: rdf_exports\\RandomForest_Iris_v20250425_135553.rdf\n",
      "✅ RDF/XML created: rdf_exports\\RandomForest_Iris_v20250425_135900.rdf\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import glob\n",
    "# import os\n",
    "# from rdflib import Graph, URIRef, Literal, Namespace, RDF\n",
    "# from rdflib.namespace import XSD\n",
    "\n",
    "# # Load your dynamic JSON ➔ RDF mapping\n",
    "# with open(\"mappings/json_to_rdf_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     field_mapping = json.load(f)\n",
    "\n",
    "# prov = Namespace(\"http://www.w3.org/ns/prov#\")\n",
    "\n",
    "# def flatten_json(obj, parent_key=''):\n",
    "#     \"\"\"Flatten nested JSON with dot notation.\"\"\"\n",
    "#     items = []\n",
    "#     if isinstance(obj, dict):\n",
    "#         for k, v in obj.items():\n",
    "#             new_key = f\"{parent_key}.{k}\" if parent_key else k\n",
    "#             items.extend(flatten_json(v, new_key))\n",
    "#     elif isinstance(obj, list):\n",
    "#         for i, v in enumerate(obj):\n",
    "#             new_key = f\"{parent_key}[{i}]\"\n",
    "#             items.extend(flatten_json(v, new_key))\n",
    "#     else:\n",
    "#         items.append((parent_key, obj))\n",
    "#     return items\n",
    "\n",
    "# def create_rdf_from_json(json_path):\n",
    "#     \"\"\"Given a run_summary JSON file, create RDF/XML.\"\"\"\n",
    "#     with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     # Flatten the JSON\n",
    "#     flat_data = dict(flatten_json(data))\n",
    "\n",
    "#     # Build RDF graph\n",
    "#     g = Graph()\n",
    "#     g.bind(\"prov\", prov)\n",
    "\n",
    "#     # Create a blank subject (could also use the run_id if you want)\n",
    "#     subj = URIRef(f\"urn:uuid:{data.get('run_id', 'unknown-run')}\")\n",
    "\n",
    "#     for key, value in flat_data.items():\n",
    "#         if key in field_mapping:\n",
    "#             pred = URIRef(field_mapping[key].replace(\"prov:\", str(prov)))\n",
    "#             if isinstance(value, (int, float)):\n",
    "#                 obj = Literal(value)\n",
    "#             else:\n",
    "#                 obj = Literal(str(value))\n",
    "#             g.add((subj, pred, obj))\n",
    "#         else:\n",
    "#             # Keys that don't have mapping: (skip or warn)\n",
    "#             pass\n",
    "\n",
    "#     return g\n",
    "\n",
    "# # 🔥 Process all run summaries\n",
    "# os.makedirs(\"rdf_exports\", exist_ok=True)\n",
    "\n",
    "# for json_file in glob.glob(\"MODEL_PROVENANCE/*/*_run_summary.json\"):\n",
    "#     model_name = os.path.basename(json_file).replace(\"_run_summary.json\", \"\")\n",
    "\n",
    "#     rdf_graph = create_rdf_from_json(json_file)\n",
    "\n",
    "#     # Save as RDF/XML\n",
    "#     out_path = os.path.join(\"rdf_exports\", f\"{model_name}.rdf\")\n",
    "#     rdf_graph.serialize(destination=out_path, format=\"xml\")\n",
    "\n",
    "#     print(f\"✅ RDF/XML created: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab0122-20c3-4f9d-8f8c-476b6bef5529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5efa6e19-03b1-4b45-845d-e4815e3f8d95",
   "metadata": {},
   "source": [
    "RDF/XML conversion from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "613be5f4-1932-476f-b5a3-7ebfe7d5ee20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_121328\\RandomForest_Iris_v20250425_121328_run_summary.json\n",
      "✅ RDF/XML created for 28f01e38b7f04d2f948fe21f57f41d0c: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_121328\\28f01e38b7f04d2f948fe21f57f41d0c.xml\n",
      "\n",
      "🔍 Processing: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_125653\\RandomForest_Iris_v20250425_125653_run_summary.json\n",
      "✅ RDF/XML created for 68d5dd35a5354061bf02395d2243b624: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_125653\\68d5dd35a5354061bf02395d2243b624.xml\n",
      "\n",
      "🔍 Processing: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_131407\\RandomForest_Iris_v20250425_131407_run_summary.json\n",
      "✅ RDF/XML created for 8f7521eaa562415d9a450f4167a127ab: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_131407\\8f7521eaa562415d9a450f4167a127ab.xml\n",
      "\n",
      "🔍 Processing: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_132526\\RandomForest_Iris_v20250425_132526_run_summary.json\n",
      "✅ RDF/XML created for 78e6e34ac94a460a893791a3e02f6da7: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_132526\\78e6e34ac94a460a893791a3e02f6da7.xml\n",
      "\n",
      "🔍 Processing: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_135553\\RandomForest_Iris_v20250425_135553_run_summary.json\n",
      "✅ RDF/XML created for 3ec1102377b049589537b68a9494fbfc: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_135553\\3ec1102377b049589537b68a9494fbfc.xml\n",
      "\n",
      "🔍 Processing: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_135900\\RandomForest_Iris_v20250425_135900_run_summary.json\n",
      "✅ RDF/XML created for 3205602562ad40ec941b325936959807: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_135900\\3205602562ad40ec941b325936959807.xml\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "from rdflib.namespace import RDF, XSD\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Define namespaces\n",
    "PROV = Namespace(\"http://www.w3.org/ns/prov#\")\n",
    "EX = Namespace(\"http://example.org/mlprovenance#\")\n",
    "\n",
    "# 🔥 Fetch all run summaries\n",
    "all_json_files = glob.glob(\"MODEL_PROVENANCE/*/*_run_summary.json\")\n",
    "\n",
    "for json_path in all_json_files:\n",
    "    print(f\"\\n🔍 Processing: {json_path}\")\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Create a graph for each run\n",
    "    g = Graph()\n",
    "    g.bind(\"prov\", PROV)\n",
    "    g.bind(\"ex\", EX)\n",
    "\n",
    "    run_uri = EX[f\"run_{data['run_id']}\"]\n",
    "    g.add((run_uri, RDF.type, PROV.Activity))\n",
    "\n",
    "    # Start time\n",
    "    def convert_millis_to_iso(ms):\n",
    "        if isinstance(ms, (int, float, str)) and str(ms).isdigit():\n",
    "            ms = int(ms)\n",
    "            return datetime.fromtimestamp(ms / 1000, tz=timezone.utc).isoformat()\n",
    "        return ms\n",
    "\n",
    "    if data.get(\"start_time\"):\n",
    "        safe_start_time = convert_millis_to_iso(data[\"start_time\"])\n",
    "        g.add((run_uri, PROV.startedAtTime, Literal(safe_start_time, datatype=XSD.dateTime)))\n",
    "\n",
    "    if data.get(\"end_time\"):\n",
    "        safe_end_time = convert_millis_to_iso(data[\"end_time\"])\n",
    "        g.add((run_uri, PROV.endedAtTime, Literal(safe_end_time, datatype=XSD.dateTime)))\n",
    "\n",
    "    for param, value in data.get('params', {}).items():\n",
    "        param_entity = URIRef(run_uri + f\"/param/{param}\")\n",
    "        g.add((param_entity, RDF.type, PROV.Entity))\n",
    "        g.add((param_entity, PROV.value, Literal(str(value))))\n",
    "        g.add((run_uri, PROV.hadParameter, param_entity))\n",
    "\n",
    "    for metric, value in data.get('metrics', {}).items():\n",
    "        metric_entity = URIRef(run_uri + f\"/metric/{metric}\")\n",
    "        g.add((metric_entity, RDF.type, PROV.Entity))\n",
    "        try:\n",
    "            val = float(value)\n",
    "            g.add((metric_entity, PROV.value, Literal(val, datatype=XSD.float)))\n",
    "        except (ValueError, TypeError):\n",
    "            g.add((metric_entity, PROV.value, Literal(str(value))))\n",
    "        g.add((run_uri, PROV.hadQuality, metric_entity))\n",
    "\n",
    "    for tag, value in data.get('tags', {}).items():\n",
    "        tag_entity = URIRef(run_uri + f\"/tag/{tag}\")\n",
    "        g.add((tag_entity, RDF.type, PROV.Entity))\n",
    "        g.add((tag_entity, PROV.value, Literal(str(value))))\n",
    "        g.add((run_uri, PROV.used, tag_entity))\n",
    "\n",
    "    for artifact in data.get('artifacts', []):\n",
    "        artifact_id = artifact.get('path', '').replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "        artifact_entity = URIRef(run_uri + f\"/artifact/{artifact_id}\")\n",
    "        g.add((artifact_entity, RDF.type, PROV.Entity))\n",
    "        g.add((artifact_entity, PROV.location, Literal(artifact.get('uri', ''))))\n",
    "        g.add((run_uri, PROV.generated, artifact_entity))\n",
    "\n",
    "    # 🌟 Save RDF/XML directly into same folder as JSON\n",
    "    json_dir = os.path.dirname(json_path)  # where the JSON lives\n",
    "    run_id_safe = data['run_id']\n",
    "    out_rdfxml = os.path.join(json_dir, f\"{run_id_safe}.xml\")\n",
    "\n",
    "    g.serialize(destination=out_rdfxml, format='xml')\n",
    "    print(f\"✅ RDF/XML created for {run_id_safe}: {out_rdfxml}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88984a48-a6a3-4f96-9faf-7979ab571227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 6 RDF/XML files to visualize.\n",
      "🎨 Visualizing: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_121328\\28f01e38b7f04d2f948fe21f57f41d0c.xml\n",
      "✅ PNG saved: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_121328\\28f01e38b7f04d2f948fe21f57f41d0cRDFXML_viz.png\n",
      "🎨 Visualizing: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_125653\\68d5dd35a5354061bf02395d2243b624.xml\n",
      "✅ PNG saved: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_125653\\68d5dd35a5354061bf02395d2243b624RDFXML_viz.png\n",
      "🎨 Visualizing: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_131407\\8f7521eaa562415d9a450f4167a127ab.xml\n",
      "✅ PNG saved: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_131407\\8f7521eaa562415d9a450f4167a127abRDFXML_viz.png\n",
      "🎨 Visualizing: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_132526\\78e6e34ac94a460a893791a3e02f6da7.xml\n",
      "✅ PNG saved: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_132526\\78e6e34ac94a460a893791a3e02f6da7RDFXML_viz.png\n",
      "🎨 Visualizing: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_135553\\3ec1102377b049589537b68a9494fbfc.xml\n",
      "✅ PNG saved: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_135553\\3ec1102377b049589537b68a9494fbfcRDFXML_viz.png\n",
      "🎨 Visualizing: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_135900\\3205602562ad40ec941b325936959807.xml\n",
      "✅ PNG saved: MODEL_PROVENANCE\\RandomForest_Iris_v20250425_135900\\3205602562ad40ec941b325936959807RDFXML_viz.png\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "from graphviz import Digraph\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "# Helper functions\n",
    "def sanitize(text):\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\\", \"\\\\\\\\\")  # Escape backslashes\n",
    "    text = re.sub(r'([\"])', r'\\\\\\1', text)  # Escape double quotes inside too\n",
    "    return '\"' + text + '\"'\n",
    "\n",
    "def simple_id(text):\n",
    "    return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "def short_uri(uri):\n",
    "    uri = str(uri)\n",
    "    if \"#\" in uri:\n",
    "        return uri.split(\"#\")[-1]\n",
    "    elif \"/\" in uri:\n",
    "        return uri.split(\"/\")[-1]\n",
    "    return uri\n",
    "\n",
    "# 🧠 Now fetch RDF/XML files correctly\n",
    "rdf_files = glob.glob('MODEL_PROVENANCE/*/*.xml')\n",
    "\n",
    "print(f\"✅ Found {len(rdf_files)} RDF/XML files to visualize.\")\n",
    "\n",
    "for rdf_path in rdf_files:\n",
    "    print(f\"🎨 Visualizing: {rdf_path}\")\n",
    "\n",
    "    g = Graph()\n",
    "    g.parse(rdf_path, format='xml')\n",
    "\n",
    "    dot = Digraph(comment=f'Graph for {os.path.basename(rdf_path)}')\n",
    "    dot.attr(rankdir='LR')  # Left-to-right layout\n",
    "\n",
    "    nodes = set()\n",
    "\n",
    "    for subj, pred, obj in g:\n",
    "        subj_id = simple_id(str(subj))\n",
    "        obj_id = simple_id(str(obj))\n",
    "        pred_label = short_uri(pred)\n",
    "\n",
    "        if subj_id not in nodes:\n",
    "            dot.node(subj_id, label=short_uri(subj))\n",
    "            nodes.add(subj_id)\n",
    "        if obj_id not in nodes:\n",
    "            dot.node(obj_id, label=short_uri(obj))\n",
    "            nodes.add(obj_id)\n",
    "\n",
    "        dot.edge(subj_id, obj_id, label=pred_label)\n",
    "\n",
    "    # 🌟 Save PNG visualization right next to the .xml\n",
    "    output_base = os.path.splitext(rdf_path)[0]\n",
    "    output_path = output_base + \"RDFXML_viz\"  # Don't overwrite the XML!\n",
    "\n",
    "    dot.render(output_path, format='png', cleanup=True)\n",
    "    print(f\"✅ PNG saved: {output_path}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d1c94d-b59a-4751-ab69-b2e97b7cdf83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
