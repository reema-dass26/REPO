{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12fa6f59-927c-4003-964f-83e53793fd36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: atm the mlflow autolog isnt capturing metrics n params\n",
    "# and sklearn.autolog throws error( posted the issue on github)\n",
    "# Ideally, I should be able to fetch most of the imp detail via MLFLOW AUTOLOG. will check that later in time\n",
    "#============================\n",
    "# 🧠 MLflow Autologging\n",
    "# ============================\n",
    "\n",
    "# mlflow.autolog(log_input_examples=True, log_model_signatures=True)\n",
    "# mlflow.sklearn.autolog() \n",
    "# mlflow.sklearn.autolog(\n",
    "#     log_input_examples=True,\n",
    "#     log_model_signatures=True,\n",
    "#     log_post_training_metrics=True,        # calls model.score() → accuracy\n",
    "#     disable_for_unsupported_versions=True,  # skips if versions still wonky\n",
    "#     exclusive=True                          # only patch the sklearn integration\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4d6b8-34a9-47b5-974d-5927c0ee2256",
   "metadata": {},
   "source": [
    "DBREPO INTEGRETION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8e3570e2-9a60-45b4-8653-28060071e728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response: [{'id': '1', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '2', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '3', 'sepallengthcm': '4.700000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '4', 'sepallengthcm': '4.600000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '5', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.600000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '6', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.900000000000000000', 'petallengthcm': '1.700000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '7', 'sepallengthcm': '4.600000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '8', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '9', 'sepallengthcm': '4.400000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '10', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '11', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.700000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '12', 'sepallengthcm': '4.800000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '13', 'sepallengthcm': '4.800000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '14', 'sepallengthcm': '4.300000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.100000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '15', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '4.000000000000000000', 'petallengthcm': '1.200000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '16', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '4.400000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '17', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.900000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '18', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '19', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '1.700000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '20', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '21', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.700000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '22', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.700000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '23', 'sepallengthcm': '4.600000000000000000', 'sepalwidthcm': '3.600000000000000000', 'petallengthcm': '1.000000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '24', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '1.700000000000000000', 'petalwidthcm': '0.500000000000000000', 'species': 'Iris-setosa'}, {'id': '25', 'sepallengthcm': '4.800000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.900000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '26', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '27', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '28', 'sepallengthcm': '5.200000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '29', 'sepallengthcm': '5.200000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '30', 'sepallengthcm': '4.700000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '31', 'sepallengthcm': '4.800000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '32', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '33', 'sepallengthcm': '5.200000000000000000', 'sepalwidthcm': '4.100000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '34', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '4.200000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '35', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '36', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '1.200000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '37', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '38', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.100000000000000000', 'species': 'Iris-setosa'}, {'id': '39', 'sepallengthcm': '4.400000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '40', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '41', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '42', 'sepallengthcm': '4.500000000000000000', 'sepalwidthcm': '2.300000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '43', 'sepallengthcm': '4.400000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '1.300000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '44', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.500000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.600000000000000000', 'species': 'Iris-setosa'}, {'id': '45', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '1.900000000000000000', 'petalwidthcm': '0.400000000000000000', 'species': 'Iris-setosa'}, {'id': '46', 'sepallengthcm': '4.800000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.300000000000000000', 'species': 'Iris-setosa'}, {'id': '47', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '1.600000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '48', 'sepallengthcm': '4.600000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '49', 'sepallengthcm': '5.300000000000000000', 'sepalwidthcm': '3.700000000000000000', 'petallengthcm': '1.500000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '50', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '1.400000000000000000', 'petalwidthcm': '0.200000000000000000', 'species': 'Iris-setosa'}, {'id': '51', 'sepallengthcm': '7.000000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '4.700000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '52', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '53', 'sepallengthcm': '6.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '4.900000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '54', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '2.300000000000000000', 'petallengthcm': '4.000000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '55', 'sepallengthcm': '6.500000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.600000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '56', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '57', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '4.700000000000000000', 'petalwidthcm': '1.600000000000000000', 'species': 'Iris-versicolor'}, {'id': '58', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '2.400000000000000000', 'petallengthcm': '3.300000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '59', 'sepallengthcm': '6.600000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.600000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '60', 'sepallengthcm': '5.200000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '3.900000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '61', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '2.000000000000000000', 'petallengthcm': '3.500000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '62', 'sepallengthcm': '5.900000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.200000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '63', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '2.200000000000000000', 'petallengthcm': '4.000000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '64', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.700000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '65', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '3.600000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '66', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '4.400000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '67', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '68', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '4.100000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '69', 'sepallengthcm': '6.200000000000000000', 'sepalwidthcm': '2.200000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '70', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '3.900000000000000000', 'petalwidthcm': '1.100000000000000000', 'species': 'Iris-versicolor'}, {'id': '71', 'sepallengthcm': '5.900000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '4.800000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-versicolor'}, {'id': '72', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.000000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '73', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '4.900000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '74', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.700000000000000000', 'petalwidthcm': '1.200000000000000000', 'species': 'Iris-versicolor'}, {'id': '75', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.300000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '76', 'sepallengthcm': '6.600000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.400000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '77', 'sepallengthcm': '6.800000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.800000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '78', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.000000000000000000', 'petalwidthcm': '1.700000000000000000', 'species': 'Iris-versicolor'}, {'id': '79', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '80', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '2.600000000000000000', 'petallengthcm': '3.500000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '81', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '2.400000000000000000', 'petallengthcm': '3.800000000000000000', 'petalwidthcm': '1.100000000000000000', 'species': 'Iris-versicolor'}, {'id': '82', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '2.400000000000000000', 'petallengthcm': '3.700000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '83', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '3.900000000000000000', 'petalwidthcm': '1.200000000000000000', 'species': 'Iris-versicolor'}, {'id': '84', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '1.600000000000000000', 'species': 'Iris-versicolor'}, {'id': '85', 'sepallengthcm': '5.400000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '86', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.600000000000000000', 'species': 'Iris-versicolor'}, {'id': '87', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '4.700000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-versicolor'}, {'id': '88', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.300000000000000000', 'petallengthcm': '4.400000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '89', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.100000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '90', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '4.000000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '91', 'sepallengthcm': '5.500000000000000000', 'sepalwidthcm': '2.600000000000000000', 'petallengthcm': '4.400000000000000000', 'petalwidthcm': '1.200000000000000000', 'species': 'Iris-versicolor'}, {'id': '92', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.600000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-versicolor'}, {'id': '93', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.600000000000000000', 'petallengthcm': '4.000000000000000000', 'petalwidthcm': '1.200000000000000000', 'species': 'Iris-versicolor'}, {'id': '94', 'sepallengthcm': '5.000000000000000000', 'sepalwidthcm': '2.300000000000000000', 'petallengthcm': '3.300000000000000000', 'petalwidthcm': '1.000000000000000000', 'species': 'Iris-versicolor'}, {'id': '95', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '4.200000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '96', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.200000000000000000', 'petalwidthcm': '1.200000000000000000', 'species': 'Iris-versicolor'}, {'id': '97', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.200000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '98', 'sepallengthcm': '6.200000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '4.300000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '99', 'sepallengthcm': '5.100000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '3.000000000000000000', 'petalwidthcm': '1.100000000000000000', 'species': 'Iris-versicolor'}, {'id': '100', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.100000000000000000', 'petalwidthcm': '1.300000000000000000', 'species': 'Iris-versicolor'}, {'id': '101', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '6.000000000000000000', 'petalwidthcm': '2.500000000000000000', 'species': 'Iris-virginica'}, {'id': '102', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '1.900000000000000000', 'species': 'Iris-virginica'}, {'id': '103', 'sepallengthcm': '7.100000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.900000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '104', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '105', 'sepallengthcm': '6.500000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.800000000000000000', 'petalwidthcm': '2.200000000000000000', 'species': 'Iris-virginica'}, {'id': '106', 'sepallengthcm': '7.600000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '6.600000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '107', 'sepallengthcm': '4.900000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '4.500000000000000000', 'petalwidthcm': '1.700000000000000000', 'species': 'Iris-virginica'}, {'id': '108', 'sepallengthcm': '7.300000000000000000', 'sepalwidthcm': '2.900000000000000000', 'petallengthcm': '6.300000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '109', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '5.800000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '110', 'sepallengthcm': '7.200000000000000000', 'sepalwidthcm': '3.600000000000000000', 'petallengthcm': '6.100000000000000000', 'petalwidthcm': '2.500000000000000000', 'species': 'Iris-virginica'}, {'id': '111', 'sepallengthcm': '6.500000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '112', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '5.300000000000000000', 'petalwidthcm': '1.900000000000000000', 'species': 'Iris-virginica'}, {'id': '113', 'sepallengthcm': '6.800000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.500000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '114', 'sepallengthcm': '5.700000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '5.000000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '115', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '2.400000000000000000', 'species': 'Iris-virginica'}, {'id': '116', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '5.300000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '117', 'sepallengthcm': '6.500000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.500000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '118', 'sepallengthcm': '7.700000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '6.700000000000000000', 'petalwidthcm': '2.200000000000000000', 'species': 'Iris-virginica'}, {'id': '119', 'sepallengthcm': '7.700000000000000000', 'sepalwidthcm': '2.600000000000000000', 'petallengthcm': '6.900000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '120', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '2.200000000000000000', 'petallengthcm': '5.000000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-virginica'}, {'id': '121', 'sepallengthcm': '6.900000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '5.700000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '122', 'sepallengthcm': '5.600000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.900000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '123', 'sepallengthcm': '7.700000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '6.700000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '124', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '4.900000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '125', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '5.700000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '126', 'sepallengthcm': '7.200000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '6.000000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '127', 'sepallengthcm': '6.200000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '4.800000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '128', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.900000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '129', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '130', 'sepallengthcm': '7.200000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.800000000000000000', 'petalwidthcm': '1.600000000000000000', 'species': 'Iris-virginica'}, {'id': '131', 'sepallengthcm': '7.400000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '6.100000000000000000', 'petalwidthcm': '1.900000000000000000', 'species': 'Iris-virginica'}, {'id': '132', 'sepallengthcm': '7.900000000000000000', 'sepalwidthcm': '3.800000000000000000', 'petallengthcm': '6.400000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '133', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '2.200000000000000000', 'species': 'Iris-virginica'}, {'id': '134', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.800000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '1.500000000000000000', 'species': 'Iris-virginica'}, {'id': '135', 'sepallengthcm': '6.100000000000000000', 'sepalwidthcm': '2.600000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '1.400000000000000000', 'species': 'Iris-virginica'}, {'id': '136', 'sepallengthcm': '7.700000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '6.100000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '137', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '2.400000000000000000', 'species': 'Iris-virginica'}, {'id': '138', 'sepallengthcm': '6.400000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '5.500000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '139', 'sepallengthcm': '6.000000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '4.800000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}, {'id': '140', 'sepallengthcm': '6.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '5.400000000000000000', 'petalwidthcm': '2.100000000000000000', 'species': 'Iris-virginica'}, {'id': '141', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '5.600000000000000000', 'petalwidthcm': '2.400000000000000000', 'species': 'Iris-virginica'}, {'id': '142', 'sepallengthcm': '6.900000000000000000', 'sepalwidthcm': '3.100000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '143', 'sepallengthcm': '5.800000000000000000', 'sepalwidthcm': '2.700000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '1.900000000000000000', 'species': 'Iris-virginica'}, {'id': '144', 'sepallengthcm': '6.800000000000000000', 'sepalwidthcm': '3.200000000000000000', 'petallengthcm': '5.900000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '145', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.300000000000000000', 'petallengthcm': '5.700000000000000000', 'petalwidthcm': '2.500000000000000000', 'species': 'Iris-virginica'}, {'id': '146', 'sepallengthcm': '6.700000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.200000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '147', 'sepallengthcm': '6.300000000000000000', 'sepalwidthcm': '2.500000000000000000', 'petallengthcm': '5.000000000000000000', 'petalwidthcm': '1.900000000000000000', 'species': 'Iris-virginica'}, {'id': '148', 'sepallengthcm': '6.500000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.200000000000000000', 'petalwidthcm': '2.000000000000000000', 'species': 'Iris-virginica'}, {'id': '149', 'sepallengthcm': '6.200000000000000000', 'sepalwidthcm': '3.400000000000000000', 'petallengthcm': '5.400000000000000000', 'petalwidthcm': '2.300000000000000000', 'species': 'Iris-virginica'}, {'id': '150', 'sepallengthcm': '5.900000000000000000', 'sepalwidthcm': '3.000000000000000000', 'petallengthcm': '5.100000000000000000', 'petalwidthcm': '1.800000000000000000', 'species': 'Iris-virginica'}]\n",
      "<built-in method count of list object at 0x000001EF7FA4E400>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# API endpoint URL\n",
    "API_URL = \"http://localhost/api/database/c3a42d17-42b7-43c9-a504-2363fb4c9c8d/table/5315e7da-64fb-4fdb-b493-95b4138c765f/data?size=100000&page=0\"\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\"  # Specify the expected response format\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the API with the Accept header\n",
    "    response = requests.get(API_URL, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        dataset = response.json()\n",
    "        print(\"API Response:\", dataset)\n",
    "        print( dataset.count)\n",
    "    else:\n",
    "        print(f\"Error: Received status code {response.status_code}\")\n",
    "        print(\"Response content:\", response.text)\n",
    "       \n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09557f94-325c-4bd6-882a-069a9e3c5ecd",
   "metadata": {},
   "source": [
    "replacing dynamic fetching of data while i integrete invenio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ce6e020d-cb80-49ec-8bcc-687b1e08885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a. Save raw JSON\n",
    "# with open(\"iris_data.json\", \"w\") as f:\n",
    "#     json.dump(dataset, f, indent=2)\n",
    "\n",
    "# 1. Read the JSON file\n",
    "with open(\"iris_data.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c6007a-2126-4b1a-90ee-3326eb39a362",
   "metadata": {},
   "source": [
    "Metadata fetching from db repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe912e7-bf9b-4bbd-8e43-6046745ade3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import mlflow\n",
    "\n",
    "DB_API = \"http://localhost/api/database/{db_id}\"\n",
    "\n",
    "def fetch_db_metadata(db_id: str) -> dict:\n",
    "    url = DB_API.format(db_id=db_id)\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "\n",
    "def log_db_metadata(db_meta: dict):\n",
    "    # 1) core DB fields as params\n",
    "    mlflow.log_param(\"database.id\",          db_meta[\"id\"])\n",
    "    mlflow.log_param(\"database.name\",        db_meta[\"name\"])\n",
    "    mlflow.log_param(\"database.description\", db_meta.get(\"description\",\"\"))\n",
    "    owner = db_meta[\"tables\"][0][\"owner\"][\"username\"]\n",
    "    mlflow.log_param(\"database.owner\",       owner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296f307e-e01b-477a-9406-92cab9f2d7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ns0:OAI-PMH xmlns:ns0=\"http://www.openarchives.org/OAI/2.0/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd\">\n",
      "    <ns0:responseDate>2025-04-23T20:56:28Z</ns0:responseDate>\n",
      "    <ns0:request verb=\"Identify\">https://localhost/api/oai</ns0:request>\n",
      "    <ns0:Identify>\n",
      "    <ns0:repositoryName>Database Repository</ns0:repositoryName>\n",
      "    <ns0:baseURL>http://localhost</ns0:baseURL>\n",
      "    <ns0:protocolVersion>2.0</ns0:protocolVersion>\n",
      "    <ns0:adminEmail>noreply@localhost</ns0:adminEmail>\n",
      "    <ns0:earliestDatestamp />\n",
      "    <ns0:deletedRecord>persistent</ns0:deletedRecord>\n",
      "    <ns0:granularity>YYYY-MM-DDThh:mm:ssZ</ns0:granularity>\n",
      "</ns0:Identify>\n",
      "</ns0:OAI-PMH>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import urllib.parse\n",
    "import mlflow, requests, json\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Fetch your database metadata\n",
    "db_url = \"http://localhost/api/database/c3a42d17-42b7-43c9-a504-2363fb4c9c8d\"\n",
    "db_resp = requests.get(db_url)\n",
    "db_resp.raise_for_status()\n",
    "db_data = db_resp.json()\n",
    "\n",
    "db_id  = db_data[\"id\"]\n",
    "tbl_id = db_data[\"tables\"][0][\"id\"]\n",
    "\n",
    "# 2) Build the OAI-PMH URL, URL-encoding the `set` param\n",
    "set_param   = f\"Databases/{db_id}/Tables/{tbl_id}\"\n",
    "encoded_set = urllib.parse.quote(set_param, safe=\"\")\n",
    "\n",
    "oai_url = (\n",
    "    \"http://localhost/api/oai\"\n",
    "    f\"?metadataPrefix=oai_dc\"\n",
    "    f\"&from=2025-03-01\"\n",
    "    f\"&until=2025-03-07\"\n",
    "    f\"&set={encoded_set}\"\n",
    "    f\"&resumptionToken=string\"\n",
    "    f\"&fromDate=2025-03-07T19%3A35%3A51.476Z\"\n",
    "    f\"&untilDate=2025-03-07T19%3A35%3A51.476Z\"\n",
    "    f\"&parametersString=string\"\n",
    ")\n",
    "\n",
    "# 3) Call and parse\n",
    "try:\n",
    "    resp = requests.get(oai_url)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    if \"xml\" in resp.headers.get(\"Content-Type\", \"\"):\n",
    "        root = ET.fromstring(resp.text)\n",
    "        print(ET.tostring(root, encoding=\"utf-8\").decode())\n",
    "    else:\n",
    "        print(\"Non-XML response:\", resp.headers.get(\"Content-Type\"), resp.text)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Request failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61cc99ab-4a5c-4142-8725-e7c940673ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# …after you fetch & parse your XML into `root`…\n",
    "ns = {\"oai\": \"http://www.openarchives.org/OAI/2.0/\"}\n",
    "repo_name   = root.findtext(\"oai:Identify/oai:repositoryName\",   namespaces=ns)\n",
    "base_url    = root.findtext(\"oai:Identify/oai:baseURL\",          namespaces=ns)\n",
    "protocol    = root.findtext(\"oai:Identify/oai:protocolVersion\",  namespaces=ns)\n",
    "admin_email = root.findtext(\"oai:Identify/oai:adminEmail\",       namespaces=ns)\n",
    "gran        = root.findtext(\"oai:Identify/oai:granularity\",      namespaces=ns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74214aa7-c12f-414e-9feb-094a366b855b",
   "metadata": {},
   "source": [
    "History Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c74e9b-c9b0-4b4a-82eb-2a6e56456508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response: [{'timestamp': '2025-04-23T20:42:29.501Z', 'event': 'insert', 'total': 150}]\n"
     ]
    }
   ],
   "source": [
    "url = \"http://localhost/api/database/c3a42d17-42b7-43c9-a504-2363fb4c9c8d/table/5315e7da-64fb-4fdb-b493-95b4138c765f/history\"\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the API\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "        print(\"API Response:\", data)\n",
    "    else:\n",
    "        print(f\"Error: Received status code {response.status_code}\")\n",
    "        print(\"Response content:\", response.text)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3630c954-5ad2-4759-b9a0-fa6e20e184ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "first   = data[0]\n",
    "last    = data[-1]\n",
    "count_0 = first[\"total\"]    # e.g. 149\n",
    "count_N = last[\"total\"]     # e.g. 149 again, or changed\n",
    "ts_last = last[\"timestamp\"]  # e.g. \"2025-03-28T17:42:38.058Z\"\n",
    "n_insert = sum(1 for ev in data if ev[\"event\"]==\"insert\")\n",
    "n_delete = sum(1 for ev in data if ev[\"event\"]==\"delete\")\n",
    "history = response.json()\n",
    "first, last = history[0], history[-1]\n",
    "\n",
    "# summary stats\n",
    "count_start = first[\"total\"]\n",
    "count_end   = last[\"total\"]\n",
    "ts_last     = last[\"timestamp\"]\n",
    "n_insert    = sum(1 for ev in history if ev[\"event\"]==\"insert\")\n",
    "n_delete    = sum(1 for ev in history if ev[\"event\"]==\"delete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd5dad-72d5-42e1-a0fa-b7bd3455937b",
   "metadata": {},
   "source": [
    "Dataset metadata fetching from ZONEDO or any public dataset repositories to gain more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7fa122a-c6e5-4b38-842a-dc81590a1f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_and_log_dataset_metadata_nested(doi_url: str):\n",
    "    # 1) fetch the CSL+JSON\n",
    "    headers = {\"Accept\": \"application/vnd.citationstyles.csl+json\"}\n",
    "    r = requests.get(doi_url, headers=headers); r.raise_for_status()\n",
    "    meta = r.json()\n",
    "\n",
    "    # 2) pull out what you care about\n",
    "    authors = [f\"{a.get('family','')} {a.get('given','')}\".strip()\n",
    "               for a in meta.get(\"author\", [])]\n",
    "    pubdate = \"-\".join(str(x) for x in meta.get(\"issued\",{}).get(\"date-parts\",[[]])[0])\n",
    "\n",
    "    # 3) assemble one nested dict\n",
    "    public_datasetRepository_metadata = {\n",
    "      \"zenodo\": {\n",
    "        \"title\":     meta.get(\"title\"),\n",
    "        \"doi\":       meta.get(\"DOI\"),\n",
    "        \"authors\":   authors,\n",
    "        \"published\": pubdate,\n",
    "        \"publisher\": meta.get(\"publisher\"),\n",
    "      },\n",
    "   \n",
    "    }\n",
    "\n",
    "      # 4) log it as a single JSON artifact\n",
    "    mlflow.log_dict(public_datasetRepository_metadata,\n",
    "                \"public_datasetRepository_metadata.json\")\n",
    "\n",
    "    # 2) Flatten and log the important bits as params:\n",
    "    z = public_datasetRepository_metadata[\"zenodo\"]\n",
    "    \n",
    "    mlflow.log_param(\"dataset.title\",     z[\"title\"])\n",
    "    mlflow.log_param(\"dataset.doi\",       z[\"doi\"])\n",
    "    mlflow.log_param(\"dataset.authors\",   json.dumps(z[\"authors\"]))\n",
    "    mlflow.log_param(\"dataset.published\", z[\"published\"])\n",
    "    mlflow.log_param(\"dataset.publisher\", z[\"publisher\"])\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832d0df-af0a-4eee-90d0-fab926e03e85",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "77402d80-22d1-4bed-9489-768958c3e9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 2) Load into a DataFrame ─────────────────────────────────────────────────\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "01309a7b-53d2-4df4-b334-0f0db8b03333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "target_col = df.columns[-1]      # e.g. \"species\"\n",
    "\n",
    "# 2) extract y as the Series of labels\n",
    "y = df[target_col]               # length == n_samples\n",
    "\n",
    "# 3) build X by dropping just that one column\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "# 4) drop any ID column (case-insensitive)\n",
    "id_cols = [c for c in X.columns if c.lower() == \"id\"]\n",
    "if id_cols:\n",
    "    X = X.drop(columns=id_cols)\n",
    "\n",
    "# 5) coerce numeric where possible\n",
    "for c in X.columns:\n",
    "    try:\n",
    "        X[c] = pd.to_numeric(X[c])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"Shapes:\", X.shape, y.shape)\n",
    "# → (150, 4) (150,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "11f5126d-6a03-48c6-9ecf-39ed0d43688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)  \n",
    "\n",
    "# now y_enc is a 1d numpy array of ints 0,1,2\n",
    "print(\"Classes:\", le.classes_)  \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "68d0a924-c65f-4a44-a5cc-bbb32d17e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 4) Cast feature columns to numeric where possible ─────────────────────────\n",
    "for col in X.columns:\n",
    "    try:\n",
    "        X[col] = pd.to_numeric(X[col])   # no errors=\"ignore\"\n",
    "    except ValueError:\n",
    "        # if it can’t be cast, just leave it as-is\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e17f39ce-3322-4626-83a6-079d304bbc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 5) Drop any “id” column (case-insensitive) ────────────────────────────────\n",
    "dropped = [c for c in X.columns if c.lower() == \"id\"]\n",
    "X = X.drop(columns=dropped, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "46f00853-206c-4fd1-b627-38115ae95a0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RQ1.2: Model Provenance Tracking in Jupyter Notebook using MLflow\n",
    "# Updated with automatic logging of environment, Git, model config, and FAIR-aligned metadata\n",
    "\n",
    "# ============================\n",
    "# ⚙️ Install Dependencies (if needed in Colab)\n",
    "# ============================\n",
    "# !pip install mlflow scikit-learn pandas numpy matplotlib seaborn shap requests GitPython\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3475b9b8-173d-4b5b-8913-de384dc60e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade threadpoolctl\n",
    "# !pip install setuptools\n",
    "# !pip install ace_tools \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "64674a5d-93d3-4557-8f5a-2c1babfcfb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 📦 Imports\n",
    "# ============================\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import psutil\n",
    "import platform\n",
    "import git\n",
    "from git import Repo\n",
    "from git import Repo, GitCommandError\n",
    "import mlflow\n",
    "import requests\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import subprocess\n",
    "import seaborn as sns\n",
    "from git import Repo, GitCommandError\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow.sklearn\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "# import setuptools\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, confusion_matrix,\n",
    "    precision_score, recall_score, f1_score, roc_curve\n",
    ")\n",
    "from mlflow import MlflowClient\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    RocCurveDisplay,\n",
    "    PrecisionRecallDisplay,\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "cbe91ec0-6447-4586-b7cc-2c1f74d4218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 📂 Setup MLflow\n",
    "# ============================\n",
    "project_dir = os.getcwd()\n",
    "mlflow.set_tracking_uri(\"mlrunlogs/mlflow.db\")\n",
    "mlflow.set_experiment(\"RandomForest-Iris-CSV\")\n",
    "# mlflow.sklearn.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "838dd233-25dc-4725-974d-4da89c257782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 🔄 Git Commit Hash\n",
    "# ============================\n",
    "repo_dir = \"C:/Users/reema/REPO\"\n",
    "previous_commit_repo = git.Repo(repo_dir)\n",
    "previous_commit_hash = previous_commit_repo.head.object.hexsha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9668451f-4352-4bdc-8b6b-bbe49074212a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/23 23:01:41 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2025/04/23 23:01:41 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n"
     ]
    }
   ],
   "source": [
    "# ─── 0) Make threadpoolctl safe so MLflow’s autologger won’t crash ───\n",
    "try:\n",
    "    import threadpoolctl\n",
    "    _orig = threadpoolctl.threadpool_info\n",
    "    def _safe_threadpool_info(*args, **kwargs):\n",
    "        try:\n",
    "            return _orig(*args, **kwargs)\n",
    "        except Exception:\n",
    "            return []\n",
    "    threadpoolctl.threadpool_info = _safe_threadpool_info\n",
    "except ImportError:\n",
    "    pass  # if threadpoolctl isn’t installed, autolog will skip unsupported versions\n",
    "\n",
    "# ─── 1) Enable generic autolog (will auto-patch sklearn under the hood) ───\n",
    "import mlflow\n",
    "mlflow.autolog(\n",
    "    log_input_examples=True,\n",
    "    log_model_signatures=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "14c62f08-a116-4060-9689-f69968e9f240",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=80): Max retries exceeded with url: /api/database/c3a42d17-42b7-43c9-a504-2363fb4c9c8d (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001EF2D576A10>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 85\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:415\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 415\u001b[0m         \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhttplib_request_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:244\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers)\u001b[0m\n\u001b[0;32m    243\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[1;32m--> 244\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1286\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1286\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1332\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1331\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1332\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1281\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1281\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1041\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1041\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1044\u001b[0m \n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:979\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 979\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x000001EF2D576A10>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    796\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 798\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    801\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=80): Max retries exceeded with url: /api/database/c3a42d17-42b7-43c9-a504-2363fb4c9c8d (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001EF2D576A10>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[206], line 16\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[38;5;66;03m# 4) log it as a single JSON artifact\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# mlflow.log_dict(public_datasetRepository_metadata, \"public_datasetRepository_metadata.json\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#Datasbase info logging\u001b[39;00m\n\u001b[0;32m     15\u001b[0m db_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc3a42d17-42b7-43c9-a504-2363fb4c9c8d\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m db_meta \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_db_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m log_db_metadata(db_meta)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#OAI metadata logging from api endpoint\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# log as tags\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m, in \u001b[0;36mfetch_db_metadata\u001b[1;34m(db_id)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_db_metadata\u001b[39m(db_id: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m      7\u001b[0m     url \u001b[38;5;241m=\u001b[39m DB_API\u001b[38;5;241m.\u001b[39mformat(db_id\u001b[38;5;241m=\u001b[39mdb_id)\n\u001b[1;32m----> 8\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     resp\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=80): Max retries exceeded with url: /api/database/c3a42d17-42b7-43c9-a504-2363fb4c9c8d (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001EF2D576A10>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================\n",
    "# 🚀 Start MLflow Run CURRENT BACKUP\n",
    "# ============================\n",
    "with mlflow.start_run() as run:\n",
    "    ##########################\n",
    "\n",
    "    meta = fetch_and_log_dataset_metadata_nested(\n",
    "            \"https://doi.org/10.5281/zenodo.1404173\",\n",
    "           \n",
    "        )\n",
    "      # 4) log it as a single JSON artifact\n",
    "    # mlflow.log_dict(public_datasetRepository_metadata, \"public_datasetRepository_metadata.json\")\n",
    "\n",
    "    #Datasbase info logging\n",
    "    db_id = \"c3a42d17-42b7-43c9-a504-2363fb4c9c8d\"\n",
    "    db_meta = fetch_db_metadata(db_id)\n",
    "    log_db_metadata(db_meta)\n",
    "\n",
    "    #OAI metadata logging from api endpoint\n",
    "    # log as tags\n",
    "    mlflow.set_tag(\"dbrepo.repository_name\", repo_name)\n",
    "    mlflow.set_tag(\"dbrepo.base_url\",       base_url)\n",
    "    mlflow.set_tag(\"dbrepo.protocol_version\", protocol)\n",
    "    mlflow.set_tag(\"dbrepo.admin_email\",     admin_email)\n",
    "    mlflow.set_tag(\"dbrepo.granularity\",     gran)\n",
    "\n",
    "    #From history API logging\n",
    "    # provenance tags\n",
    "    mlflow.set_tag(\"dbrepo.table_last_modified\", ts_last)\n",
    "\n",
    "    # row-count metrics\n",
    "    mlflow.log_metric(\"dbrepo.row_count_start\", count_start)\n",
    "    mlflow.log_metric(\"dbrepo.row_count_end\",   count_end)\n",
    "\n",
    "    # change-event metrics\n",
    "    mlflow.log_metric(\"dbrepo.num_inserts\", n_insert)\n",
    "    mlflow.log_metric(\"dbrepo.num_deletes\", n_delete)\n",
    "    \n",
    "    # 2) Capture raw metadata\n",
    "    mlflow.set_tag(\"data_source\", API_URL)\n",
    "    mlflow.log_param(\"retrieval_time\", datetime.utcnow().isoformat())\n",
    "    mlflow.log_param(\"n_records\", len(df))\n",
    "    mlflow.log_param(\"columns_raw\", df.columns.tolist())\n",
    "    mlflow.log_param(\"dropped_columns\", id_cols)\n",
    "\n",
    "    # 4) Post‐processing metadata\n",
    "    mlflow.log_param(\"n_features_final\", X.shape[1])\n",
    "    mlflow.log_param(\"feature_names\", X.columns.tolist())\n",
    "    mlflow.set_tag(\"target_name\", y)\n",
    "\n",
    "    #Lable encoding\n",
    "    label_map = { int(idx): cls for idx, cls in enumerate(le.classes_) }\n",
    "\n",
    "    with open(\"label_mapping.json\", \"w\") as fp:\n",
    "        json.dump(label_map, fp, indent=2)\n",
    "    mlflow.log_artifact(\"label_mapping.json\")\n",
    "   \n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_name = f\"RandomForest_Iris_v{ts}\"\n",
    "    mlflow.set_tag(\"model_name\",model_name)\n",
    "    \n",
    "    train_start_ts = datetime.now().isoformat()\n",
    "    mlflow.set_tag(\"training_start_time\", train_start_ts)\n",
    "\n",
    "    test_size    = 0.2\n",
    "    random_state = 42\n",
    "    # 📈 Model Training\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # ── 2) Log dataset split params ──\n",
    "    mlflow.log_param(\"test_size\", test_size)\n",
    "    mlflow.log_param(\"random_state\", random_state)\n",
    "    mlflow.log_param(\"n_train_samples\", X_train.shape[0])\n",
    "    mlflow.log_param(\"n_test_samples\",  X_test.shape[0])\n",
    "    mlflow.log_param(\"n_features\",      X_train.shape[1])\n",
    "\n",
    "     # 1) Define a more complex hyperparameter dict\n",
    "    hyperparams = {\n",
    "        \"n_estimators\":       200,\n",
    "        \"criterion\":          \"entropy\",\n",
    "        \"max_depth\":          12,\n",
    "        \"min_samples_split\":  5,\n",
    "        \"min_samples_leaf\":   2,\n",
    "        \"max_features\":       \"sqrt\",\n",
    "        \"bootstrap\":          True,\n",
    "        \"oob_score\":          False,\n",
    "        \"class_weight\":       None,\n",
    "        \"random_state\":       42,\n",
    "        \"verbose\":            1,\n",
    "        \"n_jobs\":             -1\n",
    "    }\n",
    "\n",
    "    # 2) Log them ALL at once\n",
    "    mlflow.log_params(hyperparams)\n",
    "    model = RandomForestClassifier(**hyperparams)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_end_ts = datetime.now().isoformat()\n",
    "    mlflow.set_tag(\"training_end_time\", train_end_ts)\n",
    "\n",
    "     # ── 6) Predict & log metrics ──\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba, multi_class=\"ovr\")\n",
    "    prec = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    rec  = recall_score(y_test,    y_pred, average=\"macro\")\n",
    "    f1   = f1_score(y_test,      y_pred, average=\"macro\")\n",
    "    \n",
    "    mlflow.log_metric(\"precision_macro\", prec)\n",
    "    mlflow.log_metric(\"recall_macro\",    rec)\n",
    "    mlflow.log_metric(\"f1_macro\",        f1)\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.log_metric(\"roc_auc\",   auc)\n",
    "\n",
    "    # ✅ Log Environment Automatically\n",
    "    mlflow.log_params({\n",
    "        \"python_version\": platform.python_version(),\n",
    "        \"os_platform\": f\"{platform.system()} {platform.release()}\",\n",
    "        \"sklearn_version\": sklearn.__version__,\n",
    "        \"pandas_version\": pd.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"matplotlib_version\": matplotlib.__version__,\n",
    "        \"seaborn_version\": sns.__version__,\n",
    "        \"shap_version\": shap.__version__,\n",
    "    })\n",
    "\n",
    "    # ✅ Git and Notebook Metadata\n",
    "    mlflow.set_tag(\"notebook_name\", \"RQ1.ipynb\")\n",
    "\n",
    "    # ✅ Dataset Metadata Tags\n",
    "    mlflow.set_tag(\"dataset_name\", \"Iris\") #TODO\n",
    "    mlflow.set_tag(\"dataset_version\", \"1.0.0\") #TODO\n",
    "    mlflow.set_tag(\"dataset_id\", \"iris_local\") #TODO\n",
    "\n",
    "\n",
    "    # ─── 2) Create a folder for this run’s plots ───\n",
    "    plot_dir = os.path.join(\"plots\", model_name)\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "    # 1) Feature Importance Bar Chart\n",
    "    importances = model.feature_importances_\n",
    "    # if X_train is a DataFrame, grab column names; otherwise auto-name them f0,f1,...\n",
    "    try:\n",
    "        feature_names = X_train.columns\n",
    "    except AttributeError:\n",
    "        feature_names = [f\"f{i}\" for i in range(X_train.shape[1])]\n",
    "    fi_path = os.path.join(plot_dir, \"feature_importances.png\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=importances, y=feature_names)\n",
    "    plt.title(\"Feature Importances\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fi_path)\n",
    "    mlflow.log_artifact(fi_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# 2) Multi-class ROC Curves\n",
    "# Binarize labels for one-vs-rest\n",
    "    classes = np.unique(y_test)\n",
    "    y_test_bin = label_binarize(y_test, classes=classes)\n",
    "    \n",
    "    for idx, cls in enumerate(classes):\n",
    "        disp = RocCurveDisplay.from_predictions(\n",
    "            y_test_bin[:, idx], \n",
    "            y_proba[:, idx],\n",
    "            name=f\"ROC for class {cls}\"\n",
    "        )\n",
    "        roc_path = os.path.join(plot_dir, f\"roc_curve_cls_{cls}.png\")\n",
    "        disp.figure_.savefig(roc_path)\n",
    "        mlflow.log_artifact(roc_path)\n",
    "        plt.close(disp.figure_)\n",
    "\n",
    "\n",
    "    # 3) Multi-class Precision-Recall Curves\n",
    "    for idx, cls in enumerate(classes):\n",
    "        disp = PrecisionRecallDisplay.from_predictions(\n",
    "            y_test_bin[:, idx], \n",
    "            y_proba[:, idx],\n",
    "            name=f\"PR curve for class {cls}\"\n",
    "        )\n",
    "        pr_path = os.path.join(plot_dir, f\"pr_curve_cls_{cls}.png\")\n",
    "        disp.figure_.savefig(pr_path)\n",
    "        mlflow.log_artifact(pr_path)\n",
    "        plt.close(disp.figure_)\n",
    "        \n",
    "    # ✅ Confusion Matrix Plot\n",
    "    cm_path = os.path.join(plot_dir, \"confusion_matrix.png\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.savefig(cm_path)\n",
    "    mlflow.log_artifact(cm_path)\n",
    "\n",
    "    # ✅ SHAP Summary\n",
    "    shap_path = os.path.join(plot_dir, \"shap_summary.png\")\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    shap.summary_plot(shap_values, X_test, show=False)\n",
    "    plt.savefig(shap_path)\n",
    "    mlflow.log_artifact(shap_path)\n",
    "\n",
    "\n",
    "    \n",
    "    # ─── 1) Build a .pkl filename (you can include your model_name for clarity)\n",
    "    pkl_path = f\"{model_name}.pkl\"\n",
    "    \n",
    "    # ─── 2) Serialize your trained model to disk\n",
    "    with open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    # ─── 3) Log that pickle file as an MLflow artifact\n",
    "    #     It will appear under Artifacts → models/RandomForest_Iris_vYYYYMMDD_HHMMSS.pkl\n",
    "    mlflow.log_artifact(pkl_path, artifact_path=model_name)\n",
    "        \n",
    "    def get_latest_commit_hash(repo_path=\".\"):\n",
    "        # returns the full SHA of HEAD\n",
    "        res = subprocess.run(\n",
    "            [\"git\", \"-C\", repo_path, \"rev-parse\", \"HEAD\"],\n",
    "            capture_output=True, text=True, check=True)\n",
    "        \n",
    "        return res.stdout.strip()\n",
    "\n",
    "    def get_remote_url(repo_path=\".\", remote=\"origin\"):\n",
    "        # returns something like git@github.com:user/repo.git or https://...\n",
    "        res = subprocess.run(\n",
    "            [\"git\", \"-C\", repo_path, \"config\", \"--get\", f\"remote.{remote}.url\"],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        return res.stdout.strip()\n",
    "    \n",
    "    def make_commit_link(remote_url, commit_hash):\n",
    "        # handle GitHub/GitLab convention; strip “.git” if present\n",
    "        base = remote_url.rstrip(\".git\")\n",
    "        # if SSH form (git@github.com:owner/repo), convert to https\n",
    "        if base.startswith(\"git@\"):\n",
    "            base = base.replace(\":\", \"/\").replace(\"git@\", \"https://\")\n",
    "        return f\"{base}/commit/{commit_hash}\"\n",
    "\n",
    "    \n",
    "    def simple_commit_and_push_and_log(repo_path=\".\", message=\"Auto commit\", remote=\"origin\", branch=\"main\"):\n",
    "    # 1) Check for changes\n",
    "        status = subprocess.run(\n",
    "            [\"git\", \"-C\", repo_path, \"status\", \"--porcelain\"],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        if not status.stdout.strip():\n",
    "            print(\"🟡 No changes to commit.\")\n",
    "            return None, None\n",
    "    \n",
    "        # 2) Stage everything\n",
    "        add = subprocess.run(\n",
    "            [\"git\", \"-C\", repo_path, \"add\", \"--all\"],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        if add.returncode:\n",
    "            print(\"❌ git add failed:\\n\", add.stderr)\n",
    "            return None, None\n",
    "    \n",
    "        # 3) Commit\n",
    "        commit = subprocess.run(\n",
    "            [\"git\", \"-C\", repo_path, \"commit\", \"-m\", message],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        if commit.returncode:\n",
    "            print(\"❌ git commit failed:\\n\", commit.stderr)\n",
    "            return None, None\n",
    "        print(\"✅ Commit successful.\")\n",
    "    \n",
    "        # 4) Push\n",
    "        push = subprocess.run(\n",
    "            [\"git\", \"-C\", repo_path, \"push\", \"-u\", remote, branch],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        if push.returncode:\n",
    "            print(\"❌ git push failed:\\n\", push.stderr)\n",
    "        else:\n",
    "            print(\"🚀 Push successful.\")\n",
    "    \n",
    "        # 5) Retrieve hash & remote URL\n",
    "        sha = get_latest_commit_hash(repo_path)\n",
    "        url = get_remote_url(repo_path, remote)\n",
    "        link = make_commit_link(url, sha)\n",
    "    \n",
    "        return sha, link\n",
    "    \n",
    "      \n",
    "    sha, link = simple_commit_and_push_and_log(\n",
    "        repo_path=\".\",\n",
    "        message=\"Auto commit after successful training\"\n",
    "    )\n",
    "    if sha and link:\n",
    "        diff_text = subprocess.check_output(\n",
    "            [\"git\", \"-C\", \".\", \"diff\", previous_commit_hash, sha],\n",
    "            encoding=\"utf-8\",\n",
    "            errors=\"ignore\"    # or \"replace\"\n",
    "        )\n",
    "                \n",
    "        # 1) Get your repo’s remote URL and normalize to HTTPS\n",
    "        remote_url = subprocess.check_output(\n",
    "            [\"git\", \"config\", \"--get\", \"remote.origin.url\"],\n",
    "            text=True\n",
    "        ).strip().rstrip(\".git\")\n",
    "        if remote_url.startswith(\"git@\"):\n",
    "            # git@github.com:owner/repo.git → https://github.com/owner/repo\n",
    "            remote_url = remote_url.replace(\":\", \"/\").replace(\"git@\", \"https://\")\n",
    "        \n",
    "        # 2) Build commit URLs\n",
    "        previous_commit_url  = f\"{remote_url}/commit/{previous_commit_hash}\"\n",
    "        current_commit_url = f\"{remote_url}/commit/{sha}\"\n",
    "        diff_data = {\n",
    "            \"previous_commit\":  previous_commit_hash,\n",
    "            \"previous_commit_url\":previous_commit_url,\n",
    "            \"current_commit_url\":current_commit_url,\n",
    "            \"current_commit\": sha,\n",
    "            \"diff\": diff_text\n",
    "        }\n",
    "        mlflow.log_dict(\n",
    "            diff_data,\n",
    "            artifact_file=\"commit_diff.json\"\n",
    "        )\n",
    "        mlflow.set_tag(\"git_previous_commit_hash\", previous_commit_hash)\n",
    "        mlflow.set_tag(\"git_current_commit_hash\", sha)\n",
    "        mlflow.set_tag(\"git__current_commit_url\", link) \n",
    "\n",
    "\n",
    "    client   = MlflowClient()\n",
    "    run_id    = run.info.run_id\n",
    "    run_info  = client.get_run(run_id).info\n",
    "    run_data  = client.get_run(run_id).data\n",
    "    \n",
    "    # 1) params, metrics, tags\n",
    "    params  = dict(run_data.params)\n",
    "    metrics = dict(run_data.metrics)\n",
    "    tags    = dict(run_data.tags)\n",
    "\n",
    "    # (4) List artifacts under a specific subfolder\n",
    "    # artifact_paths = [af.path for af in client.list_artifacts(run_id)]\n",
    "    run_meta     = client.get_run(run_id).info\n",
    "    artifact_uri = run_meta.artifact_uri  # base URI for all artifacts\n",
    "    \n",
    "    artifact_meta = []\n",
    "    \n",
    "    def _gather(path=\"\"):\n",
    "        for af in client.list_artifacts(run_id, path):\n",
    "            # If it’s a directory, recurse\n",
    "            if af.is_dir:\n",
    "                _gather(af.path)\n",
    "                continue\n",
    "    \n",
    "            rel_path = af.path\n",
    "            uri      = f\"{artifact_uri}/{rel_path}\"\n",
    "            lower    = rel_path.lower()\n",
    "    \n",
    "            # 1) Text files → download & embed contents\n",
    "            if lower.endswith((\".json\", \".txt\", \".patch\")):\n",
    "                local = client.download_artifacts(run_id, rel_path)\n",
    "                with open(local, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                artifact_meta.append({\n",
    "                    \"path\":    rel_path,\n",
    "                    \"type\":    \"text\",\n",
    "                    \"content\": content\n",
    "                })\n",
    "    \n",
    "            # 2) Images → surface a clickable URI\n",
    "            elif lower.endswith((\".png\", \".jpg\", \".jpeg\", \".svg\")):\n",
    "                artifact_meta.append({\n",
    "                    \"path\": rel_path,\n",
    "                    \"type\": \"image\",\n",
    "                    \"uri\":  uri\n",
    "                })\n",
    "    \n",
    "            # 3) Everything else → just link\n",
    "            else:\n",
    "                artifact_meta.append({\n",
    "                    \"path\": rel_path,\n",
    "                    \"type\": \"other\",\n",
    "                    \"uri\":  uri\n",
    "                })\n",
    "    \n",
    "    # Run the gather\n",
    "    _gather()\n",
    "     \n",
    "    summary = {\n",
    "        \"run_id\":         run_id,\n",
    "        \"run_name\": run_info.run_name,\n",
    "        \"experiment_id\":  run_info.experiment_id,\n",
    "        \"start_time\":     run_info.start_time,\n",
    "        \"end_time\":       run_info.end_time,\n",
    "        \"params\":         params,\n",
    "        \"metrics\":        metrics,\n",
    "        \"tags\":           tags,\n",
    "        \"artifacts\":      artifact_meta\n",
    "    }\n",
    "    # 1) Create (or reuse) a base folder for run summaries\n",
    "    base_dir = \"run_summaries\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "   # 2) Pick next numeric folder\n",
    "    existing = [\n",
    "        d for d in os.listdir(base_dir)\n",
    "        if os.path.isdir(os.path.join(base_dir, d)) and d.isdigit()\n",
    "    ]\n",
    "    next_num = max(map(int, existing), default=0) + 1\n",
    "\n",
    "    # 1) Determine notebook directory (where your .ipynb lives)\n",
    "    notebook_dir = os.getcwd()\n",
    "    \n",
    "    # 2) Create a subfolder for this model\n",
    "    summary_dir = os.path.join(os.getcwd(), \"MODEL_PROVENANCE\")\n",
    "    os.makedirs(summary_dir, exist_ok=True)\n",
    "    \n",
    "   # 2) Pick a filename based on your model_name\n",
    "    summary_filename   = f\"{model_name}_run_summary.json\"\n",
    "    summary_local_path = os.path.join(summary_dir, summary_filename)\n",
    "   # 3) Write the JSON locally\n",
    "    with open(summary_local_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    # 4) (Optional) Mirror it into MLflow artifacts under a single folder\n",
    "    mlflow.log_artifact(summary_local_path, artifact_path=\"run_summaries\")\n",
    "\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461eca1d-e33c-4398-835a-64f9ea850469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rdflib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5e3bbb-0288-47d0-9dc4-2855d7e4801a",
   "metadata": {},
   "source": [
    "1. Standards-compliant export (JSON-LD + Turtle)\n",
    "I already have your plain run_summary.json , wrap it in a JSON-LD context that maps your fields into PROV-O terms, then use rdflib to emit Turtle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5cf88da4-69f8-4982-a594-28cf25e4f79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted RandomForest_Iris_v20250423_230422_run_summary.json → RandomForest_Iris_v20250423_230422.jsonld, RandomForest_Iris_v20250423_230422.ttl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from datetime import datetime, timezone\n",
    "from rdflib import Graph\n",
    "\n",
    "def iso8601(ms):\n",
    "    \"\"\"Convert milliseconds since epoch to ISO8601 UTC.\"\"\"\n",
    "    return datetime.fromtimestamp(ms / 1000, tz=timezone.utc).isoformat()\n",
    "\n",
    "for json_path in glob.glob(\"MODEL_PROVENANCE/*_run_summary.json\"):\n",
    "    basename   = os.path.basename(json_path)\n",
    "    model_name = basename.rsplit(\"_run_summary.json\", 1)[0]\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        summary = json.load(f)\n",
    "\n",
    "    #–– Minimal override context: keep all your flat fields as-is,\n",
    "    #–– and only map the actual PROV terms to their IRIs.\n",
    "    ctx = {\n",
    "        # keep these flat\n",
    "        \"run_id\":       { \"@id\": \"run_id\" },\n",
    "        \"run_name\":     { \"@id\": \"run_name\" },\n",
    "        \"experiment_id\":{ \"@id\": \"experiment_id\" },\n",
    "        \"params\":       { \"@id\": \"params\" },\n",
    "        \"metrics\":      { \"@id\": \"metrics\" },\n",
    "        \"artifacts\":    { \"@id\": \"artifacts\" },\n",
    "        \"tags\":         { \"@id\": \"tags\" },\n",
    "\n",
    "        # provenance namespace\n",
    "        \"prov\": \"http://www.w3.org/ns/prov#\",\n",
    "        \"xsd\":  \"http://www.w3.org/2001/XMLSchema#\",\n",
    "\n",
    "        # map your timestamp fields into PROV\n",
    "        \"start_time\": { \"@id\": \"prov:startedAtTime\", \"@type\": \"xsd:dateTime\" },\n",
    "        \"end_time\":   { \"@id\": \"prov:endedAtTime\",   \"@type\": \"xsd:dateTime\" },\n",
    "\n",
    "        # PROV-used/generated\n",
    "        \"used\":      { \"@id\": \"prov:used\",      \"@type\": \"@id\" },\n",
    "        \"generated\": { \"@id\": \"prov:generated\", \"@type\": \"@id\" },\n",
    "\n",
    "        # JSON-LD boilerplate\n",
    "        \"@id\":   \"@id\",\n",
    "        \"@type\": \"@type\"\n",
    "    }\n",
    "\n",
    "    #–– Build JSON-LD document, re-using your original keys verbatim\n",
    "    doc = {\n",
    "        \"@context\":      ctx,\n",
    "        \"run_id\":        summary[\"run_id\"],\n",
    "        \"run_name\":      summary.get(\"run_name\"),\n",
    "        \"experiment_id\": summary.get(\"experiment_id\"),\n",
    "        \"params\":        summary.get(\"params\", {}),\n",
    "        \"metrics\":       summary.get(\"metrics\", {}),\n",
    "        \"artifacts\":     summary.get(\"artifacts\", []),\n",
    "        \"tags\":          summary.get(\"tags\", {}),\n",
    "\n",
    "        # PROV fields:\n",
    "        \"start_time\": iso8601(summary[\"start_time\"])\n",
    "    }\n",
    "\n",
    "    if summary.get(\"end_time\") is not None:\n",
    "        doc[\"end_time\"] = iso8601(summary[\"end_time\"])\n",
    "\n",
    "    # for used/generated, just point at your dataset/model URIs\n",
    "    # (or blank-node them if you prefer richer structure)\n",
    "    doc[\"used\"] = summary.get(\"tags\", {}).get(\"dataset_uri\") or []\n",
    "    doc[\"generated\"] = [\n",
    "        art.get(\"uri\") or art.get(\"path\")\n",
    "        for art in summary.get(\"artifacts\", [])\n",
    "    ]\n",
    "\n",
    "    #–– write JSON-LD\n",
    "    out_jsonld = os.path.join(\"MODEL_PROVENANCE\", f\"{model_name}.jsonld\")\n",
    "    with open(out_jsonld, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(doc, f, indent=2)\n",
    "\n",
    "    #–– parse & serialize to Turtle\n",
    "    g = Graph().parse(data=json.dumps(doc), format=\"json-ld\")\n",
    "    out_ttl = os.path.join(\"MODEL_PROVENANCE\", f\"{model_name}.ttl\")\n",
    "    g.serialize(destination=out_ttl, format=\"turtle\")\n",
    "\n",
    "    print(f\"Converted {basename} → {os.path.basename(out_jsonld)}, {os.path.basename(out_ttl)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d6d524-01da-4f20-8131-0d4a3ac005e2",
   "metadata": {},
   "source": [
    "This code programatically, finds diff between generated Json file and created JsonLD and .TTL file to make it easier to understand if there is any discrepency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "77a420c0-230d-41c0-9b63-f3dbbca1e670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== JSON-LD vs TTL ==\n",
      "Change summary:\n",
      "type\n",
      "changed    1 \n",
      "\n",
      "First 10 ‘changed’ entries:\n",
      "Top-level adds/removes:\n",
      "Empty DataFrame\n",
      "Columns: [path, type, a, b]\n",
      "Index: []\n",
      "\n",
      "== JSON vs JSON-LD ==\n",
      "Change summary:\n",
      "type\n",
      "added      3\n",
      "changed    1\n",
      "removed    1 \n",
      "\n",
      "First 10 ‘changed’ entries:\n",
      "Top-level adds/removes:\n",
      "Empty DataFrame\n",
      "Columns: [path, type, a, b]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from rdflib import Graph\n",
    "import pandas as pd\n",
    "\n",
    "def load_as_dict(path):\n",
    "    if path.endswith((\".ttl\", \".turtle\")):\n",
    "        g = Graph()\n",
    "        g.parse(path, format=\"turtle\")\n",
    "        # normalize to JSON-LD dict\n",
    "        return json.loads(g.serialize(format=\"json-ld\", indent=2))\n",
    "    else:\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "def compare_json(a, b, path=\"\"):\n",
    "    diffs = []\n",
    "    if isinstance(a, dict) and isinstance(b, dict):\n",
    "        all_keys = set(a) | set(b)\n",
    "        for k in all_keys:\n",
    "            new_path = f\"{path}/{k}\" if path else k\n",
    "            if k not in a:\n",
    "                diffs.append({\"path\": new_path, \"type\": \"added\",   \"a\": None,    \"b\": b[k]})\n",
    "            elif k not in b:\n",
    "                diffs.append({\"path\": new_path, \"type\": \"removed\", \"a\": a[k],   \"b\": None})\n",
    "            else:\n",
    "                diffs.extend(compare_json(a[k], b[k], new_path))\n",
    "    elif isinstance(a, list) and isinstance(b, list):\n",
    "        for i, (ia, ib) in enumerate(zip(a, b)):\n",
    "            diffs.extend(compare_json(ia, ib, f\"{path}[{i}]\"))\n",
    "        # handle length mismatches\n",
    "        if len(a) < len(b):\n",
    "            for i in range(len(a), len(b)):\n",
    "                diffs.append({\"path\": f\"{path}[{i}]\", \"type\": \"added\",   \"a\": None,  \"b\": b[i]})\n",
    "        elif len(a) > len(b):\n",
    "            for i in range(len(b), len(a)):\n",
    "                diffs.append({\"path\": f\"{path}[{i}]\", \"type\": \"removed\", \"a\": a[i],  \"b\": None})\n",
    "    else:\n",
    "        if a != b:\n",
    "            diffs.append({\"path\": path, \"type\": \"changed\", \"a\": a, \"b\": b})\n",
    "    return diffs\n",
    "\n",
    "# --- Usage example -----------------------------------------------\n",
    "\n",
    "# Compare JSON-LD vs Turtle:\n",
    "a = load_as_dict(\"MODEL_PROVENANCE/RandomForest_Iris_v20250423_230422_run_summary.json\")\n",
    "b = load_as_dict(\"MODEL_PROVENANCE/RandomForest_Iris_v20250423_230422.ttl\")\n",
    "diffs_jsonld_vs_ttl = compare_json(a, b)\n",
    "\n",
    "# Compare JSON vs JSON-LD:\n",
    "c = load_as_dict(\"MODEL_PROVENANCE/RandomForest_Iris_v20250423_230422_run_summary.json\")\n",
    "d = load_as_dict(\"MODEL_PROVENANCE/RandomForest_Iris_v20250423_230422.jsonld\")\n",
    "diffs_json_vs_jsonld = compare_json(c, d)\n",
    "\n",
    "# Build DataFrames for interactive inspection\n",
    "df1 = pd.DataFrame(diffs_jsonld_vs_ttl)\n",
    "df2 = pd.DataFrame(diffs_json_vs_jsonld)\n",
    "\n",
    "# --- Summaries & Filtering ---------------------------------------\n",
    "\n",
    "def summarize_and_preview(df, preview_n=10):\n",
    "    print(\"Change summary:\")\n",
    "    print(df['type'].value_counts().to_string(), \"\\n\")\n",
    "    \n",
    "    print(f\"First {preview_n} ‘changed’ entries:\")\n",
    "    # print(df[df['type']==\"changed\"].head(preview_n).to_string(index=False), \"\\n\")\n",
    "    \n",
    "    # Top‐level (one slash) adds/removes\n",
    "    top = df[df['path'].str.count(\"/\") == 1]\n",
    "    print(\"Top-level adds/removes:\")\n",
    "    print(top[top['type'].isin(['added','removed'])].to_string(index=False))\n",
    "\n",
    "print(\"== JSON-LD vs TTL ==\")\n",
    "summarize_and_preview(df1)\n",
    "\n",
    "print(\"\\n== JSON vs JSON-LD ==\")\n",
    "summarize_and_preview(df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "41af9d6e-c683-45f9-bac1-296611b4d0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed in JSON-LD comparison:\n",
      "    path\n",
      "end_time\n",
      "\n",
      "Added in JSON-LD comparison:\n",
      "     path\n",
      " @context\n",
      "generated\n",
      "     used\n"
     ]
    }
   ],
   "source": [
    "# show all the removed paths (in JSON but not in JSON-LD)\n",
    "print(\"Removed in JSON-LD comparison:\")\n",
    "print(df2[df2['type']==\"removed\"][['path']].to_string(index=False))\n",
    "\n",
    "# show all the added paths (in JSON-LD but not in JSON)\n",
    "print(\"\\nAdded in JSON-LD comparison:\")\n",
    "print(df2[df2['type']==\"added\"][['path']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f0d6f92a-5cd9-4c78-9c2a-0cd3247137c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed in .ttl comparison:\n",
      "Empty DataFrame\n",
      "Columns: [path]\n",
      "Index: []\n",
      "\n",
      "Added in .ttl comparison:\n",
      "Empty DataFrame\n",
      "Columns: [path]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# show all the removed paths (in JSON but not in JSON-LD)\n",
    "print(\"Removed in .ttl comparison:\")\n",
    "print(df1[df1['type']==\"removed\"][['path']].to_string(index=False))\n",
    "\n",
    "# show all the added paths (in JSON-LD but not in JSON)\n",
    "print(\"\\nAdded in .ttl comparison:\")\n",
    "print(df1[df1['type']==\"added\"][['path']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efd0d0-9277-4efa-88cf-d2fd1b90d74c",
   "metadata": {},
   "source": [
    "Checks for completeness and mapping and time taken, needs work #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "165a13eb-7679-4f4c-b346-24f25da72cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "── RandomForest_Iris_v20250423_230422 diffs ──\n",
      "  • JSON → JSON-LD: 17 differences\n",
      "  • JSON-LD → TTL → JSON-LD: 1 differences\n",
      "\n",
      "1/1 runs passed completeness checks (100.0%).\n",
      "\n",
      "Mapping integrity: 0/1 runs have zero diffs — 0.0%\n",
      "Overall quality score: 50.0%\n",
      "\n",
      "Benchmarking train_and_log() overhead:\n",
      "  • No MLflow : 0.501s\n",
      "  • With MLflow: 0.601s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from rdflib import Graph\n",
    "# from your_compare_module import compare_json  # ← your existing compare_json()\n",
    "\n",
    "# ── User configuration ─────────────────────────────────────────────────────────\n",
    "\n",
    "# Which keys must appear in every run_summary.json?\n",
    "REQUIRED_TOPLEVEL = {\n",
    "    \"run_id\", \"start_time\", \"end_time\",\n",
    "    \"params\", \"metrics\", \"tags\", \"artifacts\"\n",
    "}\n",
    "\n",
    "# A couple of sub-fields we also want to spot-check:\n",
    "REQUIRED_PARAMS  = {\"random_state\"}\n",
    "REQUIRED_METRICS = {\"accuracy\"}\n",
    "\n",
    "JSON_SUMMARIES = glob.glob(\"MODEL_PROVENANCE/*_run_summary.json\")\n",
    "\n",
    "\n",
    "# ── Helpers ────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def iso8601(ms):\n",
    "    return datetime.fromtimestamp(ms/1000, tz=timezone.utc).isoformat()\n",
    "\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def write_json(path, obj):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "\n",
    "def convert_to_jsonld_and_ttl(summary, basename):\n",
    "    # build @context\n",
    "    ctx = {\n",
    "        \"prov\":    \"http://www.w3.org/ns/prov#\",\n",
    "        \"xsd\":     \"http://www.w3.org/2001/XMLSchema#\",\n",
    "        \"run\":     \"prov:Activity\",\n",
    "        \"start\":   \"prov:startedAtTime\",\n",
    "        \"end\":     \"prov:endedAtTime\",\n",
    "        \"used\":    \"prov:used\",\n",
    "        \"gen\":     \"prov:generated\",\n",
    "        \"param\":   \"prov:hadParameter\",\n",
    "        \"metric\":  \"prov:hadQuality\",\n",
    "        \"entity\":  \"prov:Entity\",\n",
    "        \"label\":   \"prov:label\",\n",
    "        \"value\":   \"prov:value\",\n",
    "        \"version\": \"prov:hadRevision\",\n",
    "        \"id\":      \"@id\",\n",
    "        \"type\":    \"@type\"\n",
    "    }\n",
    "\n",
    "    jsonld = {\n",
    "        \"@context\": ctx,\n",
    "        \"@id\":      f\"urn:run:{summary['run_id']}\",\n",
    "        \"@type\":    \"run\",\n",
    "        \"start\": {\n",
    "            \"@type\":  \"xsd:dateTime\",\n",
    "            \"@value\": iso8601(summary[\"start_time\"])\n",
    "        }\n",
    "    }\n",
    "    if summary.get(\"end_time\") is not None:\n",
    "        jsonld[\"end\"] = {\n",
    "            \"@type\":  \"xsd:dateTime\",\n",
    "            \"@value\": iso8601(summary[\"end_time\"])\n",
    "        }\n",
    "\n",
    "    # params\n",
    "    jsonld[\"param\"] = [\n",
    "        {\"@type\":\"entity\",\"label\":k,\"value\":str(v)}\n",
    "        for k,v in summary.get(\"params\",{}).items()\n",
    "    ]\n",
    "    # metrics\n",
    "    jsonld[\"metric\"] = [\n",
    "        {\"@type\":\"entity\",\"label\":k,\n",
    "         \"value\":{\"@type\":\"xsd:decimal\",\"@value\":v}}\n",
    "        for k,v in summary.get(\"metrics\",{}).items()\n",
    "    ]\n",
    "    # artifacts\n",
    "    jsonld[\"gen\"] = [\n",
    "        {\n",
    "            \"@type\":\"entity\",\n",
    "            \"label\": art.get(\"path\") or art.get(\"label\"),\n",
    "            \"prov:location\": (\n",
    "                art.get(\"uri\")\n",
    "                or (art.get(\"content\",\"\")[:30]+\"…\")\n",
    "                if isinstance(art.get(\"content\"),str)\n",
    "                else \"\"\n",
    "            )\n",
    "        }\n",
    "        for art in summary.get(\"artifacts\",[])\n",
    "    ]\n",
    "    # dataset used\n",
    "    jsonld[\"used\"] = {\n",
    "        \"@type\":\"entity\",\n",
    "        \"label\": summary[\"tags\"].get(\"dataset_name\"),\n",
    "        \"version\": summary[\"tags\"].get(\"dataset_version\")\n",
    "    }\n",
    "\n",
    "    # write JSON-LD\n",
    "    out_jsonld = f\"MODEL_PROVENANCE/{basename}.jsonld\"\n",
    "    write_json(out_jsonld, jsonld)\n",
    "\n",
    "    # serialize TTL\n",
    "    g = Graph().parse(data=json.dumps(jsonld), format=\"json-ld\")\n",
    "    out_ttl = f\"MODEL_PROVENANCE/{basename}.ttl\"\n",
    "    g.serialize(destination=out_ttl, format=\"turtle\")\n",
    "\n",
    "    return out_jsonld, out_ttl\n",
    "\n",
    "\n",
    "def normalize_jsonld(js):\n",
    "    \"\"\"Simple deep-sort so compare_json doesn’t trip over ordering.\"\"\"\n",
    "    if isinstance(js, dict):\n",
    "        return {k: normalize_jsonld(js[k]) for k in sorted(js)}\n",
    "    if isinstance(js, list):\n",
    "        return sorted((normalize_jsonld(el) for el in js),\n",
    "                      key=lambda x: json.dumps(x, sort_keys=True))\n",
    "    return js\n",
    "\n",
    "\n",
    "def diff_roundtrip(orig_json, jsonld_path, ttl_path):\n",
    "    orig = load_json(orig_json)\n",
    "    ld   = load_json(jsonld_path)\n",
    "\n",
    "    # parse TTL back to JSON-LD\n",
    "    g = Graph().parse(ttl_path, format=\"turtle\")\n",
    "    ttl_as_ld = json.loads(g.serialize(format=\"json-ld\"))\n",
    "\n",
    "    # normalize\n",
    "    nl = normalize_jsonld(ld)\n",
    "    nt = normalize_jsonld(ttl_as_ld)\n",
    "\n",
    "    return {\n",
    "        \"orig_vs_jsonld\":   compare_json(orig, ld),\n",
    "        \"jsonld_vs_ttl_ld\": compare_json(nl, nt)\n",
    "    }\n",
    "\n",
    "\n",
    "# ── Main flow ─────────────────────────────────────────────────────────────────\n",
    "\n",
    "def main():\n",
    "    ok = 0\n",
    "    total = len(JSON_SUMMARIES)\n",
    "    missing_reports = []\n",
    "    cases = {}  # store diff results per run\n",
    "\n",
    "    for js_path in JSON_SUMMARIES:\n",
    "        summary = load_json(js_path)\n",
    "        base    = os.path.basename(js_path).split(\"_run_summary.json\")[0]\n",
    "\n",
    "        # 1) completeness check\n",
    "        if not REQUIRED_TOPLEVEL.issubset(summary):\n",
    "            missing = REQUIRED_TOPLEVEL - set(summary)\n",
    "            missing_reports.append((js_path, f\"missing fields {missing}\"))\n",
    "            continue\n",
    "\n",
    "        if not (REQUIRED_PARAMS <= summary[\"params\"].keys()):\n",
    "            missing_reports.append((js_path, f\"params missing {REQUIRED_PARAMS - summary['params'].keys()}\"))\n",
    "            continue\n",
    "\n",
    "        if not (REQUIRED_METRICS <= summary[\"metrics\"].keys()):\n",
    "            missing_reports.append((js_path, f\"metrics missing {REQUIRED_METRICS - summary['metrics'].keys()}\"))\n",
    "            continue\n",
    "\n",
    "        ok += 1\n",
    "\n",
    "        # 2) convert\n",
    "        jsonld_path, ttl_path = convert_to_jsonld_and_ttl(summary, base)\n",
    "\n",
    "        # 3) diff\n",
    "        diffs = diff_roundtrip(js_path, jsonld_path, ttl_path)\n",
    "        cases[base] = diffs\n",
    "        print(f\"\\n── {base} diffs ──\")\n",
    "        print(\"  • JSON → JSON-LD:\", len(diffs[\"orig_vs_jsonld\"]), \"differences\")\n",
    "        print(\"  • JSON-LD → TTL → JSON-LD:\", len(diffs[\"jsonld_vs_ttl_ld\"]), \"differences\")\n",
    "\n",
    "    # 4) completeness summary\n",
    "    completeness_pct = (100 * ok / total) if total else 0\n",
    "    print(f\"\\n{ok}/{total} runs passed completeness checks ({completeness_pct:.1f}%).\")\n",
    "    if missing_reports:\n",
    "        print(\"\\nFailures:\")\n",
    "        for path, reason in missing_reports:\n",
    "            print(f\" • {path}: {reason}\")\n",
    "\n",
    "    # 5) integrity check\n",
    "    total_runs = len(cases)\n",
    "    zero_diff_runs = sum(\n",
    "        1\n",
    "        for diffs in cases.values()\n",
    "        if not diffs[\"orig_vs_jsonld\"] and not diffs[\"jsonld_vs_ttl_ld\"]\n",
    "    )\n",
    "    integrity_pct = (100 * zero_diff_runs / total_runs) if total_runs else 0\n",
    "    print(f\"\\nMapping integrity: {zero_diff_runs}/{total_runs} runs have zero diffs — {integrity_pct:.1f}%\")\n",
    "\n",
    "    # 6) overall quality score\n",
    "    overall_score = (completeness_pct + integrity_pct) / 2\n",
    "    print(f\"Overall quality score: {overall_score:.1f}%\")\n",
    "\n",
    "    # 7) Benchmark your training fn\n",
    "    print(\"\\nBenchmarking train_and_log() overhead:\")\n",
    "    def train_and_log(use_mlflow=False):\n",
    "        # ← your real instrumentation + fit logic here\n",
    "        time.sleep(0.5 + (0.1 if use_mlflow else 0))  # stub\n",
    "        return\n",
    "\n",
    "    for flag in (False, True):\n",
    "        start = time.time()\n",
    "        train_and_log(use_mlflow=flag)\n",
    "        elapsed = time.time() - start\n",
    "        label = \"With MLflow\" if flag else \"No MLflow\"\n",
    "        print(f\"  • {label:10s}: {elapsed:.3f}s\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5883f673-371e-415e-a73e-5c9c88b56fb1",
   "metadata": {},
   "source": [
    "RQ2  implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "57c1f653-ff09-494a-9c11-4433936f1824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ace_tools in c:\\users\\reema\\anaconda3\\lib\\site-packages (0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6d07ac1c-ea80-4787-bcb9-da047d12167d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['run_id', 'param_bootstrap', 'param_ccp_alpha', 'param_class_weight',\n",
       "       'param_columns_raw', 'param_criterion', 'param_database.description',\n",
       "       'param_database.id', 'param_database.name', 'param_database.owner',\n",
       "       'param_dataset.authors', 'param_dataset.doi', 'param_dataset.published',\n",
       "       'param_dataset.publisher', 'param_dataset.title',\n",
       "       'param_dropped_columns', 'param_feature_names',\n",
       "       'param_matplotlib_version', 'param_max_depth', 'param_max_features',\n",
       "       'param_max_leaf_nodes', 'param_max_samples',\n",
       "       'param_min_impurity_decrease', 'param_min_samples_leaf',\n",
       "       'param_min_samples_split', 'param_min_weight_fraction_leaf',\n",
       "       'param_numpy_version', 'param_n_estimators', 'param_n_features',\n",
       "       'param_n_features_final', 'param_n_jobs', 'param_n_records',\n",
       "       'param_n_test_samples', 'param_n_train_samples', 'param_oob_score',\n",
       "       'param_os_platform', 'param_pandas_version', 'param_python_version',\n",
       "       'param_random_state', 'param_retrieval_time', 'param_seaborn_version',\n",
       "       'param_shap_version', 'param_sklearn_version', 'param_test_size',\n",
       "       'param_verbose', 'param_warm_start', 'metric_accuracy',\n",
       "       'metric_accuracy_score_X_test', 'metric_dbrepo.num_deletes',\n",
       "       'metric_dbrepo.num_inserts', 'metric_dbrepo.row_count_end',\n",
       "       'metric_dbrepo.row_count_start', 'metric_f1_macro',\n",
       "       'metric_f1_score_X_test', 'metric_precision_macro',\n",
       "       'metric_precision_score_X_test', 'metric_recall_macro',\n",
       "       'metric_recall_score_X_test', 'metric_roc_auc',\n",
       "       'metric_roc_auc_score_X_test', 'metric_training_accuracy_score',\n",
       "       'metric_training_f1_score', 'metric_training_log_loss',\n",
       "       'metric_training_precision_score', 'metric_training_recall_score',\n",
       "       'metric_training_roc_auc', 'metric_training_score', 'tag_dataset_id',\n",
       "       'tag_dataset_name', 'tag_dataset_version', 'tag_data_source',\n",
       "       'tag_dbrepo.admin_email', 'tag_dbrepo.base_url',\n",
       "       'tag_dbrepo.granularity', 'tag_dbrepo.protocol_version',\n",
       "       'tag_dbrepo.repository_name', 'tag_dbrepo.table_last_modified',\n",
       "       'tag_estimator_class', 'tag_estimator_name',\n",
       "       'tag_git_current_commit_hash', 'tag_git_previous_commit_hash',\n",
       "       'tag_git__current_commit_url', 'tag_mlflow.log-model.history',\n",
       "       'tag_mlflow.runName', 'tag_mlflow.source.name',\n",
       "       'tag_mlflow.source.type', 'tag_mlflow.user', 'tag_model_name',\n",
       "       'tag_notebook_name', 'tag_target_name', 'tag_training_end_time',\n",
       "       'tag_training_start_time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Load all run summary JSON files\n",
    "files = glob.glob(\"MODEL_PROVENANCE/*_run_summary.json\")\n",
    "rows = []\n",
    "for f in files:\n",
    "    with open(f) as fh:\n",
    "        summary = json.load(fh)\n",
    "    # Flatten parameters and metrics\n",
    "    row = {\"run_id\": summary[\"run_id\"]}\n",
    "    row.update({f\"param_{k}\": v for k, v in summary.get(\"params\", {}).items()})\n",
    "    row.update({f\"metric_{k}\": v for k, v in summary.get(\"metrics\", {}).items()})\n",
    "    row.update({f\"tag_{k}\": v for k, v in summary.get(\"tags\", {}).items()})\n",
    "    rows.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba148da6-6ce5-45cf-a985-f164a53c969b",
   "metadata": {},
   "source": [
    "1) Tracing preprocessing steps\n",
    ":\n",
    "Here are the top 4 Iris‐focused preprocessing‐tracing use cases I’d tackle first:\n",
    "\n",
    "Reconstruct a run’s exact preprocessing\n",
    "Fetch a run’s run_id, columns_raw, dropped_columns, feature_names and test_size so you can replay the exact data pull & split.\n",
    "\n",
    "Feature‐drop impact analysis\n",
    "Identify runs where one or more measurements (e.g. petalwidthcm) were dropped and compare their test accuracies.\n",
    "\n",
    "Best feature subset discovery\n",
    "Group runs by which features they used (sepals only vs petals only vs both) and rank them by test F1 or accuracy.\n",
    "\n",
    "Common steps in high-accuracy runs\n",
    "Filter for runs with accuracy_score_X_test ≥ 0.95 and list the shared preprocessing settings (dropped columns, test_size, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6e147555-afbf-4bba-b6da-7e90ff391920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/24 12:07:41 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'de1ce9f489f949cc8121489b43ce6ea0', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dfb26a56b7a498982e572a07bb00f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/24 12:07:47 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '138898a600114e3d9ab06790184b5356', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed955ab5fb7246d0aa6f829e1f0062cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/24 12:07:53 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '54d74e4b53e148fb94fa7338ab205c64', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afb20462ac648bcb6b9d84e4953ae37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/24 12:07:59 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '3aaa56d6cd9f4f99917238af626c09e8', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d774e068fd42a19c1c85f664375d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/24 12:08:05 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '28c0256f7c314fb3a85beee8b915b84f', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da728bc463814a9b9ab9b03226e7d758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dropped_feature': 'sepallengthcm', 'baseline_acc': 1.0, 'dropped_acc': 1.0, 'impact': 0.0}, {'dropped_feature': 'sepalwidthcm', 'baseline_acc': 1.0, 'dropped_acc': 1.0, 'impact': 0.0}, {'dropped_feature': 'petallengthcm', 'baseline_acc': 1.0, 'dropped_acc': 0.9666666666666667, 'impact': 0.0333}, {'dropped_feature': 'petalwidthcm', 'baseline_acc': 1.0, 'dropped_acc': 0.9666666666666667, 'impact': 0.0333}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# --------------------------------------------\n",
    "# Query functions for Iris provenance exploration\n",
    "# --------------------------------------------\n",
    "import ast\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Helper to get the “official” feature_names from your summary DF\n",
    "def _get_all_features(df):\n",
    "    # assumes every row has the same param_feature_names\n",
    "    raw = df.loc[0, 'param_feature_names']\n",
    "    return ast.literal_eval(raw)\n",
    "\n",
    "# Train & eval RF on just these columns of Iris\n",
    "def evaluate_subset(features, test_size=0.2, random_state=42, n_estimators=200):\n",
    "    iris = load_iris()\n",
    "    X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    # map sklearn’s names to your param names, e.g. \"sepal length (cm)\" → \"sepallengthcm\"\n",
    "    canon = _get_all_features(df)\n",
    "    mapping = dict(zip(iris.feature_names, canon))\n",
    "    X = X.rename(columns=mapping)\n",
    "    X_sub = X[features]\n",
    "    y = iris.target\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_sub, y, test_size=test_size, random_state=random_state)\n",
    "    m = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "    m.fit(Xtr, ytr)\n",
    "    return accuracy_score(yte, m.predict(Xte))\n",
    "def trace_preprocessing(df, run_id=None):\n",
    "    cols = ['run_id',\n",
    "            'param_dataset.title',\n",
    "            'param_columns_raw',\n",
    "            'param_dropped_columns',\n",
    "            'param_feature_names',\n",
    "            'param_dataset.authors', 'param_dataset.doi', 'param_dataset.published',\n",
    "            'param_test_size',\n",
    "            'param_criterion',\n",
    "            'param_max_depth','param_max_leaf_nodes', 'param_max_samples',\n",
    "           'metric_accuracy','metric_f1_macro','metric_roc_auc']\n",
    "    if run_id is None:\n",
    "        subset = df.loc[:, cols]\n",
    "    else:\n",
    "        subset = df.loc[df['run_id'] == run_id, cols]\n",
    "    return subset.to_dict(orient='records')\n",
    "\n",
    "\n",
    "def drop_impact(df, feature, **_):\n",
    "    all_feats = _get_all_features(df)\n",
    "    baseline = evaluate_subset(all_feats)\n",
    "    without = [f for f in all_feats if f!=feature]\n",
    "    dropped = evaluate_subset(without)\n",
    "    return {\n",
    "      'dropped_feature': feature,\n",
    "      'baseline_acc': baseline,\n",
    "      'dropped_acc': dropped,\n",
    "      'impact': baseline - dropped\n",
    "    }\n",
    "\n",
    "def drop_impact_all(df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Compute drop-impact for every feature in the dataset.\n",
    "    Returns list of dicts with dropped_feature, baseline_acc, dropped_acc, impact.\n",
    "    \"\"\"\n",
    "    feats = _get_all_features(df)\n",
    "    baseline = evaluate_subset(feats)\n",
    "    summary = []\n",
    "    for feat in feats:\n",
    "        without = [f for f in feats if f != feat]\n",
    "        acc = evaluate_subset(without)\n",
    "        summary.append({\n",
    "            'dropped_feature': feat,\n",
    "            'baseline_acc': baseline,\n",
    "            'dropped_acc': acc,\n",
    "            'impact': round(baseline - acc, 4)\n",
    "        })\n",
    "    return summary\n",
    "\n",
    "def best_feature_subset(df, features, **_):\n",
    "    acc = evaluate_subset(features)\n",
    "    return {'features': features, 'accuracy': acc}\n",
    "\n",
    "def common_high_accuracy(df: pd.DataFrame, threshold: float = 0.95) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Filter runs with test accuracy >= threshold and list unique shared preprocessing settings.\n",
    "    \"\"\"\n",
    "    high = df[df['metric_accuracy_score_X_test'] >= threshold]\n",
    "    cols = ['param_dropped_columns', 'param_test_size', 'param_feature_names']\n",
    "    return high[cols].drop_duplicates().to_dict(orient='records')\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# Use Case Registry with parameter order for minimal input\n",
    "# --------------------------------------------\n",
    "USE_CASES = {\n",
    "    'trace_preprocessing': {\n",
    "        'func': trace_preprocessing,\n",
    "        'required_params': [],            # none strictly required\n",
    "        'optional_params': ['run_id'],    # run_id can be supplied or not\n",
    "    },\n",
    "    'drop_impact': {\n",
    "        'func': drop_impact,\n",
    "        'required_params': ['feature'],\n",
    "        'optional_params': [],\n",
    "    },\n",
    "     'drop_impact_all': {\n",
    "        'func': drop_impact_all,\n",
    "        'required_params': [],\n",
    "        'optional_params': [],\n",
    "    },\n",
    "    'best_feature_subset': {\n",
    "        'func': best_feature_subset,\n",
    "        'required_params': ['features'],\n",
    "        'optional_params': [],\n",
    "    },\n",
    "    'common_high_accuracy': {\n",
    "        'func': common_high_accuracy,\n",
    "        'required_params': ['threshold'],\n",
    "        'optional_params': [],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def call_use_case(df, use_case_name, **kwargs):\n",
    "    if use_case_name not in USE_CASES:\n",
    "        raise ValueError(f\"Unknown use case: {use_case_name}\")\n",
    "    case = USE_CASES[use_case_name]\n",
    "    func = case['func']\n",
    "    # check required\n",
    "    missing = [p for p in case['required_params'] if p not in kwargs]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{use_case_name} missing required params: {missing}\")\n",
    "    # build args\n",
    "    args = {p: kwargs[p] for p in case['required_params']}\n",
    "    for p in case['optional_params']:\n",
    "        args[p] = kwargs.get(p)\n",
    "    return func(df, **args)\n",
    "\n",
    "# --------------------------------------------\n",
    "# Example Usage\n",
    "# --------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "   # # 1) trace_preprocessing for all runs\n",
    "    # print(call_use_case(df, 'trace_preprocessing'))\n",
    "    \n",
    "    # 2) trace_preprocessing for a single run_id\n",
    "    # print(call_use_case(df, 'trace_preprocessing', run_id='361daa12f99f4129a06cd20b78dd6fa7'))\n",
    "\n",
    "    # 5) common_high_accuracy\n",
    "    # print(call_use_case(df, 'common_high_accuracy', threshold=0.99))\n",
    "\n",
    "    # 4) Best‐subset on just sepals:\n",
    "    # print(call_use_case(df, 'best_feature_subset', features=['sepallengthcm','sepalwidthcm']))\n",
    "\n",
    "    # 3) Drop‐impact for “petallengthcm”:\n",
    "    # print(call_use_case(df, 'drop_impact', feature='petallengthcm'))\n",
    "\n",
    "    print(call_use_case(df, 'drop_impact_all'))  # summary for all features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f912d6-0e84-4155-858a-9668bef63f6e",
   "metadata": {},
   "source": [
    " • Detecting models trained with deprecated code versions\n",
    " • Mapping models to specific datasets used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "34a02c9a-5459-478f-a3c5-7f7a58ff22b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'param_dataset.doi': '10.5281/ZENODO.1404173',\n",
      "  'param_dataset.published': '2018-8-27',\n",
      "  'param_dataset.publisher': 'Zenodo',\n",
      "  'param_dataset.title': 'Scikit-Learn Iris',\n",
      "  'run_id': '361daa12f99f4129a06cd20b78dd6fa7',\n",
      "  'tag_model_name': 'RandomForest_Iris_v20250423_230422'},\n",
      " {'param_dataset.doi': '10.5281/ZENODO.1404173',\n",
      "  'param_dataset.published': '2018-8-27',\n",
      "  'param_dataset.publisher': 'Zenodo',\n",
      "  'param_dataset.title': 'Scikit-Learn Iris',\n",
      "  'run_id': 'dcb65d2337a047fdac192b7fe9c8e3d6',\n",
      "  'tag_model_name': 'RandomForest_Iris_v20250424_110923'},\n",
      " {'param_dataset.doi': '10.5281/ZENODO.1404173',\n",
      "  'param_dataset.published': '2018-8-27',\n",
      "  'param_dataset.publisher': 'Zenodo',\n",
      "  'param_dataset.title': 'Scikit-Learn Iris',\n",
      "  'run_id': 'e519450da74a4abbb11e6d00901bd435',\n",
      "  'tag_model_name': 'RandomForest_Iris_v20250424_111946'}]\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# New Query Functions\n",
    "# --------------------------------------------\n",
    "from typing import List, Dict, Any\n",
    "from pprint import pprint\n",
    "\n",
    "def detect_deprecated_code(df: pd.DataFrame, deprecated_commits: List[str], **_) -> List[Dict[str, Any]]:\n",
    "    # we know the column is called tag_git_current_commit_hash\n",
    "    commit_col = 'tag_git_current_commit_hash'\n",
    "    if commit_col not in df.columns:\n",
    "        raise KeyError(f\"Missing {commit_col} in DataFrame\")\n",
    "    out = df[df[commit_col].isin(deprecated_commits)]\n",
    "    # include run_id and notebook/runName for context\n",
    "    cols = ['run_id', commit_col, 'tag_notebook_name', 'tag_mlflow.runName']\n",
    "    # drop any that don’t exist\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    return out[cols].to_dict(orient='records')\n",
    "\n",
    "\n",
    "def map_model_dataset(df: pd.DataFrame, **_) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For each run, return its model name (or run_id) alongside the dataset\n",
    "    title, DOI, published date and publisher.\n",
    "    \"\"\"\n",
    "    # pick whichever model-name column you have\n",
    "    model_col = 'tag_model_name' if 'tag_model_name' in df.columns else 'param_model_name'\n",
    "    cols = [\n",
    "        'run_id',\n",
    "        model_col,\n",
    "        'param_dataset.title',\n",
    "        'param_dataset.doi',\n",
    "        'param_dataset.published',\n",
    "        'param_dataset.publisher'\n",
    "    ]\n",
    "    # filter out any columns that don’t actually exist\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    return df[cols].to_dict(orient='records')\n",
    "\n",
    "# --------------------------------------------\n",
    "# Extend Use-Case Registry\n",
    "# --------------------------------------------\n",
    "USE_CASES.update({\n",
    "    'detect_deprecated_code': {\n",
    "        'func': detect_deprecated_code,\n",
    "        'required_params': ['deprecated_commits'],\n",
    "        'optional_params': []\n",
    "    },\n",
    "    'map_model_dataset': {\n",
    "        'func': map_model_dataset,\n",
    "        'required_params': [],\n",
    "        'optional_params': []\n",
    "    },\n",
    "})\n",
    "# 1) Detect runs on deprecated commits:\n",
    "deprecated = [\n",
    "    \"a07434af4f547af2daab044d6873eb7081162293\",\n",
    "    \"d329c92495e196ec0f39fbb19dfdd367131a77d9\"\n",
    "]\n",
    "# print(call_use_case(df, \"detect_deprecated_code\", deprecated_commits=deprecated))\n",
    "pprint(call_use_case(df, 'map_model_dataset'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f16e7-4086-4867-b326-5d6eecab2439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c52607ad-5849-4a2d-97ef-e8fc1ca16dc7",
   "metadata": {},
   "source": [
    "Goal: Notify collaborators who have forked the GitHub repo if their fork is outdated (i.e., behind the current commit used to train a model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29c8ad9-00bb-4c1e-ac3b-ee6861991acd",
   "metadata": {},
   "source": [
    "🧠 What We Need\n",
    "Current training run’s Git commit hash\n",
    "\n",
    "GitHub API to fetch all forks of your repo\n",
    "\n",
    "Compare each fork’s main or master branch head commit\n",
    "\n",
    "Create an issue on their fork or on your repo tagging them if they’re behind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72bed50-fb56-442d-a21e-bb7991892d07",
   "metadata": {},
   "source": [
    "Option 1 (Practical): Notify via issues on your own repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "852f147c-9d0a-4d7f-a4ab-545d1e2375fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to notify collaborators whose forks are behind? (y/N):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest upstream commit: d329c92495e196ec0f39fbb19dfdd367131a77d9\n",
      "→ POST https://api.github.com/repos/reema-dass26/REPO/issues\n",
      "→ Status code: 201\n",
      "→ Response headers: {'Date': 'Wed, 23 Apr 2025 21:06:22 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '2383', 'Cache-Control': 'private, max-age=60, s-maxage=60', 'Vary': 'Accept, Authorization, Cookie, X-GitHub-OTP,Accept-Encoding, Accept, X-Requested-With', 'ETag': '\"7c97cb7c44baea87eb39ae010093b566d7f13db1ddd45bd81e7d8a302d330834\"', 'X-OAuth-Scopes': 'admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, admin:ssh_signing_key, audit_log, codespace, copilot, delete:packages, delete_repo, gist, notifications, project, repo, user, workflow, write:discussion, write:network_configurations, write:packages', 'X-Accepted-OAuth-Scopes': '', 'Location': 'https://api.github.com/repos/reema-dass26/REPO/issues/3', 'X-GitHub-Media-Type': 'github.v3; format=json', 'x-github-api-version-selected': '2022-11-28', 'X-RateLimit-Limit': '5000', 'X-RateLimit-Remaining': '4996', 'X-RateLimit-Reset': '1745445980', 'X-RateLimit-Used': '4', 'X-RateLimit-Resource': 'core', 'Access-Control-Expose-Headers': 'ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset', 'Access-Control-Allow-Origin': '*', 'Strict-Transport-Security': 'max-age=31536000; includeSubdomains; preload', 'X-Frame-Options': 'deny', 'X-Content-Type-Options': 'nosniff', 'X-XSS-Protection': '0', 'Referrer-Policy': 'origin-when-cross-origin, strict-origin-when-cross-origin', 'Content-Security-Policy': \"default-src 'none'\", 'Server': 'github.com', 'X-GitHub-Request-Id': 'FF27:37CDC4:28E48A1:29EB238:6809564E'}\n",
      "→ Response JSON: {'url': 'https://api.github.com/repos/reema-dass26/REPO/issues/3', 'repository_url': 'https://api.github.com/repos/reema-dass26/REPO', 'labels_url': 'https://api.github.com/repos/reema-dass26/REPO/issues/3/labels{/name}', 'comments_url': 'https://api.github.com/repos/reema-dass26/REPO/issues/3/comments', 'events_url': 'https://api.github.com/repos/reema-dass26/REPO/issues/3/events', 'html_url': 'https://github.com/reema-dass26/REPO/issues/3', 'id': 3015254398, 'node_id': 'I_kwDOOdxdk86zuSF-', 'number': 3, 'title': '🔔 Notification: Your fork is behind the latest commit', 'user': {'login': 'reema-dass26', 'id': 106236154, 'node_id': 'U_kgDOBlUI-g', 'avatar_url': 'https://avatars.githubusercontent.com/u/106236154?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/reema-dass26', 'html_url': 'https://github.com/reema-dass26', 'followers_url': 'https://api.github.com/users/reema-dass26/followers', 'following_url': 'https://api.github.com/users/reema-dass26/following{/other_user}', 'gists_url': 'https://api.github.com/users/reema-dass26/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/reema-dass26/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/reema-dass26/subscriptions', 'organizations_url': 'https://api.github.com/users/reema-dass26/orgs', 'repos_url': 'https://api.github.com/users/reema-dass26/repos', 'events_url': 'https://api.github.com/users/reema-dass26/events{/privacy}', 'received_events_url': 'https://api.github.com/users/reema-dass26/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'labels': [], 'state': 'open', 'locked': False, 'assignee': None, 'assignees': [], 'milestone': None, 'comments': 0, 'created_at': '2025-04-23T21:06:22Z', 'updated_at': '2025-04-23T21:06:22Z', 'closed_at': None, 'author_association': 'OWNER', 'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0}, 'active_lock_reason': None, 'body': 'Hi @reemagdass,\\n\\nThe main repository has been updated to commit `d329c92495e196ec0f39fbb19dfdd367131a77d9`.\\nPlease consider pulling the latest changes to stay in sync.\\n\\nThanks!', 'closed_by': None, 'reactions': {'url': 'https://api.github.com/repos/reema-dass26/REPO/issues/3/reactions', 'total_count': 0, '+1': 0, '-1': 0, 'laugh': 0, 'hooray': 0, 'confused': 0, 'heart': 0, 'rocket': 0, 'eyes': 0}, 'timeline_url': 'https://api.github.com/repos/reema-dass26/REPO/issues/3/timeline', 'performed_via_github_app': None, 'state_reason': None}\n",
      "→ html_url field: https://github.com/reema-dass26/REPO/issues/3\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def notify_outdated_forks():\n",
    "    load_dotenv()\n",
    "    token     = os.getenv(\"THESIS_TOKEN\")\n",
    "    owner     = \"reema-dass26\"\n",
    "    repo      = \"REPO\"\n",
    "\n",
    "    if not token:\n",
    "        print(\"⚠️ GITHUB_TOKEN not set.\")\n",
    "        return\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {token}\",\n",
    "        \"Accept\":        \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "\n",
    "    # 1) Get latest upstream commit\n",
    "    main_commits = requests.get(\n",
    "        f\"https://api.github.com/repos/{owner}/{repo}/commits\",\n",
    "        headers=headers,\n",
    "        params={\"per_page\": 1}\n",
    "    )\n",
    "    main_commits.raise_for_status()\n",
    "    new_commit_hash = main_commits.json()[0][\"sha\"]\n",
    "    print(f\"Latest upstream commit: {new_commit_hash}\")\n",
    "\n",
    "    # 2) List forks\n",
    "    forks_resp = requests.get(f\"https://api.github.com/repos/{owner}/{repo}/forks\", headers=headers)\n",
    "    forks_resp.raise_for_status()\n",
    "    forks = forks_resp.json()\n",
    "\n",
    "    # 3) Compare each fork\n",
    "    outdated = []\n",
    "    for fork in forks:\n",
    "        fork_owner = fork[\"owner\"][\"login\"]\n",
    "        fork_comm = requests.get(\n",
    "            fork[\"url\"] + \"/commits\",\n",
    "            headers=headers,\n",
    "            params={\"per_page\": 1}\n",
    "        )\n",
    "        if fork_comm.status_code != 200:\n",
    "            print(f\"  – could not fetch commits for {fork_owner}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        fork_sha = fork_comm.json()[0][\"sha\"]\n",
    "        if fork_sha != new_commit_hash:\n",
    "            outdated.append(f\"@{fork_owner}\")\n",
    "\n",
    "    # 4) Open an issue if any are behind\n",
    "    if outdated:\n",
    "        title = \"🔔 Notification: Your fork is behind the latest commit\"\n",
    "        body  = (\n",
    "            f\"Hi {' '.join(outdated)},\\n\\n\"\n",
    "            f\"The main repository has been updated to commit `{new_commit_hash}`.\\n\"\n",
    "            \"Please consider pulling the latest changes to stay in sync.\\n\\n\"\n",
    "            \"Thanks!\"\n",
    "        )\n",
    "        issues_url = f\"https://api.github.com/repos/{owner}/{repo}/issues\"\n",
    "        resp = requests.post(\n",
    "        issues_url,\n",
    "        headers=headers,\n",
    "        json={\"title\": title, \"body\": body}\n",
    "    )\n",
    "\n",
    "    # DEBUGGING OUTPUT\n",
    "    print(f\"→ POST {issues_url}\")\n",
    "    print(\"→ Status code:\", resp.status_code)\n",
    "    print(\"→ Response headers:\", resp.headers)\n",
    "    try:\n",
    "        data = resp.json()\n",
    "        print(\"→ Response JSON:\", data)\n",
    "        print(\"→ html_url field:\", data.get(\"html_url\"))\n",
    "    except ValueError:\n",
    "        print(\"→ No JSON response body; raw text:\", resp.text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    answer = input(\"Do you want to notify collaborators whose forks are behind? (y/N): \").strip().lower()\n",
    "    if answer in (\"y\", \"yes\"):\n",
    "        notify_outdated_forks()\n",
    "    else:\n",
    "        print(\"No action taken.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda31f16-fbe9-40ce-ac1b-9ebc898c8820",
   "metadata": {},
   "source": [
    "INVENIO INTEGRETION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7dd71550-7221-41db-b64d-f87ffead2f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Draft created: 7vtzn-76461\n",
      "  • Uploaded plots/RandomForest_Iris_v20250423_230422/confusion_matrix.png\n",
      "❌ PUT failed (400): {\"status\": 400, \"message\": \"The file upload transfer failed, please try again.\"}\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: BAD REQUEST for url: https://127.0.0.1:5000/api/records/7vtzn-76461/draft/files/plots/RandomForest_Iris_v20250423_230422/confusion_matrix.png/content",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[168], line 84\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m r2\u001b[38;5;241m.\u001b[39mok:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ PUT failed (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mr2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# 2c) commit the upload\u001b[39;00m\n\u001b[0;32m     87\u001b[0m r3 \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m     88\u001b[0m     commit_url,\n\u001b[0;32m     89\u001b[0m     headers\u001b[38;5;241m=\u001b[39mH_JSON,\n\u001b[0;32m     90\u001b[0m     verify\u001b[38;5;241m=\u001b[39mVERIFY_SSL\n\u001b[0;32m     91\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 400 Client Error: BAD REQUEST for url: https://127.0.0.1:5000/api/records/7vtzn-76461/draft/files/plots/RandomForest_Iris_v20250423_230422/confusion_matrix.png/content"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "API_BASE   = \"https://127.0.0.1:5000\"\n",
    "TOKEN      = \"ZctHtk65umtROkzOFq2Ot7WbJTqz46q5wS8uAYc2WOMry2c2rT9CqaRbySNp\"\n",
    "VERIFY_SSL = False  # only for local dev with self-signed cert\n",
    "\n",
    "H_JSON = {\n",
    "    \"Accept\":        \"application/json\",\n",
    "    \"Content-Type\":  \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "}\n",
    "\n",
    "H_OCTET = {\n",
    "    \"Content-Type\":  \"application/octet-stream\",\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "}\n",
    "\n",
    "TO_UPLOAD = [\"plots\", \"MODEL_PROVENANCE\"]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Create a new draft record\n",
    "# -----------------------------------------------------------------------------\n",
    "payload = {\n",
    "    \"title\":       \"My trained ML model\",\n",
    "    \"description\": \"Unstructured metadata + artifacts for RandomForest on Iris\",\n",
    "    \"creator\":     \"Reema Dass\"\n",
    "}\n",
    "\n",
    "r = requests.post(\n",
    "    f\"{API_BASE}/api/records\",\n",
    "    headers=H_JSON,\n",
    "    data=json.dumps(payload),\n",
    "    verify=VERIFY_SSL\n",
    ")\n",
    "r.raise_for_status()\n",
    "draft = r.json()\n",
    "recid = draft[\"id\"]\n",
    "links = draft[\"links\"]\n",
    "print(f\"✅ Draft created: {recid}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Walk & upload every file\n",
    "# -----------------------------------------------------------------------------\n",
    "for topdir in TO_UPLOAD:\n",
    "    if not os.path.isdir(topdir):\n",
    "        print(f\"⚠️  Skipping missing folder {topdir}\")\n",
    "        continue\n",
    "\n",
    "    for root, _, files in os.walk(topdir):\n",
    "        for fn in files:\n",
    "            local_path = os.path.join(root, fn)\n",
    "            # build a key that preserves the folder structure under topdir\n",
    "            key = os.path.relpath(local_path, start=os.path.dirname(topdir)).replace(\"\\\\\", \"/\")\n",
    "\n",
    "            # 2a) register the file key\n",
    "            register_body = [{\"key\": key}]\n",
    "            r1 = requests.post(\n",
    "                links[\"files\"],\n",
    "                headers=H_JSON,\n",
    "                data=json.dumps(register_body),\n",
    "                verify=VERIFY_SSL\n",
    "            )\n",
    "            r1.raise_for_status()\n",
    "            entry = r1.json()[\"entries\"][0]\n",
    "            upload_url = entry[\"links\"][\"content\"]\n",
    "            commit_url = entry[\"links\"][\"commit\"]\n",
    "\n",
    "            # 2b) read & PUT the file in one shot so Content-Length is set\n",
    "            with open(local_path, \"rb\") as fp:\n",
    "                data = fp.read()\n",
    "            r2 = requests.put(\n",
    "                upload_url,\n",
    "                headers=H_OCTET,\n",
    "                data=data,\n",
    "                verify=VERIFY_SSL\n",
    "            )\n",
    "            if not r2.ok:\n",
    "                print(f\"❌ PUT failed ({r2.status_code}): {r2.text}\")\n",
    "                r2.raise_for_status()\n",
    "\n",
    "            # 2c) commit the upload\n",
    "            r3 = requests.post(\n",
    "                commit_url,\n",
    "                headers=H_JSON,\n",
    "                verify=VERIFY_SSL\n",
    "            )\n",
    "            r3.raise_for_status()\n",
    "\n",
    "            print(f\"  • Uploaded {key}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Publish the draft\n",
    "# -----------------------------------------------------------------------------\n",
    "rpub = requests.post(\n",
    "    links[\"publish\"],\n",
    "    headers=H_JSON,\n",
    "    verify=VERIFY_SSL\n",
    ")\n",
    "rpub.raise_for_status()\n",
    "print(f\"✅ Published: {rpub.json()['id']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d1809a51-46a4-4595-bea9-da64e86f9b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit-agraph\n",
      "  Obtaining dependency information for streamlit-agraph from https://files.pythonhosted.org/packages/b9/80/8a666e700332a9fe19e458678c95fab4d78340251d2f12da7d2ad915458a/streamlit_agraph-0.0.45-py3-none-any.whl.metadata\n",
      "  Downloading streamlit_agraph-0.0.45-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: streamlit>=0.63 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit-agraph) (1.44.1)\n",
      "Requirement already satisfied: networkx>=2.5 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit-agraph) (3.1)\n",
      "Requirement already satisfied: rdflib>=6.0.2 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit-agraph) (6.3.2)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from rdflib>=6.0.2->streamlit-agraph) (0.6.1)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from rdflib>=6.0.2->streamlit-agraph) (3.0.9)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=0.63->streamlit-agraph) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=0.63->streamlit-agraph) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=0.63->streamlit-agraph) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=0.63->streamlit-agraph) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=0.63->streamlit-agraph) (1.24.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=0.63->streamlit-agraph) (23.1)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=0.63->streamlit-agraph) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=0.63->streamlit-agraph) (9.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=0.63->streamlit-agraph) (5.29.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=0.63->streamlit-agraph) (11.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=0.63->streamlit-agraph) (2.31.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=0.63->streamlit-agraph) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=0.63->streamlit-agraph) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=0.63->streamlit-agraph) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=0.63->streamlit-agraph) (2.1.6)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=0.63->streamlit-agraph) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=0.63->streamlit-agraph) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\reema\\appdata\\roaming\\python\\python311\\site-packages (from streamlit>=0.63->streamlit-agraph) (6.3.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-agraph) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-agraph) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-agraph) (1.30.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\reema\\appdata\\roaming\\python\\python311\\site-packages (from click<9,>=7.0->streamlit>=0.63->streamlit-agraph) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.63->streamlit-agraph) (4.0.11)\n",
      "Requirement already satisfied: six in c:\\users\\reema\\anaconda3\\lib\\site-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.0.2->streamlit-agraph) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit>=0.63->streamlit-agraph) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit>=0.63->streamlit-agraph) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit>=0.63->streamlit-agraph) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-agraph) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-agraph) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-agraph) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-agraph) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.63->streamlit-agraph) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit>=0.63->streamlit-agraph) (2.1.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-agraph) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-agraph) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-agraph) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-agraph) (0.22.3)\n",
      "Downloading streamlit_agraph-0.0.45-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 0.7/1.3 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.0/1.3 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.0/1.3 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.0/1.3 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 5.5 MB/s eta 0:00:00\n",
      "Installing collected packages: streamlit-agraph\n",
      "Successfully installed streamlit-agraph-0.0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit-agraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "962a7e5d-f305-40f1-b853-0a236b444d86",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'MODEL_PROVENANCE/run1234.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[169], line 37\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(g\u001b[38;5;241m.\u001b[39mserialize(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson-ld\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#  1) Read your raw provenance JSON\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMODEL_PROVENANCE/run1234.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m     38\u001b[0m     run_meta \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fp)\n\u001b[0;32m     40\u001b[0m ttl \u001b[38;5;241m=\u001b[39m json_to_prov_ttl(run_meta)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MODEL_PROVENANCE/run1234.json'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "31d1fd16-0257-4fbb-b20d-4b05424d67ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit-option-menu\n",
      "  Obtaining dependency information for streamlit-option-menu from https://files.pythonhosted.org/packages/fd/52/2f525ad4262dc83d67297f69ec5afcee1438b9e9ae22aa318396725ddbed/streamlit_option_menu-0.4.0-py3-none-any.whl.metadata\n",
      "  Downloading streamlit_option_menu-0.4.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: streamlit>=1.36 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit-option-menu) (1.44.1)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=1.36->streamlit-option-menu) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=1.36->streamlit-option-menu) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=1.36->streamlit-option-menu) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=1.36->streamlit-option-menu) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=1.36->streamlit-option-menu) (1.24.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=1.36->streamlit-option-menu) (23.1)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=1.36->streamlit-option-menu) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=1.36->streamlit-option-menu) (9.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=1.36->streamlit-option-menu) (5.29.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=1.36->streamlit-option-menu) (11.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=1.36->streamlit-option-menu) (2.31.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=1.36->streamlit-option-menu) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=1.36->streamlit-option-menu) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=1.36->streamlit-option-menu) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=1.36->streamlit-option-menu) (2.1.6)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=1.36->streamlit-option-menu) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from streamlit>=1.36->streamlit-option-menu) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\reema\\appdata\\roaming\\python\\python311\\site-packages (from streamlit>=1.36->streamlit-option-menu) (6.3.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (1.30.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\reema\\appdata\\roaming\\python\\python311\\site-packages (from click<9,>=7.0->streamlit>=1.36->streamlit-option-menu) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.36->streamlit-option-menu) (4.0.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit>=1.36->streamlit-option-menu) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit>=1.36->streamlit-option-menu) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit>=1.36->streamlit-option-menu) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit>=1.36->streamlit-option-menu) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit>=1.36->streamlit-option-menu) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit>=1.36->streamlit-option-menu) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit>=1.36->streamlit-option-menu) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.36->streamlit-option-menu) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (2.1.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (0.22.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\reema\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit>=1.36->streamlit-option-menu) (1.17.0)\n",
      "Downloading streamlit_option_menu-0.4.0-py3-none-any.whl (829 kB)\n",
      "   ---------------------------------------- 0.0/829.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/829.3 kB ? eta -:--:--\n",
      "    -------------------------------------- 20.5/829.3 kB 682.7 kB/s eta 0:00:02\n",
      "   -------- ------------------------------- 184.3/829.3 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 829.3/829.3 kB 7.5 MB/s eta 0:00:00\n",
      "Installing collected packages: streamlit-option-menu\n",
      "Successfully installed streamlit-option-menu-0.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit-option-menu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fb02692d-ffdb-4431-997d-5198e3e8f755",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "403 Client Error: FORBIDDEN for url: https://127.0.0.1:5000/api/records",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[170], line 84\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m#  2) Create a new draft\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     80\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAPI_BASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/records\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m                   headers\u001b[38;5;241m=\u001b[39mH_JSON,\n\u001b[0;32m     82\u001b[0m                   json\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[0;32m     83\u001b[0m                   verify\u001b[38;5;241m=\u001b[39mVERIFY_SSL)\n\u001b[1;32m---> 84\u001b[0m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m draft \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     86\u001b[0m recid \u001b[38;5;241m=\u001b[39m draft[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: FORBIDDEN for url: https://127.0.0.1:5000/api/records"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from rdflib import Graph, URIRef, Literal\n",
    "from rdflib.namespace import PROV, XSD\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  CONFIGURATION\n",
    "# -----------------------------------------------------------------------------\n",
    "API_BASE   = \"https://127.0.0.1:5000\"\n",
    "TOKEN      = \"ZctHtk65umtROkzOFq2Ot7WbJTqz46q5w8SOMry2c2rT9CqaRbySNp\"\n",
    "VERIFY_SSL = False   # for local self-signed\n",
    "\n",
    "H_JSON = {\n",
    "    \"Accept\":        \"application/json\",\n",
    "    \"Content-Type\":  \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "}\n",
    "H_OCTET = {\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "    \"Content-Type\":  \"application/octet-stream\",\n",
    "}\n",
    "\n",
    "# top-level folders you want to push\n",
    "TO_UPLOAD = [\"Trained_models\", \"plots\", \"MODEL_PROVENANCE\"]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  OPTIONAL: turn your JSON provenance → PROV-O → JSON-LD (if you want structured metadata)\n",
    "# -----------------------------------------------------------------------------\n",
    "def json_to_prov_ttl(raw: dict) -> str:\n",
    "    g   = Graph()\n",
    "    run = URIRef(f\"urn:run:{raw['run_id']}\")\n",
    "    g.bind(\"prov\", PROV)\n",
    "    # example: timestamps\n",
    "    g.add((run, PROV.startedAtTime,\n",
    "           Literal(raw[\"start_time\"], datatype=XSD.dateTime)))\n",
    "    # params → prov:hadParameter\n",
    "    for k, v in raw.get(\"params\", {}).items():\n",
    "        g.add((run, PROV.hadParameter, Literal(v, datatype=XSD.string)))\n",
    "    # metrics → prov:hadQuality\n",
    "    for k, v in raw.get(\"metrics\", {}).items():\n",
    "        g.add((run, PROV.hadQuality, Literal(v, datatype=XSD.decimal)))\n",
    "    return g.serialize(format=\"turtle\").decode()\n",
    "\n",
    "\n",
    "def ttl_to_jsonld(ttl: str) -> dict:\n",
    "    g = Graph().parse(data=ttl, format=\"turtle\")\n",
    "    return json.loads(g.serialize(format=\"json-ld\"))\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  1) (optional) read your raw provenance JSON and embed as JSON-LD under \"metadata\"\n",
    "# -----------------------------------------------------------------------------\n",
    "# with open(\"MODEL_PROVENANCE/run1234.json\") as fp:\n",
    "#     raw_meta = json.load(fp)\n",
    "# ttl    = json_to_prov_ttl(raw_meta)\n",
    "# jsonld = ttl_to_jsonld(ttl)\n",
    "# meta   = {\n",
    "#     \"title\":       f\"Iris RF run {raw_meta['run_id']}\",\n",
    "#     \"description\": \"Structured PROV-O in JSON-LD\",\n",
    "#     \"metadata\": {\n",
    "#         \"@context\": jsonld.get(\"@context\", {}),\n",
    "#         \"@graph\":   jsonld.get(\"@graph\", [])\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  1') Or, if you don’t need JSON-LD, just supply minimal metadata:\n",
    "# -----------------------------------------------------------------------------\n",
    "meta = {\n",
    "    \"title\":       \"My trained ML model\",\n",
    "    \"description\": \"All artifacts for RandomForest on Iris\",\n",
    "    \"creator\":     \"Reema Dass\"\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  2) Create a new draft\n",
    "# -----------------------------------------------------------------------------\n",
    "r = requests.post(f\"{API_BASE}/api/records\",\n",
    "                  headers=H_JSON,\n",
    "                  json=meta,\n",
    "                  verify=VERIFY_SSL)\n",
    "r.raise_for_status()\n",
    "draft = r.json()\n",
    "recid = draft[\"id\"]\n",
    "files_link = draft[\"links\"][\"files\"]\n",
    "print(f\"✅ Draft created: {recid}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  3) Walk & upload every file in each folder\n",
    "# -----------------------------------------------------------------------------\n",
    "for folder in TO_UPLOAD:\n",
    "    if not os.path.isdir(folder):\n",
    "        print(f\"⚠️  Skipping missing folder {folder}\")\n",
    "        continue\n",
    "\n",
    "    for root, _, filenames in os.walk(folder):\n",
    "        for fn in filenames:\n",
    "            local_path = os.path.join(root, fn)\n",
    "            # build the “key” so that in Invenio it shows up under folder/…\n",
    "            rel = os.path.relpath(local_path, start=folder).replace(\"\\\\\", \"/\")\n",
    "            key = f\"{folder}/{rel}\"\n",
    "\n",
    "            # 3a) register the file in the draft\n",
    "            entry = [{\"key\": key}]\n",
    "            r1 = requests.post(files_link,\n",
    "                               headers=H_JSON,\n",
    "                               json=entry,\n",
    "                               verify=VERIFY_SSL)\n",
    "            r1.raise_for_status()\n",
    "            file_links = r1.json()[\"entries\"][0][\"links\"]\n",
    "\n",
    "            # 3b) upload the content\n",
    "            with open(local_path, \"rb\") as fp:\n",
    "                r2 = requests.put(file_links[\"content\"],\n",
    "                                  headers=H_OCTET,\n",
    "                                  data=fp,\n",
    "                                  verify=VERIFY_SSL)\n",
    "            r2.raise_for_status()\n",
    "\n",
    "            # 3c) commit it\n",
    "            r3 = requests.post(file_links[\"commit\"],\n",
    "                               headers=H_JSON,\n",
    "                               verify=VERIFY_SSL)\n",
    "            r3.raise_for_status()\n",
    "\n",
    "            print(f\"  • Uploaded {key}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  4) Publish the draft\n",
    "# -----------------------------------------------------------------------------\n",
    "rp = requests.post(draft[\"links\"][\"publish\"],\n",
    "                   headers=H_JSON,\n",
    "                   verify=VERIFY_SSL)\n",
    "rp.raise_for_status()\n",
    "print(\"✅ Published:\", rp.json()[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "82bda3d9-62af-44e8-96a5-58008973bd70",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "403 Client Error: FORBIDDEN for url: https://127.0.0.1:5000/api/records",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[176], line 43\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 3) Create the draft record\u001b[39;00m\n\u001b[0;32m     39\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAPI_BASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/records\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m                   headers\u001b[38;5;241m=\u001b[39mH_JSON,\n\u001b[0;32m     41\u001b[0m                   json\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[0;32m     42\u001b[0m                   verify\u001b[38;5;241m=\u001b[39mVERIFY_SSL)\n\u001b[1;32m---> 43\u001b[0m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m draft \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     45\u001b[0m recid \u001b[38;5;241m=\u001b[39m draft[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: FORBIDDEN for url: https://127.0.0.1:5000/api/records"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------------------------------------------------------\n",
    "API_BASE   = \"https://127.0.0.1:5000\"\n",
    "TOKEN      = \"ZctHtk65umtROkzOFq2Ot7WbJTqz46q5w8SOMry2c2rT9CqaRbySNp\"\n",
    "VERIFY_SSL = False  # only for local dev\n",
    "\n",
    "H_JSON = {\n",
    "    \"Accept\":        \"application/json\",\n",
    "    \"Content-Type\":  \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "}\n",
    "H_OCTET = {\n",
    "    \"Accept\":        \"application/json\",\n",
    "    \"Content-Type\":  \"application/octet-stream\",\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Point this at your folder that contains only .pkl (and maybe other) files\n",
    "# -----------------------------------------------------------------------------\n",
    "RECORD_FOLDER = \"Trained_models\"     # ← adjust as needed\n",
    "CREATOR       = \"Reema Dass\"         # your name or email\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Build a minimal metadata payload in‐memory\n",
    "# -----------------------------------------------------------------------------\n",
    "payload = {\n",
    "    \"title\":       os.path.basename(RECORD_FOLDER),\n",
    "    \"description\": f\"All artifacts in folder `{RECORD_FOLDER}`\",\n",
    "    \"creator\":     CREATOR\n",
    "}\n",
    "\n",
    "# 3) Create the draft record\n",
    "r = requests.post(f\"{API_BASE}/api/records\",\n",
    "                  headers=H_JSON,\n",
    "                  json=payload,\n",
    "                  verify=VERIFY_SSL)\n",
    "r.raise_for_status()\n",
    "draft = r.json()\n",
    "recid = draft[\"id\"]\n",
    "links = draft[\"links\"]\n",
    "print(f\"✅ Draft created: {recid}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Walk your folder and upload every file you find\n",
    "# -----------------------------------------------------------------------------\n",
    "for root, _, filenames in os.walk(RECORD_FOLDER):\n",
    "    for fn in filenames:\n",
    "        local_path = os.path.join(root, fn)\n",
    "        # compute the “key” under which it’ll live in your record\n",
    "        key = os.path.relpath(local_path, start=RECORD_FOLDER).replace(\"\\\\\", \"/\")\n",
    "\n",
    "        # a) register that key\n",
    "        entry = [{\"key\": key}]\n",
    "        r1 = requests.post(links[\"files\"],\n",
    "                           headers=H_JSON,\n",
    "                           json=entry,\n",
    "                           verify=VERIFY_SSL)\n",
    "        r1.raise_for_status()\n",
    "        file_links = r1.json()[\"entries\"][0][\"links\"]\n",
    "\n",
    "        # b) upload bytes\n",
    "        with open(local_path, \"rb\") as fp:\n",
    "            r2 = requests.put(file_links[\"content\"],\n",
    "                              headers=H_OCTET,\n",
    "                              data=fp,\n",
    "                              verify=VERIFY_SSL)\n",
    "        r2.raise_for_status()\n",
    "\n",
    "        # c) commit\n",
    "        r3 = requests.post(file_links[\"commit\"],\n",
    "                           headers=H_JSON,\n",
    "                           verify=VERIFY_SSL)\n",
    "        r3.raise_for_status()\n",
    "        print(f\"  • Uploaded {key}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Publish\n",
    "# -----------------------------------------------------------------------------\n",
    "rpub = requests.post(links[\"publish\"],\n",
    "                     headers=H_JSON,\n",
    "                     verify=VERIFY_SSL)\n",
    "rpub.raise_for_status()\n",
    "print(f\"✅ Published: {rpub.json()['id']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "94115adc-62d2-413a-9f51-5b13d3f46392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200 {'hits': {'hits': [{'id': 'a5eb3-j9f36', 'created': '2025-04-24T11:52:42.696314+00:00', 'updated': '2025-04-24T11:52:42.961276+00:00', 'links': {'self': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36', 'self_html': 'https://127.0.0.1:5000/records/a5eb3-j9f36', 'parent': 'https://127.0.0.1:5000/api/records/39q9z-4cp60', 'parent_html': 'https://127.0.0.1:5000/records/39q9z-4cp60', 'self_iiif_manifest': 'https://127.0.0.1:5000/api/iiif/record:a5eb3-j9f36/manifest', 'self_iiif_sequence': 'https://127.0.0.1:5000/api/iiif/record:a5eb3-j9f36/sequence/default', 'files': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36/files', 'media_files': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36/media-files', 'archive': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36/files-archive', 'archive_media': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36/media-files-archive', 'latest': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36/versions/latest', 'latest_html': 'https://127.0.0.1:5000/records/a5eb3-j9f36/latest', 'draft': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36/draft', 'versions': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36/versions', 'access_links': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36/access/links', 'access_grants': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36/access/grants', 'access_users': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36/access/users', 'access_groups': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36/access/groups', 'access_request': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36/access/request', 'access': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36/access', 'reserve_doi': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36/draft/pids/doi', 'communities': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36/communities', 'communities-suggestions': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36/communities-suggestions', 'requests': 'https://127.0.0.1:5000/api/records/a5eb3-j9f36/requests'}, 'revision_id': 4, 'parent': {'id': '39q9z-4cp60', 'access': {'grants': [], 'owned_by': {'user': '3'}, 'links': [], 'settings': {'allow_user_requests': False, 'allow_guest_requests': False, 'accept_conditions_text': None, 'secret_link_expiration': 0}}, 'communities': {}, 'pids': {}}, 'versions': {'is_latest': True, 'is_latest_draft': True, 'index': 1}, 'is_published': True, 'is_draft': False, 'pids': {'oai': {'identifier': 'oai:my-site.com:a5eb3-j9f36', 'provider': 'oai'}}, 'metadata': {'resource_type': {'id': 'other', 'title': {'de': 'Sonstige', 'en': 'Other', 'sv': 'Övrig'}}, 'creators': [{'person_or_org': {'type': 'personal', 'name': 'Dass, Reema', 'given_name': 'Reema', 'family_name': 'Dass'}}], 'title': 'ML model', 'publisher': 'My Site', 'publication_date': '2025-04-24', 'rights': [{'id': 'cc-by-4.0', 'title': {'en': 'Creative Commons Attribution 4.0 International'}, 'description': {'en': 'The Creative Commons Attribution license allows re-distribution and re-use of a licensed work on the condition that the creator is appropriately credited.'}, 'icon': 'cc-by-icon', 'props': {'url': 'https://creativecommons.org/licenses/by/4.0/', 'scheme': 'spdx'}}]}, 'custom_fields': {}, 'access': {'record': 'public', 'files': 'public', 'embargo': {'active': False, 'reason': None}, 'status': 'open'}, 'files': {'enabled': True, 'order': [], 'count': 9, 'total_bytes': 171368, 'entries': {'roc_curve_cls_1.png': {'id': '4f4c8b29-02b3-40c4-af33-412f589a05f4', 'checksum': 'md5:917e9847fda527b030c0bcad8f2a2ea3', 'ext': 'png', 'size': 20143, 'mimetype': 'image/png', 'key': 'roc_curve_cls_1.png', 'metadata': {'width': 640, 'height': 480}, 'access': {'hidden': False}}, 'shap_summary.png': {'id': 'aebc0a0b-0b09-44ea-85ef-83ad0ac64fb3', 'checksum': 'md5:ac7959b6085a30cab3d8778a32d54d1a', 'ext': 'png', 'size': 16885, 'mimetype': 'image/png', 'key': 'shap_summary.png', 'metadata': {'width': 1150, 'height': 660}, 'access': {'hidden': False}}, 'roc_curve_cls_2.png': {'id': '2dfc6ced-6321-42dc-80bf-294dfaffe88c', 'checksum': 'md5:c076643c49b9ae7ba3cf0794b18f74db', 'ext': 'png', 'size': 20227, 'mimetype': 'image/png', 'key': 'roc_curve_cls_2.png', 'metadata': {'width': 640, 'height': 480}, 'access': {'hidden': False}}, 'confusion_matrix.png': {'id': 'dde4083c-8029-44de-b64e-bdef98e75601', 'checksum': 'md5:41c356960df1b924d7db4d4a5cc9b254', 'ext': 'png', 'size': 14453, 'mimetype': 'image/png', 'key': 'confusion_matrix.png', 'metadata': {'width': 600, 'height': 600}, 'access': {'hidden': False}}, 'feature_importances.png': {'id': 'cdfb75da-1e15-4fdb-a5fe-3bc5e181bf84', 'checksum': 'md5:1702afd2578c67988efdcf1f28fe1b04', 'ext': 'png', 'size': 19167, 'mimetype': 'image/png', 'key': 'feature_importances.png', 'metadata': {'width': 800, 'height': 600}, 'access': {'hidden': False}}, 'pr_curve_cls_0.png': {'id': '363871b4-6f5a-4c89-b197-e5cc5cbccf2f', 'checksum': 'md5:30a6a3742bcbd69750bc5fd30aea58d2', 'ext': 'png', 'size': 20358, 'mimetype': 'image/png', 'key': 'pr_curve_cls_0.png', 'metadata': {'width': 640, 'height': 480}, 'access': {'hidden': False}}, 'pr_curve_cls_1.png': {'id': '77ca9443-8953-4b20-8b6f-293961b4eabd', 'checksum': 'md5:7f396723afbfa90eb3d5f3e850f52c80', 'ext': 'png', 'size': 20226, 'mimetype': 'image/png', 'key': 'pr_curve_cls_1.png', 'metadata': {'width': 640, 'height': 480}, 'access': {'hidden': False}}, 'pr_curve_cls_2.png': {'id': '74ce57eb-f11d-434c-b030-7f045c467f48', 'checksum': 'md5:22a124dad652fd24fb3a5691abcae494', 'ext': 'png', 'size': 19809, 'mimetype': 'image/png', 'key': 'pr_curve_cls_2.png', 'metadata': {'width': 640, 'height': 480}, 'access': {'hidden': False}}, 'roc_curve_cls_0.png': {'id': 'f0e6047e-2af3-477d-936d-6041ed4d5d85', 'checksum': 'md5:f6d5e559a27d81fb010b416bddc2fb02', 'ext': 'png', 'size': 20100, 'mimetype': 'image/png', 'key': 'roc_curve_cls_0.png', 'metadata': {'width': 640, 'height': 480}, 'access': {'hidden': False}}}}, 'media_files': {'enabled': False, 'order': [], 'count': 0, 'total_bytes': 0, 'entries': {}}, 'status': 'published', 'deletion_status': {'is_deleted': False, 'status': 'P'}, 'stats': {'this_version': {'views': 0, 'unique_views': 0, 'downloads': 0, 'unique_downloads': 0, 'data_volume': 0.0}, 'all_versions': {'views': 0, 'unique_views': 0, 'downloads': 0, 'unique_downloads': 0, 'data_volume': 0.0}}}], 'total': 101}, 'aggregations': {'access_status': {'buckets': [{'key': 'metadata-only', 'doc_count': 100, 'label': 'Metadata-only', 'is_selected': False}, {'key': 'open', 'doc_count': 1, 'label': 'Open', 'is_selected': False}], 'label': 'Access status'}, 'file_type': {'buckets': [{'key': 'png', 'doc_count': 1, 'label': 'PNG', 'is_selected': False}], 'label': 'File type'}, 'resource_type': {'buckets': [{'key': 'publication', 'doc_count': 50, 'label': 'Publication', 'is_selected': False, 'inner': {'buckets': [{'key': 'publication-annotationcollection', 'doc_count': 4, 'label': 'Annotation collection', 'is_selected': False}, {'key': 'publication-datapaper', 'doc_count': 4, 'label': 'Data paper', 'is_selected': False}, {'key': 'publication-peerreview', 'doc_count': 4, 'label': 'Peer review', 'is_selected': False}, {'key': 'publication-deliverable', 'doc_count': 3, 'label': 'Project deliverable', 'is_selected': False}, {'key': 'publication-preprint', 'doc_count': 3, 'label': 'Preprint', 'is_selected': False}, {'key': 'publication-section', 'doc_count': 3, 'label': 'Book chapter', 'is_selected': False}, {'key': 'publication-taxonomictreatment', 'doc_count': 3, 'label': 'Taxonomic treatment', 'is_selected': False}, {'key': 'publication-thesis', 'doc_count': 3, 'label': 'Thesis', 'is_selected': False}, {'key': 'publication-book', 'doc_count': 2, 'label': 'Book', 'is_selected': False}, {'key': 'publication-datamanagementplan', 'doc_count': 2, 'label': 'Output management plan', 'is_selected': False}]}}, {'key': 'image', 'doc_count': 19, 'label': 'Image', 'is_selected': False, 'inner': {'buckets': [{'key': 'image-diagram', 'doc_count': 5, 'label': 'Diagram', 'is_selected': False}, {'key': 'image-other', 'doc_count': 5, 'label': 'Other', 'is_selected': False}, {'key': 'image-photo', 'doc_count': 4, 'label': 'Photo', 'is_selected': False}, {'key': 'image-plot', 'doc_count': 2, 'label': 'Plot', 'is_selected': False}, {'key': 'image-drawing', 'doc_count': 1, 'label': 'Drawing', 'is_selected': False}, {'key': 'image-figure', 'doc_count': 1, 'label': 'Figure', 'is_selected': False}]}}, {'key': 'software', 'doc_count': 6, 'label': 'Software', 'is_selected': False, 'inner': {'buckets': [{'key': 'software-computationalnotebook', 'doc_count': 2, 'label': 'Computational notebook', 'is_selected': False}]}}, {'key': 'model', 'doc_count': 5, 'label': 'Model', 'is_selected': False, 'inner': {'buckets': []}}, {'key': 'dataset', 'doc_count': 4, 'label': 'Dataset', 'is_selected': False, 'inner': {'buckets': []}}, {'key': 'other', 'doc_count': 4, 'label': 'Other', 'is_selected': False, 'inner': {'buckets': []}}, {'key': 'workflow', 'doc_count': 3, 'label': 'Workflow', 'is_selected': False, 'inner': {'buckets': []}}, {'key': 'audio', 'doc_count': 2, 'label': 'Audio', 'is_selected': False, 'inner': {'buckets': []}}, {'key': 'physicalobject', 'doc_count': 2, 'label': 'Physical object', 'is_selected': False, 'inner': {'buckets': []}}, {'key': 'presentation', 'doc_count': 2, 'label': 'Presentation', 'is_selected': False, 'inner': {'buckets': []}}], 'label': 'Resource types'}}, 'sortBy': 'newest', 'links': {'self': 'https://127.0.0.1:5000/api/records?page=1&size=1&sort=newest', 'next': 'https://127.0.0.1:5000/api/records?page=2&size=1&sort=newest'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_BASE = \"https://127.0.0.1:5000\"\n",
    "TOKEN    = \"8LnqJuz3TsBHffnDJ3isPLHYHtRbWrC0M667Nb5haEbnXpWqGbFRyfDApymr\"\n",
    "\n",
    "# 1) Test read‐scope by listing records (no size param or size=1)\n",
    "resp = requests.get(\n",
    "    f\"{API_BASE}/api/records\",\n",
    "    headers={\"Authorization\": f\"Bearer {TOKEN}\"},\n",
    "    verify=False\n",
    ")\n",
    "print(resp.status_code)\n",
    "# should be 200 and a JSON page of records\n",
    "\n",
    "# or explicitly:\n",
    "resp = requests.get(\n",
    "    f\"{API_BASE}/api/records?size=1\",\n",
    "    headers={\"Authorization\": f\"Bearer {TOKEN}\"},\n",
    "    verify=False\n",
    ")\n",
    "print(resp.status_code, resp.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8965535e-fa56-49c6-a489-e75b63900fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed methods: HEAD, POST, OPTIONS, GET\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_BASE = \"https://127.0.0.1:5000\"\n",
    "TOKEN    = \"8LnqJuz3TsBHffnDJ3isPLHYHtRbWrC0M667Nb5haEbnXpWqGbFRyfDApymr\"\n",
    "\n",
    "resp = requests.options(\n",
    "    f\"{API_BASE}/api/records\",\n",
    "    headers={\"Authorization\": f\"Bearer {TOKEN}\"},\n",
    "    verify=False\n",
    ")\n",
    "print(\"Allowed methods:\", resp.headers.get(\"Allow\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7e5b2cc5-ecf3-4e13-8cac-47f57f12cbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Draft created: p8a8y-1bn93\n",
      "  • Uploaded RandomForest_Iris_v20250423_230422.pkl\n",
      "  • Uploaded RandomForest_Iris_v20250424_111946.pkl\n",
      "  • Uploaded RandomForest_Iris_v20250423_230422/confusion_matrix.png\n",
      "  • Uploaded RandomForest_Iris_v20250423_230422/feature_importances.png\n",
      "  • Uploaded RandomForest_Iris_v20250423_230422/pr_curve_cls_0.png\n",
      "  • Uploaded RandomForest_Iris_v20250423_230422/pr_curve_cls_1.png\n",
      "  • Uploaded RandomForest_Iris_v20250423_230422/pr_curve_cls_2.png\n",
      "  • Uploaded RandomForest_Iris_v20250423_230422/roc_curve_cls_0.png\n",
      "  • Uploaded RandomForest_Iris_v20250423_230422/roc_curve_cls_1.png\n",
      "  • Uploaded RandomForest_Iris_v20250423_230422/roc_curve_cls_2.png\n",
      "  • Uploaded RandomForest_Iris_v20250423_230422/shap_summary.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_110923/confusion_matrix.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_110923/feature_importances.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_110923/pr_curve_cls_0.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_110923/pr_curve_cls_1.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_110923/pr_curve_cls_2.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_110923/roc_curve_cls_0.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_110923/roc_curve_cls_1.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_110923/roc_curve_cls_2.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_110923/shap_summary.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_111946/confusion_matrix.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_111946/feature_importances.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_111946/pr_curve_cls_0.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_111946/pr_curve_cls_1.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_111946/pr_curve_cls_2.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_111946/roc_curve_cls_0.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_111946/roc_curve_cls_1.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_111946/roc_curve_cls_2.png\n",
      "  • Uploaded RandomForest_Iris_v20250424_111946/shap_summary.png\n",
      "  • Uploaded RandomForest_Iris_v20250423_230422.jsonld\n",
      "  • Uploaded RandomForest_Iris_v20250423_230422.ttl\n",
      "  • Uploaded RandomForest_Iris_v20250423_230422_run_summary.json\n",
      "  • Uploaded RandomForest_Iris_v20250424_110923_run_summary.json\n",
      "  • Uploaded RandomForest_Iris_v20250424_111946_run_summary.json\n",
      "  • Uploaded .ipynb_checkpoints/RandomForest_Iris_v20250423_230422_run_summary-checkpoint.json\n",
      "  • Uploaded .ipynb_checkpoints/RandomForest_Iris_v20250424_110923_run_summary-checkpoint.json\n",
      "  • Uploaded .ipynb_checkpoints/RandomForest_Iris_v20250424_111946_run_summary-checkpoint.json\n",
      "✅ Published: p8a8y-1bn93\n",
      "✅ Metadata fetched successfully\n",
      "✅ Metadata saved as metadata_p8a8y-1bn93.json\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os, json, requests\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "API_BASE   = \"https://127.0.0.1:5000\"\n",
    "TOKEN      = \"8LnqJuz3TsBHffnDJ3isPLHYHtRbWrC0M667Nb5haEbnXpWqGbFRyfDApymr\"\n",
    "VERIFY_SSL = False  # only for self‐signed dev\n",
    "\n",
    "HEADERS_JSON = {\n",
    "    \"Accept\":        \"application/json\",\n",
    "    \"Content-Type\":  \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "}\n",
    "\n",
    "HEADERS_OCTET = {\n",
    "    \"Content-Type\":  \"application/octet-stream\",\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "}\n",
    "\n",
    "# The folders you want to walk & upload:\n",
    "TO_UPLOAD = [\"Trained_models\", \"plots\", \"MODEL_PROVENANCE\"]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Create draft with ALL required metadata\n",
    "# -----------------------------------------------------------------------------\n",
    "def create_draft():\n",
    "    payload = {\n",
    "  \"metadata\": {\n",
    "    \"title\":            \"RandomForest Iris Model Artifacts\",\n",
    "    \"creators\": [ {\n",
    "      \"person_or_org\": {\n",
    "        \"type\":        \"personal\",\n",
    "        \"given_name\":  \"Reema\",\n",
    "        \"family_name\": \"Dass\"\n",
    "      }\n",
    "    } ],\n",
    "    \"publication_date\": \"2025-04-24\",\n",
    "    \"resource_type\":    { \"id\": \"software\" },\n",
    "    \"access\": {\n",
    "      \"record\": \"public\",\n",
    "      \"files\":  \"public\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "    r = requests.post(f\"{API_BASE}/api/records\",\n",
    "                      headers=HEADERS_JSON,\n",
    "                      json=payload,\n",
    "                      verify=VERIFY_SSL)\n",
    "    r.raise_for_status()\n",
    "    draft = r.json()\n",
    "    print(\"✅ Draft created:\", draft[\"id\"])\n",
    "    return draft[\"id\"], draft[\"links\"]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Register, upload and commit a single file\n",
    "# -----------------------------------------------------------------------------\n",
    "def upload_and_commit(links, key, path):\n",
    "    # 2a) register the filename in the draft\n",
    "    r1 = requests.post(links[\"files\"],\n",
    "                       headers=HEADERS_JSON,\n",
    "                       json=[{\"key\": key}],\n",
    "                       verify=VERIFY_SSL)\n",
    "    r1.raise_for_status()\n",
    "    entry = next(e for e in r1.json()[\"entries\"] if e[\"key\"] == key)\n",
    "    file_links = entry[\"links\"]\n",
    "\n",
    "    # 2b) upload the bytes\n",
    "    with open(path, \"rb\") as fp:\n",
    "        r2 = requests.put(file_links[\"content\"],\n",
    "                          headers=HEADERS_OCTET,\n",
    "                          data=fp,\n",
    "                          verify=VERIFY_SSL)\n",
    "    r2.raise_for_status()\n",
    "\n",
    "    # 2c) commit the upload\n",
    "    r3 = requests.post(file_links[\"commit\"],\n",
    "                       headers=HEADERS_JSON,\n",
    "                       verify=VERIFY_SSL)\n",
    "    r3.raise_for_status()\n",
    "    print(f\"  • Uploaded {key}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Walk each folder and upload every file\n",
    "# -----------------------------------------------------------------------------\n",
    "def upload_folder(links):\n",
    "    for folder in TO_UPLOAD:\n",
    "        if not os.path.isdir(folder):\n",
    "            print(f\"⚠️ Skipping missing folder {folder}\")\n",
    "            continue\n",
    "        base = os.path.dirname(folder) or folder\n",
    "        for root, _, files in os.walk(folder):\n",
    "            for fn in files:\n",
    "                local = os.path.join(root, fn)\n",
    "                # create a POSIX‐style key preserving subfolders\n",
    "                key = os.path.relpath(local, start=base).replace(os.sep, \"/\")\n",
    "                upload_and_commit(links, key, local)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Publish the draft\n",
    "# -----------------------------------------------------------------------------\n",
    "def publish(links):\n",
    "    r = requests.post(links[\"publish\"],\n",
    "                      headers=HEADERS_JSON,\n",
    "                      verify=VERIFY_SSL)\n",
    "    if not r.ok:\n",
    "        print(\"❌ Publish failed:\", r.status_code, r.text)\n",
    "        try: print(r.json())\n",
    "        except: pass\n",
    "        r.raise_for_status()\n",
    "    print(\"✅ Published:\", r.json()[\"id\"])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Fetch metadata and save to a file\n",
    "# -----------------------------------------------------------------------------\n",
    "def fetch_metadata(record_id):\n",
    "    r = requests.get(f\"{API_BASE}/api/records/{record_id}\",\n",
    "                     headers=HEADERS_JSON,\n",
    "                     verify=VERIFY_SSL)\n",
    "    r.raise_for_status()\n",
    "    metadata = r.json()\n",
    "    print(\"✅ Metadata fetched successfully\")\n",
    "    \n",
    "    # Save the metadata to a file\n",
    "    with open(f\"metadata_{record_id}.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "    print(f\"✅ Metadata saved as metadata_{record_id}.json\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    recid, links = create_draft()\n",
    "    upload_folder(links)\n",
    "    publish(links)\n",
    "\n",
    "    # Fetch and save metadata after publishing\n",
    "    print(fetch_metadata(recid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0013878b-37da-4a22-9586-3773531bfd01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Original Metadata (start): {\n",
      "    \"id\": \"p8a8y-1bn93\",\n",
      "    \"created\": \"2025-04-24T13:48:33.108906+00:00\",\n",
      "    \"updated\": \"2025-04-24T13:48:34.519283+00:00\",\n",
      "    \"links\": {\n",
      "        \"self\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93\",\n",
      "        \"self_html\": \"https://127.0.0.1:5000/records/p8a8y-1bn93\",\n",
      "        \"parent\": \"https://127.0.0.1:5000/api/records/0r0p8-gzf02\",\n",
      "        \"parent_html\": \"https://127.0.0.1:5000/records/0r0p8-gzf02\",\n",
      "        \"self_iiif_manifest\": \"https://127.0.0.1:5000/api/iiif/record:p8a8y-1bn93/manifest\",\n",
      "        \"self_iiif_sequence\": \"https://127.0.0.1:5000/api/iiif/record:p8a8y-1bn93/sequence/default\",\n",
      "        \"files\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files\",\n",
      "        \"media_files\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/media-files\",\n",
      "        \"archive\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files-archive\",\n",
      "        \"archive_media\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/media-files-archive\",\n",
      "        \"latest\": \"https://127.0.0.1:5000/api/records/p8\n",
      "Debug: Metadata loaded successfully\n",
      "p8a8y-1bn93\n",
      "Debug: Extracting fields from metadata...\n",
      "Debug: Extracted Metadata: {\n",
      "    \"invenio_metadata\": {\n",
      "        \"id\": \"p8a8y-1bn93\",\n",
      "        \"title\": \"RandomForest Iris Model Artifacts\",\n",
      "        \"creator\": \"Dass, Reema\",\n",
      "        \"publication_date\": \"2025-04-24\",\n",
      "        \"files\": [\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250423_230422.pkl\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250423_230422.pkl/content\",\n",
      "                \"size\": 282910,\n",
      "                \"mimetype\": \"application/octet-stream\",\n",
      "                \"checksum\": \"md5:a9f9e15b9c808d94c8e5737089beaa7d\",\n",
      "                \"metadata\": {}\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_110923/pr_curve_cls_1.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_110923/pr_curve_cls_1.png/content\",\n",
      "                \"size\": 20226,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:7f396723afbfa90eb3d5f3e850f52c80\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250423_230422/confusion_matrix.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250423_230422/confusion_matrix.png/content\",\n",
      "                \"size\": 14453,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:41c356960df1b924d7db4d4a5cc9b254\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 600,\n",
      "                    \"height\": 600\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_110923/feature_importances.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_110923/feature_importances.png/content\",\n",
      "                \"size\": 19167,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:1702afd2578c67988efdcf1f28fe1b04\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 800,\n",
      "                    \"height\": 600\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250423_230422/roc_curve_cls_1.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250423_230422/roc_curve_cls_1.png/content\",\n",
      "                \"size\": 20143,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:917e9847fda527b030c0bcad8f2a2ea3\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250423_230422/pr_curve_cls_0.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250423_230422/pr_curve_cls_0.png/content\",\n",
      "                \"size\": 20358,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:30a6a3742bcbd69750bc5fd30aea58d2\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250423_230422/pr_curve_cls_2.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250423_230422/pr_curve_cls_2.png/content\",\n",
      "                \"size\": 19809,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:22a124dad652fd24fb3a5691abcae494\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_110923/pr_curve_cls_0.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_110923/pr_curve_cls_0.png/content\",\n",
      "                \"size\": 20358,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:30a6a3742bcbd69750bc5fd30aea58d2\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250423_230422/shap_summary.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250423_230422/shap_summary.png/content\",\n",
      "                \"size\": 16939,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:ac330a1d4feeb8463ca1e9551303c4c8\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 1150,\n",
      "                    \"height\": 660\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_110923/roc_curve_cls_0.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_110923/roc_curve_cls_0.png/content\",\n",
      "                \"size\": 20100,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:f6d5e559a27d81fb010b416bddc2fb02\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_110923/pr_curve_cls_2.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_110923/pr_curve_cls_2.png/content\",\n",
      "                \"size\": 19809,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:22a124dad652fd24fb3a5691abcae494\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_110923/roc_curve_cls_1.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_110923/roc_curve_cls_1.png/content\",\n",
      "                \"size\": 20143,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:917e9847fda527b030c0bcad8f2a2ea3\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_110923/roc_curve_cls_2.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_110923/roc_curve_cls_2.png/content\",\n",
      "                \"size\": 20227,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:c076643c49b9ae7ba3cf0794b18f74db\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_110923/shap_summary.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_110923/shap_summary.png/content\",\n",
      "                \"size\": 16821,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:28ee8be9c0dbf6dc16c4acd70b42273d\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 1150,\n",
      "                    \"height\": 660\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_111946/confusion_matrix.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_111946/confusion_matrix.png/content\",\n",
      "                \"size\": 14453,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:41c356960df1b924d7db4d4a5cc9b254\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 600,\n",
      "                    \"height\": 600\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250423_230422/pr_curve_cls_1.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250423_230422/pr_curve_cls_1.png/content\",\n",
      "                \"size\": 20226,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:7f396723afbfa90eb3d5f3e850f52c80\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_111946.pkl\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_111946.pkl/content\",\n",
      "                \"size\": 282910,\n",
      "                \"mimetype\": \"application/octet-stream\",\n",
      "                \"checksum\": \"md5:5055123396a01237596e771a2621a82f\",\n",
      "                \"metadata\": {}\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250423_230422/feature_importances.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250423_230422/feature_importances.png/content\",\n",
      "                \"size\": 19167,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:1702afd2578c67988efdcf1f28fe1b04\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 800,\n",
      "                    \"height\": 600\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250423_230422/roc_curve_cls_0.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250423_230422/roc_curve_cls_0.png/content\",\n",
      "                \"size\": 20100,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:f6d5e559a27d81fb010b416bddc2fb02\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250423_230422/roc_curve_cls_2.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250423_230422/roc_curve_cls_2.png/content\",\n",
      "                \"size\": 20227,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:c076643c49b9ae7ba3cf0794b18f74db\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_110923/confusion_matrix.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_110923/confusion_matrix.png/content\",\n",
      "                \"size\": 14453,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:41c356960df1b924d7db4d4a5cc9b254\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 600,\n",
      "                    \"height\": 600\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_111946/feature_importances.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_111946/feature_importances.png/content\",\n",
      "                \"size\": 19167,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:1702afd2578c67988efdcf1f28fe1b04\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 800,\n",
      "                    \"height\": 600\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_111946/pr_curve_cls_1.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_111946/pr_curve_cls_1.png/content\",\n",
      "                \"size\": 20226,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:7f396723afbfa90eb3d5f3e850f52c80\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_111946/roc_curve_cls_0.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_111946/roc_curve_cls_0.png/content\",\n",
      "                \"size\": 20100,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:f6d5e559a27d81fb010b416bddc2fb02\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_111946/roc_curve_cls_2.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_111946/roc_curve_cls_2.png/content\",\n",
      "                \"size\": 20227,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:c076643c49b9ae7ba3cf0794b18f74db\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_111946/pr_curve_cls_0.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_111946/pr_curve_cls_0.png/content\",\n",
      "                \"size\": 20358,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:30a6a3742bcbd69750bc5fd30aea58d2\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_111946/pr_curve_cls_2.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_111946/pr_curve_cls_2.png/content\",\n",
      "                \"size\": 19809,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:22a124dad652fd24fb3a5691abcae494\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_111946/roc_curve_cls_1.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_111946/roc_curve_cls_1.png/content\",\n",
      "                \"size\": 20143,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:917e9847fda527b030c0bcad8f2a2ea3\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 640,\n",
      "                    \"height\": 480\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_111946/shap_summary.png\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_111946/shap_summary.png/content\",\n",
      "                \"size\": 16885,\n",
      "                \"mimetype\": \"image/png\",\n",
      "                \"checksum\": \"md5:ac7959b6085a30cab3d8778a32d54d1a\",\n",
      "                \"metadata\": {\n",
      "                    \"width\": 1150,\n",
      "                    \"height\": 660\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250423_230422.jsonld\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250423_230422.jsonld/content\",\n",
      "                \"size\": 11745,\n",
      "                \"mimetype\": \"application/ld+json\",\n",
      "                \"checksum\": \"md5:e037b37c0a582b6f32c1c638c003480b\",\n",
      "                \"metadata\": {}\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250423_230422.ttl\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250423_230422.ttl/content\",\n",
      "                \"size\": 10107,\n",
      "                \"mimetype\": \"text/turtle\",\n",
      "                \"checksum\": \"md5:50f91e3780c4abfb55bf58d6ac5388d7\",\n",
      "                \"metadata\": {}\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250423_230422_run_summary.json\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250423_230422_run_summary.json/content\",\n",
      "                \"size\": 135528,\n",
      "                \"mimetype\": \"application/json\",\n",
      "                \"checksum\": \"md5:b017a4306d62eb2ef12f95d992b2946d\",\n",
      "                \"metadata\": {}\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_110923_run_summary.json\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_110923_run_summary.json/content\",\n",
      "                \"size\": 152427,\n",
      "                \"mimetype\": \"application/json\",\n",
      "                \"checksum\": \"md5:dbec09724b35b554410d3df63d89e334\",\n",
      "                \"metadata\": {}\n",
      "            },\n",
      "            {\n",
      "                \"key\": \"RandomForest_Iris_v20250424_111946_run_summary.json\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/RandomForest_Iris_v20250424_111946_run_summary.json/content\",\n",
      "                \"size\": 155273,\n",
      "                \"mimetype\": \"application/json\",\n",
      "                \"checksum\": \"md5:1b0f020894735a7e335f8dd9715b8157\",\n",
      "                \"metadata\": {}\n",
      "            },\n",
      "            {\n",
      "                \"key\": \".ipynb_checkpoints/RandomForest_Iris_v20250423_230422_run_summary-checkpoint.json\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/.ipynb_checkpoints/RandomForest_Iris_v20250423_230422_run_summary-checkpoint.json/content\",\n",
      "                \"size\": 135528,\n",
      "                \"mimetype\": \"application/json\",\n",
      "                \"checksum\": \"md5:b017a4306d62eb2ef12f95d992b2946d\",\n",
      "                \"metadata\": {}\n",
      "            },\n",
      "            {\n",
      "                \"key\": \".ipynb_checkpoints/RandomForest_Iris_v20250424_110923_run_summary-checkpoint.json\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/.ipynb_checkpoints/RandomForest_Iris_v20250424_110923_run_summary-checkpoint.json/content\",\n",
      "                \"size\": 152427,\n",
      "                \"mimetype\": \"application/json\",\n",
      "                \"checksum\": \"md5:dbec09724b35b554410d3df63d89e334\",\n",
      "                \"metadata\": {}\n",
      "            },\n",
      "            {\n",
      "                \"key\": \".ipynb_checkpoints/RandomForest_Iris_v20250424_111946_run_summary-checkpoint.json\",\n",
      "                \"url\": \"https://127.0.0.1:5000/api/records/p8a8y-1bn93/files/.ipynb_checkpoints/RandomForest_Iris_v20250424_111946_run_summary-checkpoint.json/content\",\n",
      "                \"size\": 155273,\n",
      "                \"mimetype\": \"application/json\",\n",
      "                \"checksum\": \"md5:1b0f020894735a7e335f8dd9715b8157\",\n",
      "                \"metadata\": {}\n",
      "            }\n",
      "        ],\n",
      "        \"pids\": {\n",
      "            \"oai\": {\n",
      "                \"identifier\": \"oai:my-site.com:p8a8y-1bn93\",\n",
      "                \"provider\": \"oai\"\n",
      "            }\n",
      "        },\n",
      "        \"version_info\": {\n",
      "            \"is_latest\": true,\n",
      "            \"is_latest_draft\": true,\n",
      "            \"index\": 1\n",
      "        },\n",
      "        \"status\": \"published\",\n",
      "        \"views\": 0,\n",
      "        \"downloads\": 0\n",
      "    }\n",
      "}\n",
      "✅ New dynamic metadata added successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to dynamically extract and structure metadata from the original JSON\n",
    "def extract_metadata(metadata):\n",
    "    # Debug: Check if metadata is loaded correctly\n",
    "    print(\"Debug: Metadata loaded successfully\")\n",
    "    print(metadata.get(\"id\", \"\"))  # Check if 'id' is being fetched\n",
    "\n",
    "    # Check if the required fields are in the metadata\n",
    "    print(\"Debug: Extracting fields from metadata...\")\n",
    "\n",
    "    extracted_data = {\n",
    "        \"invenio_metadata\": {\n",
    "            \"id\": metadata.get(\"id\", \"\"),\n",
    "            \"title\": metadata.get(\"metadata\", {}).get(\"title\", \"\"),\n",
    "            \"creator\": \", \".join([creator[\"person_or_org\"].get(\"name\", \"\") for creator in metadata.get(\"metadata\", {}).get(\"creators\", [])]),\n",
    "            \"publication_date\": metadata.get(\"metadata\", {}).get(\"publication_date\", \"\"),\n",
    "            \"files\": [],  # Initialize 'files' as a list\n",
    "            \"pids\": metadata.get(\"pids\", {}),\n",
    "            \"version_info\": metadata.get(\"versions\", {}),\n",
    "            \"status\": metadata.get(\"status\", \"\"),\n",
    "            \"views\": metadata.get(\"stats\", {}).get(\"this_version\", {}).get(\"views\", 0),\n",
    "            \"downloads\": metadata.get(\"stats\", {}).get(\"this_version\", {}).get(\"downloads\", 0),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Extract file details from the metadata\n",
    "    for key, file_info in metadata.get(\"files\", {}).get(\"entries\", {}).items():\n",
    "        file_detail = {\n",
    "            \"key\": key,\n",
    "            \"url\": file_info[\"links\"].get(\"content\", \"\"),\n",
    "            \"size\": file_info.get(\"size\", 0),\n",
    "            \"mimetype\": file_info.get(\"mimetype\", \"\"),\n",
    "            \"checksum\": file_info.get(\"checksum\", \"\"),\n",
    "            \"metadata\": file_info.get(\"metadata\", {}),\n",
    "        }\n",
    "        extracted_data[\"invenio_metadata\"][\"files\"].append(file_detail)  # Append to the 'files' list\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "# Load the original metadata from the JSON file (replace with your actual file path)\n",
    "with open('metadata_p8a8y-1bn93.json', 'r') as f: \n",
    "    original_metadata = json.load(f)\n",
    "\n",
    "# Debugging: print out the first part of the original metadata to verify its structure\n",
    "print(\"Debug: Original Metadata (start):\", json.dumps(original_metadata, indent=4)[:1000])  # Print only the start for review\n",
    "\n",
    "# Extract relevant details dynamically\n",
    "extracted_metadata = extract_metadata(original_metadata)\n",
    "\n",
    "# Debugging: print the extracted metadata to verify it's correct\n",
    "print(\"Debug: Extracted Metadata:\", json.dumps(extracted_metadata, indent=4))\n",
    "\n",
    "# Load the existing JSON file (replace with your actual file path)\n",
    "with open('MODEL_PROVENANCE/RandomForest_Iris_v20250424_111946_run_summary.json', 'r') as f:\n",
    "    existing_metadata = json.load(f)\n",
    "\n",
    "# Add the extracted metadata as a new node\n",
    "existing_metadata.update(extracted_metadata)\n",
    "\n",
    "# Save the updated metadata back to the file\n",
    "with open('updated_metadata.json', 'w') as f:\n",
    "    json.dump(existing_metadata, f, indent=4)\n",
    "\n",
    "print(\"✅ New dynamic metadata added successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55d4a99-2bf2-4bd2-aeb0-4cbcb3698f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "38a807a7-6ecd-4ea7-93ac-78c0f853825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "mlflow.sklearn.autolog()\n",
    "with mlflow.start_run() as run:\n",
    "\n",
    "    model = RandomForestClassifier(**hyperparams)\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fa169-a5e2-47b3-b7f5-44f9577f22ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b7a46-a70d-44ea-976c-322a1a795311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # ============================\n",
    "# # 🚀 Start MLflow Run CURRENT BACKUP\n",
    "# # ============================\n",
    "# with mlflow.start_run() as run:\n",
    "#     model_name = f\"RandomForest_Iris_v1.0.0\"\n",
    "#     training_time_start = time.time()\n",
    "\n",
    "#     # 📈 Model Training\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#     model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     y_proba = model.predict_proba(X_test)\n",
    "#     acc = accuracy_score(y_test, y_pred)\n",
    "#     auc = roc_auc_score(y_test, y_proba, multi_class=\"ovr\")\n",
    "\n",
    "#     # ✅ Log Environment Automatically\n",
    "#     mlflow.log_params({\n",
    "#         \"python_version\": platform.python_version(),\n",
    "#         \"os_platform\": f\"{platform.system()} {platform.release()}\",\n",
    "#         \"sklearn_version\": sklearn.__version__,\n",
    "#         \"pandas_version\": pd.__version__,\n",
    "#         \"numpy_version\": np.__version__,\n",
    "#         \"matplotlib_version\": matplotlib.__version__,\n",
    "#         \"seaborn_version\": sns.__version__,\n",
    "#         \"shap_version\": shap.__version__,\n",
    "#     })\n",
    "\n",
    "#     # ✅ Git and Notebook Metadata\n",
    "#     mlflow.set_tag(\"notebook_name\", \"RQ1.ipynb\")\n",
    "\n",
    "#     # ✅ Dataset Metadata Tags\n",
    "#     mlflow.set_tag(\"dataset_name\", \"Iris\") #TODO\n",
    "#     mlflow.set_tag(\"dataset_version\", \"1.0.0\") #TODO\n",
    "#     mlflow.set_tag(\"dataset_id\", \"iris_local\") #TODO\n",
    "\n",
    "#     # ✅ Confusion Matrix Plot\n",
    "#     cm = confusion_matrix(y_test, y_pred)\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "#     plt.title(\"Confusion Matrix\")\n",
    "#     plt.xlabel(\"Predicted\")\n",
    "#     plt.ylabel(\"Actual\")\n",
    "#     cm_path = \"confusion_matrix.png\"\n",
    "#     plt.savefig(cm_path)\n",
    "#     mlflow.log_artifact(cm_path)\n",
    "\n",
    "#     # ✅ SHAP Summary\n",
    "#     explainer = shap.TreeExplainer(model)\n",
    "#     shap_values = explainer.shap_values(X_test)\n",
    "#     shap.summary_plot(shap_values, X_test, show=False)\n",
    "#     shap_path = \"shap_summary.png\"\n",
    "#     plt.savefig(shap_path)\n",
    "#     mlflow.log_artifact(shap_path)\n",
    "\n",
    "\n",
    "#     def get_latest_commit_hash(repo_path=\".\"):\n",
    "#         # returns the full SHA of HEAD\n",
    "#         res = subprocess.run(\n",
    "#             [\"git\", \"-C\", repo_path, \"rev-parse\", \"HEAD\"],\n",
    "#             capture_output=True, text=True, check=True\n",
    "        \n",
    "#         return res.stdout.strip()\n",
    "\n",
    "#     def get_remote_url(repo_path=\".\", remote=\"origin\"):\n",
    "#         # returns something like git@github.com:user/repo.git or https://...\n",
    "#         res = subprocess.run(\n",
    "#             [\"git\", \"-C\", repo_path, \"config\", \"--get\", f\"remote.{remote}.url\"],\n",
    "#             capture_output=True, text=True, check=True\n",
    "#         )\n",
    "#         return res.stdout.strip()\n",
    "    \n",
    "#     def make_commit_link(remote_url, commit_hash):\n",
    "#         # handle GitHub/GitLab convention; strip “.git” if present\n",
    "#         base = remote_url.rstrip(\".git\")\n",
    "#         # if SSH form (git@github.com:owner/repo), convert to https\n",
    "#         if base.startswith(\"git@\"):\n",
    "#             base = base.replace(\":\", \"/\").replace(\"git@\", \"https://\")\n",
    "#         return f\"{base}/commit/{commit_hash}\"\n",
    "\n",
    "    \n",
    "#     def simple_commit_and_push_and_log(repo_path=\".\", message=\"Auto commit\", remote=\"origin\", branch=\"main\"):\n",
    "#     # 1) Check for changes\n",
    "#         status = subprocess.run(\n",
    "#             [\"git\", \"-C\", repo_path, \"status\", \"--porcelain\"],\n",
    "#             capture_output=True, text=True\n",
    "#         )\n",
    "#         if not status.stdout.strip():\n",
    "#             print(\"🟡 No changes to commit.\")\n",
    "#             return None, None\n",
    "    \n",
    "#         # 2) Stage everything\n",
    "#         add = subprocess.run(\n",
    "#             [\"git\", \"-C\", repo_path, \"add\", \"--all\"],\n",
    "#             capture_output=True, text=True\n",
    "#         )\n",
    "#         if add.returncode:\n",
    "#             print(\"❌ git add failed:\\n\", add.stderr)\n",
    "#             return None, None\n",
    "    \n",
    "#         # 3) Commit\n",
    "#         commit = subprocess.run(\n",
    "#             [\"git\", \"-C\", repo_path, \"commit\", \"-m\", message],\n",
    "#             capture_output=True, text=True\n",
    "#         )\n",
    "#         if commit.returncode:\n",
    "#             print(\"❌ git commit failed:\\n\", commit.stderr)\n",
    "#             return None, None\n",
    "#         print(\"✅ Commit successful.\")\n",
    "    \n",
    "#         # 4) Push\n",
    "#         push = subprocess.run(\n",
    "#             [\"git\", \"-C\", repo_path, \"push\", \"-u\", remote, branch],\n",
    "#             capture_output=True, text=True\n",
    "#         )\n",
    "#         if push.returncode:\n",
    "#             print(\"❌ git push failed:\\n\", push.stderr)\n",
    "#         else:\n",
    "#             print(\"🚀 Push successful.\")\n",
    "    \n",
    "#         # 5) Retrieve hash & remote URL\n",
    "#         sha = get_latest_commit_hash(repo_path)\n",
    "#         url = get_remote_url(repo_path, remote)\n",
    "#         link = make_commit_link(url, sha)\n",
    "    \n",
    "#         return sha, link\n",
    "    \n",
    "      \n",
    "#     sha, link = simple_commit_and_push_and_log(\n",
    "#         repo_path=\".\",\n",
    "#         message=\"Auto commit after successful training\"\n",
    "#     )\n",
    "#     if sha and link:\n",
    "#         diff_text = subprocess.check_output(\n",
    "#             [\"git\", \"-C\", \".\", \"diff\", previous_commit_hash, sha], text=True\n",
    "#         )\n",
    "                \n",
    "#         # 1) Get your repo’s remote URL and normalize to HTTPS\n",
    "#         remote_url = subprocess.check_output(\n",
    "#             [\"git\", \"config\", \"--get\", \"remote.origin.url\"],\n",
    "#             text=True\n",
    "#         ).strip().rstrip(\".git\")\n",
    "#         if remote_url.startswith(\"git@\"):\n",
    "#             # git@github.com:owner/repo.git → https://github.com/owner/repo\n",
    "#             remote_url = remote_url.replace(\":\", \"/\").replace(\"git@\", \"https://\")\n",
    "        \n",
    "#         # 2) Build commit URLs\n",
    "#         previous_commit_url  = f\"{remote_url}/commit/{previous_commit_hash}\"\n",
    "#         current_commit_url = f\"{remote_url}/commit/{sha}\"\n",
    "#         diff_data = {\n",
    "#             \"previous_commit\":  previous_commit_hash,\n",
    "#             \"previous_commit_url\":previous_commit_url,\n",
    "#             \"current_commit_url\":current_commit_url,\n",
    "#             \"current_commit\": sha,\n",
    "#             \"diff\": diff_text\n",
    "#         }\n",
    "#         mlflow.log_dict(\n",
    "#             diff_data,\n",
    "#             artifact_file=\"commit_diff.json\"\n",
    "#         )\n",
    "#         mlflow.set_tag(\"git_previous_commit_hash\", previous_commit_hash)\n",
    "#         mlflow.set_tag(\"git_current_commit_hash\", sha)\n",
    "#         mlflow.set_tag(\"git__current_commit_url\", link) \n",
    "\n",
    "\n",
    "#     client   = MlflowClient()\n",
    "#     run_id    = run.info.run_id\n",
    "#     run_info  = client.get_run(run_id).info\n",
    "#     run_data  = client.get_run(run_id).data\n",
    "    \n",
    "#     # 1) params, metrics, tags\n",
    "#     params  = dict(run_data.params)\n",
    "#     metrics = dict(run_data.metrics)\n",
    "#     tags    = dict(run_data.tags)\n",
    "\n",
    "#     # (4) List artifacts under a specific subfolder\n",
    "#     artifact_paths = [af.path for af in client.list_artifacts(run_id)]\n",
    "    \n",
    "#     # # 2) recursively gather all artifact paths\n",
    "#     # artifact_paths = []\n",
    "#     # def _gather(path=\"\"):\n",
    "#     #     for af in client.list_artifacts(run_id, path):\n",
    "#     #         if af.is_dir:\n",
    "#     #             _gather(af.path)\n",
    "#     #         else:\n",
    "#     #             artifact_paths.append(af.path)\n",
    "#     # _gather()\n",
    "\n",
    "#     # 3) assemble summary\n",
    "#     summary = {\n",
    "#         \"run_id\":         run_id,\n",
    "#         \"run_name\": run_info.run_name,\n",
    "#         \"experiment_id\":  run_info.experiment_id,\n",
    "#         \"start_time\":     run_info.start_time,\n",
    "#         \"end_time\":       run_info.end_time,\n",
    "#         \"params\":         params,\n",
    "#         \"metrics\":        metrics,\n",
    "#         \"tags\":           tags,\n",
    "#         \"artifacts\":      artifact_paths\n",
    "#     }\n",
    "#     # 1) Create (or reuse) a base folder for run summaries\n",
    "#     base_dir = \"run_summaries\"\n",
    "#     os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "#    # 2) Pick next numeric folder\n",
    "#     existing = [\n",
    "#         d for d in os.listdir(base_dir)\n",
    "#         if os.path.isdir(os.path.join(base_dir, d)) and d.isdigit()\n",
    "#     ]\n",
    "#     next_num = max(map(int, existing), default=0) + 1\n",
    "\n",
    "#     mlflow.log_dict(\n",
    "#         summary,\n",
    "#         artifact_file=f\"run_summaries/{next_num}/run_summary.json\"\n",
    "#     )\n",
    "#     # 3) Save the summary JSON into that folder\n",
    "#     # local_path = os.path.join(run_folder, \"run_summary.json\")\n",
    "#     # with open(local_path, \"w\") as f:\n",
    "#     #     json.dump(summary, f, indent=2)\n",
    "#     # # 4) (Optional) Mirror it into MLflow artifacts under the same counter path\n",
    "#     # mlflow.log_artifact(local_path, artifact_path=f\"run_summaries/{next_num}\")\n",
    "    \n",
    "#     # 5) Also tag the run with the folder name for easy reference\n",
    "#     mlflow.set_tag(\"summary_folder\", str(next_num))\n",
    "#         # with open(\"model_metadata_fair4ml.json\", \"w\") as f:\n",
    "#     #     json.dump(fair4ml_metadata, f, indent=2)\n",
    "#     # mlflow.log_artifact(\"model_metadata_fair4ml.json\")\n",
    "\n",
    "#     mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211bdef-5785-472d-8ea5-0bc24a3faf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================\n",
    "# # 🚀 Start MLflow Run\n",
    "# # ============================\n",
    "# with mlflow.start_run() as run:\n",
    "#     model_name = f\"RandomForest_Iris_v1.0.0\"\n",
    "#     training_time_start = time.time()\n",
    "\n",
    "#     # 📈 Model Training\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#     model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     y_proba = model.predict_proba(X_test)\n",
    "#     acc = accuracy_score(y_test, y_pred)\n",
    "#     auc = roc_auc_score(y_test, y_proba, multi_class=\"ovr\")\n",
    "\n",
    "#     # ✅ Log Environment Automatically\n",
    "#     mlflow.log_params({\n",
    "#         \"python_version\": platform.python_version(),\n",
    "#         \"os_platform\": f\"{platform.system()} {platform.release()}\",\n",
    "#         \"sklearn_version\": sklearn.__version__,\n",
    "#         \"pandas_version\": pd.__version__,\n",
    "#         \"numpy_version\": np.__version__,\n",
    "#         \"matplotlib_version\": matplotlib.__version__,\n",
    "#         \"seaborn_version\": sns.__version__,\n",
    "#         \"shap_version\": shap.__version__,\n",
    "#     })\n",
    "\n",
    "#     # ✅ Git and Notebook Metadata\n",
    "#     mlflow.set_tag(\"git_commit_hash\", commit_hash)\n",
    "#     mlflow.set_tag(\"notebook_name\", \"RQ1.ipynb\")\n",
    "\n",
    "#     # ✅ Dataset Metadata Tags\n",
    "#     mlflow.set_tag(\"dataset_name\", \"Iris\")\n",
    "#     mlflow.set_tag(\"dataset_version\", \"1.0.0\")\n",
    "#     mlflow.set_tag(\"dataset_id\", \"iris_local\")\n",
    "\n",
    "#     # ✅ Confusion Matrix Plot\n",
    "#     cm = confusion_matrix(y_test, y_pred)\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "#     plt.title(\"Confusion Matrix\")\n",
    "#     plt.xlabel(\"Predicted\")\n",
    "#     plt.ylabel(\"Actual\")\n",
    "#     cm_path = \"confusion_matrix.png\"\n",
    "#     plt.savefig(cm_path)\n",
    "#     mlflow.log_artifact(cm_path)\n",
    "\n",
    "#     # ✅ SHAP Summary\n",
    "#     explainer = shap.TreeExplainer(model)\n",
    "#     shap_values = explainer.shap_values(X_test)\n",
    "#     shap.summary_plot(shap_values, X_test, show=False)\n",
    "#     shap_path = \"shap_summary.png\"\n",
    "#     plt.savefig(shap_path)\n",
    "#     mlflow.log_artifact(shap_path)\n",
    "\n",
    "#     # ✅ FAIR4ML-style Metadata JSON\n",
    "#     fair4ml_metadata = {\n",
    "#         \"@type\": \"MLModel\",\n",
    "#         \"name\": model_name,\n",
    "#         \"algorithm\": \"RandomForestClassifier\",\n",
    "#         \"hyperParameters\": model.get_params(),\n",
    "#         \"trainingDataset\": {\n",
    "#             \"name\": \"Iris\",\n",
    "#             \"version\": \"1.0.0\",\n",
    "#             \"identifier\": \"iris_local\"\n",
    "#         },\n",
    "#         \"trainingMetrics\": {\n",
    "#             \"accuracy\": acc,\n",
    "#             \"roc_auc\": auc,\n",
    "#             \"precision\": precision_score(y_test, y_pred, average='macro'),\n",
    "#             \"recall\": recall_score(y_test, y_pred, average='macro'),\n",
    "#             \"f1_score\": f1_score(y_test, y_pred, average='macro')\n",
    "#         },\n",
    "#         \"environment\": {\n",
    "#             \"python\": platform.python_version(),\n",
    "#             \"os\": f\"{platform.system()} {platform.release()}\",\n",
    "#             \"libraries\": {\n",
    "#                 \"sklearn\": sklearn.__version__,\n",
    "#                 \"pandas\": pd.__version__,\n",
    "#                 \"numpy\": np.__version__\n",
    "#             }\n",
    "#         },\n",
    "#         \"source\": {\n",
    "#             \"git_commit\": commit_hash,\n",
    "#             \"notebook\": \"RQ1.ipynb\"\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     # ─── UPDATE: Save FAIR4ML metadata as artifact (already in place)\n",
    "#     with open(\"model_metadata_fair4ml.json\", \"w\") as f:\n",
    "#         json.dump(fair4ml_metadata, f, indent=2)\n",
    "#     mlflow.log_artifact(\"model_metadata_fair4ml.json\")\n",
    "\n",
    "#     # ← NEW: Import PROV‑O library for RQ1.2\n",
    "#     from prov.model import ProvDocument, Namespace, PROV\n",
    "\n",
    "#     # ← NEW: Build PROV‑O document for the training run (RQ1.2)\n",
    "#     prov = ProvDocument()\n",
    "#     ex = Namespace(\"ex\", \"http://example.org/\")\n",
    "#     prov.add_namespace(ex)\n",
    "\n",
    "#     # ← NEW: Define entities and activity\n",
    "#     data_ent = prov.entity(ex[\"dataset/iris\"], {\n",
    "#         \"prov:label\": \"Iris Dataset\",\n",
    "#         \"ex:split\": \"80/20\"\n",
    "#     })\n",
    "#     model_ent = prov.entity(ex[f\"model/{run.info.run_id}\"], {\n",
    "#         \"prov:label\": \"RandomForestClassifier\",\n",
    "#         \"ex:params\": json.dumps(model.get_params())\n",
    "#     })\n",
    "#     act_train = prov.activity(\n",
    "#         ex[f\"activity/train/{run.info.run_id}\"],\n",
    "#         None, None,\n",
    "#         {\"prov:label\": \"Model training\",\n",
    "#          \"ex:startTime\": run.info.start_time,\n",
    "#          \"ex:endTime\": run.info.end_time}\n",
    "#     )\n",
    "#     prov.wasGeneratedBy(model_ent, act_train)\n",
    "#     prov.used(act_train, data_ent)\n",
    "\n",
    "#     # ← NEW: Serialize & log PROV‑O JSON‑LD artifact\n",
    "#     prov_path = \"model_provenance.jsonld\"\n",
    "#     with open(prov_path, \"w\") as f:\n",
    "#         f.write(prov.serialize(indent=2))\n",
    "#     mlflow.log_artifact(prov_path)\n",
    "\n",
    "#     # (your existing git‐commit & push helpers unchanged…)\n",
    "\n",
    "#     sha, link = simple_commit_and_push_and_log(\n",
    "#         repo_path=\".\",\n",
    "#         message=\"Auto commit after successful training\"\n",
    "#     )\n",
    "#     if sha and link:\n",
    "#         mlflow.set_tag(\"git_commit_hash\", sha)\n",
    "#         mlflow.set_tag(\"git_commit_url\", link)\n",
    "\n",
    "#     mlflow.end_run()\n",
    "\n",
    "# # ← NEW: RQ2 helper functions for auditing and reproducibility checks\n",
    "\n",
    "# def trace_run_provenance(run_id):\n",
    "#     \"\"\"Print key metadata and artifacts for a given MLflow run.\"\"\"\n",
    "#     from mlflow.tracking import MlflowClient\n",
    "#     client = MlflowClient()\n",
    "#     run = client.get_run(run_id)\n",
    "#     print(f\"📝 Run {run_id} provenance:\")\n",
    "#     print(\"  • Git SHA:\", run.data.tags.get(\"git_commit_hash\"))\n",
    "#     print(\"  • Commit URL:\", run.data.tags.get(\"git_commit_url\"))\n",
    "#     print(\"  • Dataset split:\", run.data.tags.get(\"dataset_split\"))\n",
    "#     print(\"  • Metrics:\", run.data.metrics)\n",
    "#     print(\"  • Artifacts:\", [a.path for a in client.list_artifacts(run_id)])\n",
    "\n",
    "# def detect_deprecated_code(run_id, grace_days=7):\n",
    "#     \"\"\"\n",
    "#     Check if the run's commit is older than origin/main by > grace_days.\n",
    "#     \"\"\"\n",
    "#     import subprocess\n",
    "#     from mlflow.tracking import MlflowClient\n",
    "#     client = MlflowClient()\n",
    "#     run = client.get_run(run_id)\n",
    "#     sha = run.data.tags.get(\"git_commit_hash\")\n",
    "#     main_ts = int(subprocess.check_output(\n",
    "#         [\"git\", \"log\", \"-1\", \"--format=%ct\", \"origin/main\"]\n",
    "#     ).strip())\n",
    "#     run_ts = int(subprocess.check_output(\n",
    "#         [\"git\", \"log\", \"-1\", \"--format=%ct\", sha]\n",
    "#     ).strip())\n",
    "#     age = (main_ts - run_ts) / 86400\n",
    "#     status = \"⚠️\" if age > grace_days else \"✅\"\n",
    "#     print(f\"{status} Run {run_id} is {age:.1f} days behind main.\")\n",
    "\n",
    "# # ─── USAGE:\n",
    "# # After you execute your training cell, you can call:\n",
    "# # trace_run_provenance(<run_id>)\n",
    "# # detect_deprecated_code(<run_id>, grace_days=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4d71f2-ef66-4e04-9d9b-c4b381d45590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
